{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to R-MATLAB-Julia course material \u00b6 Prerequisites Day Language Schedule Monday 6 October R R schedule Tuesday 7 October MATLAB MATLAB schedule Wednesday 8 October Julia Julia schedule Friday 10 October Advanced Advanced schedule","title":"Overview"},{"location":"#welcome-to-r-matlab-julia-course-material","text":"Prerequisites Day Language Schedule Monday 6 October R R schedule Tuesday 7 October MATLAB MATLAB schedule Wednesday 8 October Julia Julia schedule Friday 10 October Advanced Advanced schedule","title":"Welcome to R-MATLAB-Julia course material"},{"location":"preparations_account/","text":"Preparations of course project \u00b6 Info Before the course, you must have done these three things: Prerequisite 1/3: You have registered at our registration form . Prerequisite 2/3: You are part of the course project and have a user account in a cluster and are able to log in. This material Prerequisite 3/3: You have a good Zoom setup. Also, be a bit prepared about Linux environment and some basics of the languages we will cover. See the Prerequistes page](prereqs.md) Course project \u00b6 As part of the hands-on, you will be given temporary access to a course project, which will be used for running the hands-on examples. There are some policies regarding this, that we ask that you follow: You may be given access to the project before the course; please do not use the allocation for running your own codes in. Usage of the project before the course means the priority of jobs submitted to it goes down, diminishing the opportunity for you and your fellow participants to run the examples during the course. The course project will be open 1-2 weeks after the course, giving the participants the opportunity to test run examples and shorter codes related to the course. During this time, we ask that you only use it for running course related jobs. Use your own discretion, but it could be: (modified) examples from the hands-on, short personal codes that have been modified to test things learned at the course, etc. Policies Anyone found to be misusing the course project, using up large amounts of the allocation for their own production runs, will be removed from the course project. Start preparing early Start preparing early! The process of getting an account can take several days, due to some waiting steps. Registration Form \u00b6 Registration form . Get access SUPR project management (first time user) \u00b6 Register at SUPR . After doing so, it may take one evening before you are registered. You will get an email when being registered. Ask for access to a NAISS course project \u00b6 After registration, on SUPR , ask for membership according to the list below. Choose a cluster where you already work today. If you have no cluster resources yet, we recommend Tetralith at NSC . NSC: Course project: naiss2025/22-934 C3SE: Course project: naiss2025/22-934 UPPMAX: Course project: uppmax2025/2-360 HPC2N: Course project: hpc2n2025/151 LUNARC: Course project: lu2025/2-94 PDC: Course project: naiss2025/22-934 After doing so, it may take some days being granted membership. You will get an email when being granted access. Activate you user account (first time user) \u00b6 After having been granted membership on SUPR, activate your user accounts After doing so, it may take some hours before your user account is activated. First time log in \u00b6 You will get an email when being granted access. Check trash bin if you don\u2019t find it after some time. Follow the steps in the email about logging in and changing password. Tip Log in to your HPC cluster . Be prepared and skip the first hour of workshop \u00b6 Follow the steps in Log in to your HPC cluster to get used to different login methods (ssh and desktop environment/ThinLinc). test an editor ( nano ) Tip If you need help we have a login support session 9.00-9.45 each language days (the first 3 of the workshop).","title":"Preparations with course project"},{"location":"preparations_account/#preparations-of-course-project","text":"Info Before the course, you must have done these three things: Prerequisite 1/3: You have registered at our registration form . Prerequisite 2/3: You are part of the course project and have a user account in a cluster and are able to log in. This material Prerequisite 3/3: You have a good Zoom setup. Also, be a bit prepared about Linux environment and some basics of the languages we will cover. See the Prerequistes page](prereqs.md)","title":"Preparations of course project"},{"location":"preparations_account/#course-project","text":"As part of the hands-on, you will be given temporary access to a course project, which will be used for running the hands-on examples. There are some policies regarding this, that we ask that you follow: You may be given access to the project before the course; please do not use the allocation for running your own codes in. Usage of the project before the course means the priority of jobs submitted to it goes down, diminishing the opportunity for you and your fellow participants to run the examples during the course. The course project will be open 1-2 weeks after the course, giving the participants the opportunity to test run examples and shorter codes related to the course. During this time, we ask that you only use it for running course related jobs. Use your own discretion, but it could be: (modified) examples from the hands-on, short personal codes that have been modified to test things learned at the course, etc. Policies Anyone found to be misusing the course project, using up large amounts of the allocation for their own production runs, will be removed from the course project. Start preparing early Start preparing early! The process of getting an account can take several days, due to some waiting steps.","title":"Course project"},{"location":"preparations_account/#registration-form","text":"Registration form .","title":"Registration Form"},{"location":"preparations_account/#get-access-supr-project-management-first-time-user","text":"Register at SUPR . After doing so, it may take one evening before you are registered. You will get an email when being registered.","title":"Get access SUPR project management (first time user)"},{"location":"preparations_account/#ask-for-access-to-a-naiss-course-project","text":"After registration, on SUPR , ask for membership according to the list below. Choose a cluster where you already work today. If you have no cluster resources yet, we recommend Tetralith at NSC . NSC: Course project: naiss2025/22-934 C3SE: Course project: naiss2025/22-934 UPPMAX: Course project: uppmax2025/2-360 HPC2N: Course project: hpc2n2025/151 LUNARC: Course project: lu2025/2-94 PDC: Course project: naiss2025/22-934 After doing so, it may take some days being granted membership. You will get an email when being granted access.","title":"Ask for access to a NAISS course project"},{"location":"preparations_account/#activate-you-user-account-first-time-user","text":"After having been granted membership on SUPR, activate your user accounts After doing so, it may take some hours before your user account is activated.","title":"Activate you user account (first time user)"},{"location":"preparations_account/#first-time-log-in","text":"You will get an email when being granted access. Check trash bin if you don\u2019t find it after some time. Follow the steps in the email about logging in and changing password. Tip Log in to your HPC cluster .","title":"First time log in"},{"location":"preparations_account/#be-prepared-and-skip-the-first-hour-of-workshop","text":"Follow the steps in Log in to your HPC cluster to get used to different login methods (ssh and desktop environment/ThinLinc). test an editor ( nano ) Tip If you need help we have a login support session 9.00-9.45 each language days (the first 3 of the workshop).","title":"Be prepared and skip the first hour of workshop"},{"location":"preparations_cluster/","text":"Prepare the environment \u00b6 Goal The goal of this page to make sure you can follow the course. These are the things you need to follow the course: you can log in to at least one HPC cluster, in at least one way. See our login guide . you can start a text editor, See our text editor guide . Note There will be an opportunity to get help with log in every morning of the workshop at 9:00. The steps to make \u00b6 Step 1: Log in! Step 2: Make a work directory! Step 3: Test to work in the text editor nano Need help? Contact support: HPC cluster HPC Center How to contact support COSMOS LUNARC Contact LUNARC support Dardel PDC Contact PDC support Kebnekaise HPC2N Contact HPC2N support Pelle UPPMAX Contact UPPMAX support Rackham UPPMAX Contact UPPMAX support Tetralith NSC Contact NSC support","title":"Preparations on the cluster"},{"location":"preparations_cluster/#prepare-the-environment","text":"Goal The goal of this page to make sure you can follow the course. These are the things you need to follow the course: you can log in to at least one HPC cluster, in at least one way. See our login guide . you can start a text editor, See our text editor guide . Note There will be an opportunity to get help with log in every morning of the workshop at 9:00.","title":"Prepare the environment"},{"location":"preparations_cluster/#the-steps-to-make","text":"Step 1: Log in! Step 2: Make a work directory! Step 3: Test to work in the text editor nano Need help? Contact support: HPC cluster HPC Center How to contact support COSMOS LUNARC Contact LUNARC support Dardel PDC Contact PDC support Kebnekaise HPC2N Contact HPC2N support Pelle UPPMAX Contact UPPMAX support Rackham UPPMAX Contact UPPMAX support Tetralith NSC Contact NSC support","title":"The steps to make"},{"location":"prereqs/","text":"Prerequisites \u00b6 Goal The goal of this page to make sure you can follow the course. There are 3 parts. Needed knowledge of Linux environment (this page). Preparing the technical settings on the Course project preparation page . Actual logging in and preparation of the course folder . Start preparing early Start preparing early! The process of getting an account (part 2 above) can take several days, due to some waiting steps. Needed knowledge and suggested workshop/course material \u00b6 You are not a complete beginner of the Linux environment. To get familiar with the Linux/Bash command line, we recommend the courses and material at Linux101 For the language you are interested in, you are welcome both as a user of the language and as an interested person. See our other courses to find some basic material. What to prepare \u00b6 Before the course, you must have done these three things: 1. You have registered at our registration form \u00b6 2. You have done the steps in the course project preparation page \u00b6 3. Actual logging in and preparation of the course folder \u00b6 If you have problems here there is a session at 9.00 every language day (Monday-Wednesday). 4. You have a good Zoom setup \u00b6 Zoom application Working camera Working microphone Pro tip \u00b6 Learn about our clusters","title":"Prerequisites"},{"location":"prereqs/#prerequisites","text":"Goal The goal of this page to make sure you can follow the course. There are 3 parts. Needed knowledge of Linux environment (this page). Preparing the technical settings on the Course project preparation page . Actual logging in and preparation of the course folder . Start preparing early Start preparing early! The process of getting an account (part 2 above) can take several days, due to some waiting steps.","title":"Prerequisites"},{"location":"prereqs/#needed-knowledge-and-suggested-workshopcourse-material","text":"You are not a complete beginner of the Linux environment. To get familiar with the Linux/Bash command line, we recommend the courses and material at Linux101 For the language you are interested in, you are welcome both as a user of the language and as an interested person. See our other courses to find some basic material.","title":"Needed knowledge and suggested workshop/course material"},{"location":"prereqs/#what-to-prepare","text":"Before the course, you must have done these three things:","title":"What to prepare"},{"location":"prereqs/#1-you-have-registered-at-our-registration-form","text":"","title":"1. You have registered at our registration form"},{"location":"prereqs/#2-you-have-done-the-steps-in-the-course-project-preparation-page","text":"","title":"2. You have done the steps in the course project preparation page"},{"location":"prereqs/#3-actual-logging-in-and-preparation-of-the-course-folder","text":"If you have problems here there is a session at 9.00 every language day (Monday-Wednesday).","title":"3. Actual logging in and preparation of the course folder"},{"location":"prereqs/#4-you-have-a-good-zoom-setup","text":"Zoom application Working camera Working microphone","title":"4. You have a good Zoom setup"},{"location":"prereqs/#pro-tip","text":"Learn about our clusters","title":"Pro tip"},{"location":"projects/","text":"Projects \u00b6 HPC cluster Compute allocation Alvis naiss2025-22-934 COSMOS lu2025-2-94 Dardel naiss2025-22-934 Kebnekaise hpc2n2025-151 Pelle uppmax2025-2-360 Rackham uppmax2025-2-360 Tetralith naiss2025-22-934 HPC cluster Project folder Alvis /mimer/NOBACKUP/groups/courses-fall-2025/ Dardel /cfs/klemming/projects/snic/courses-fall-2025 Kebnekaise /proj/nobackup/fall-courses Rackham /proj/r-matlab-julia-pelle Tetralith /proj/courses-fall-2025/users/","title":"Projects"},{"location":"projects/#projects","text":"HPC cluster Compute allocation Alvis naiss2025-22-934 COSMOS lu2025-2-94 Dardel naiss2025-22-934 Kebnekaise hpc2n2025-151 Pelle uppmax2025-2-360 Rackham uppmax2025-2-360 Tetralith naiss2025-22-934 HPC cluster Project folder Alvis /mimer/NOBACKUP/groups/courses-fall-2025/ Dardel /cfs/klemming/projects/snic/courses-fall-2025 Kebnekaise /proj/nobackup/fall-courses Rackham /proj/r-matlab-julia-pelle Tetralith /proj/courses-fall-2025/users/","title":"Projects"},{"location":"advanced/ML/","text":"Machine Learning (ML) \u00b6 Questions What is it? When/why to use it? How to use it at HPC centres (R. MATLAB, Julia) Most likely everyone has heard about machine learning, but what exactly is it, and when and why is it suitable for your research? Machine learning (ML) is a subtopic of artificial intelligence (AI). Generally, you make predictive or decision models from data, instead of programming them explicitly. ML and data mining have a large overlap and both often use the same methods. HOWEVER ML focuses on prediction, based on known properties learned from the training data. It uses data mining methods (\u201cunsupervised learning\u201d) or during pre-processing steps. Data mining focuses on the discovery of (previously) unknown properties in the data. It uses many ML methods, but with different goals. Large Language Models (LLMs) are a special subset of ML, using learning patterns from data and built on neural networks, thus belonging more under Deep Learning (DL) Deep Learning (DL) is a subset of Machine Learning (ML), which is a subset of Artificial Intelligence (AI). ML approaches (This text is partially taken from the Wikipedia page on Machine Learning ) Supervised learning A mathematical model of a set of training data that contains training examples (inputs and the desired outputs). Classification (images, diagnostics, etc.). Target is categorical. Regression (predictions, forecasts, etc.). Target is numerical. Unsupervised learning Find structures in data that has not been labelled, classified or categorised. Identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Clustering (Recommendations, targeted marketing, etc.) Dimensionality Reduction (discovering structures, big data visualization, compression, etc.) Reinforcement Learning Looks at how software agents should take actions in an environment in order to maximise some notion of cumulative reward. real-time decisions Game AIs Robot navigation etc. When to use ML Generally: When tasks are too complex or dynamic for a traditional algorithm When you cannot define a set of rules to solve a problem image recognition spam detection etc. When you have tasks involving large amounts of unstructured data (images, audio, etc.) When you need to be able to easily adapt to new information over time ML at Swedish HPC centres \u00b6 Python, R, and MATLAB have several installed packages that will help you do machine learning, but you can do ML with any language. In this course, we will give you some information about how to find R packages for ML, as well as give an example on using MATLAB with ML, and some examples for Julia. We will only cover most of these briefly. If you are interested in how to use Python for ML at the Swedish HPC centres, then there will be some material about that in the course \u201cIntroduction to Python and using Python in an HPC environment\u201d which will be given later this year (27 Nov, 28 Nov, Dec 1, Dec 2).","title":"Introduction"},{"location":"advanced/ML/#machine-learning-ml","text":"Questions What is it? When/why to use it? How to use it at HPC centres (R. MATLAB, Julia) Most likely everyone has heard about machine learning, but what exactly is it, and when and why is it suitable for your research? Machine learning (ML) is a subtopic of artificial intelligence (AI). Generally, you make predictive or decision models from data, instead of programming them explicitly. ML and data mining have a large overlap and both often use the same methods. HOWEVER ML focuses on prediction, based on known properties learned from the training data. It uses data mining methods (\u201cunsupervised learning\u201d) or during pre-processing steps. Data mining focuses on the discovery of (previously) unknown properties in the data. It uses many ML methods, but with different goals. Large Language Models (LLMs) are a special subset of ML, using learning patterns from data and built on neural networks, thus belonging more under Deep Learning (DL) Deep Learning (DL) is a subset of Machine Learning (ML), which is a subset of Artificial Intelligence (AI). ML approaches (This text is partially taken from the Wikipedia page on Machine Learning ) Supervised learning A mathematical model of a set of training data that contains training examples (inputs and the desired outputs). Classification (images, diagnostics, etc.). Target is categorical. Regression (predictions, forecasts, etc.). Target is numerical. Unsupervised learning Find structures in data that has not been labelled, classified or categorised. Identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Clustering (Recommendations, targeted marketing, etc.) Dimensionality Reduction (discovering structures, big data visualization, compression, etc.) Reinforcement Learning Looks at how software agents should take actions in an environment in order to maximise some notion of cumulative reward. real-time decisions Game AIs Robot navigation etc. When to use ML Generally: When tasks are too complex or dynamic for a traditional algorithm When you cannot define a set of rules to solve a problem image recognition spam detection etc. When you have tasks involving large amounts of unstructured data (images, audio, etc.) When you need to be able to easily adapt to new information over time","title":"Machine Learning (ML)"},{"location":"advanced/ML/#ml-at-swedish-hpc-centres","text":"Python, R, and MATLAB have several installed packages that will help you do machine learning, but you can do ML with any language. In this course, we will give you some information about how to find R packages for ML, as well as give an example on using MATLAB with ML, and some examples for Julia. We will only cover most of these briefly. If you are interested in how to use Python for ML at the Swedish HPC centres, then there will be some material about that in the course \u201cIntroduction to Python and using Python in an HPC environment\u201d which will be given later this year (27 Nov, 28 Nov, Dec 1, Dec 2).","title":"ML at Swedish HPC centres"},{"location":"advanced/MLR/","text":"ML with R \u00b6 Questions Is R suitable for Machine Learning (ML)? Which machine learning tools are installed at HPCs? How to run R ML jobs on an HPC system (NSC, PDC, C3SE, UPPMAX, HPC2N, LUNARC) Objectives Short introduction to ML with R Overview of installed ML tools at Swedish HPC centres Workflow Show the structure of a suitable batch script Examples to try We will not learn about: How to write and optimize ML/DL code. How to use multi-node setup for training models on CPU and GPU. R provides many packages that are specifically designed for machine learning. R is also known for its statistical capabilities for analysis and interpretation of data. This all makes it easier to develop and deploy models, also without having to write a lot of code yourself. The R community has contributed many powerful packages, both for machine learning and data science. Some of the popular packages are: Package What it does Dplyr Enables dataframe manipulation in an intuitive, user-friendly way. One of the core packages of the popular tidyverse set of packages Tidyr Provides a bunch of tools to help tidy up your messy datasets. tidyr is a member of the core tidyverse Caret A set of functions that attempt to streamline the process for creating predictive models. Short for Classification And REgression Training MLR R has no standardized interface for its ML algorithms. MLR provides this infrastructure. The framework provides supervised methods like classification, regression and survival analysis along with their corresponding evaluation and optimization methods, as well as unsupervised methods like clustering. You can extend it yourself or deviate from the implemented convenience methods and construct your own complex experiments or algorithms ggplot2 A system for declaratively creating graphics. Part of tidyverse randomForest implements Breiman\u2019s random forest algorithm mlbench A collection of artificial and real-world machine learning benchmark problem stringr Provides a cohesive set of functions designed to make working with strings as easy as possible tidyverse A set of packages that work in harmony because they share common data representations and API design. Some of the popular packages in this set is: ggplot2, dplyr, tidyr, stringr, and many more and others. Installed ML tools \u00b6 There are differences depending on the centre as well as minor differences depending on the version of R. This table is not exhaustive, but lists the more popular libraries/packages and what the module is called at the various centres. Please do module spider on them to see how to load them as well as which versions are available. Package NSC PDC C3SE UPPMAX (Rackham) UPPMAX (Pelle) HPC2N LUNARC dplyr N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN tidyr N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN caret N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN mlr N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN randomForest R R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN stringr N/A R R R_packages R R R kernlab N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN Some centes have several packages installed with R, some have module \u201cbundles\u201d of R packages installed at the various centres: NSC (R/4.4.0): ~30 packages installed with R. You will have to install the rest yourself. PDC (R/4.4.2): ~1250 packages installed with R. In addition, there are many Bioconductor packages installed with the Rbio module C3SE: R(R/4.2.1): ~1340 packages installed with R. (The R/4.3.3 only has ~100 packages installed with R). HPC2N (R/4.4.1): ~100 packages installed with R. In addition many installed with R-bundle-CRAN, R-bundle-CRAN-extra, R-bundle-Bioconductor UPPMAX - Rackham (R/4.1.1): Almost all packages in CRAN and BioConductor are contained in the R_packages module, as is a small number of other R packages not in CRAN/BioConductor. Total of 23476 R packages are installed. UPPMAX - Pelle (R/4.4.2): ~100 packages installed with R. In addition many installed with R-bundle-CRAN, R-bundle-Bioconductor LUNARC (R/4.4.1): ~100 packages installed with R. In addition many installed with R-bundle-CRAN and R-bundle-Bioconductor Running your code \u00b6 Workflow Determine if you need any R libraries that are not already installed (load R module and R_packages/R-bundle-CRAN/R-bundle-Bioconductor and check) Determine if you want to run on CPUs or GPUs - some of the R version modules are not CUDA-aware Install any missing R libraries in an isolated environment Possibly download any datasets Write a batch script Submit the batch script Example Type-along We will run a simple example taken from https://machinelearningmastery.com/machine-learning-in-r-step-by-step/ If you cannot access remote data-sets, change the R code as mentioned inside to use a local data-set, which has already been downloaded NOTE : normally we would not run this on the command line, but through a batch script, but since these are short examples we will run it on the command line. NSC PDC C3SE UPPMAX HPC2N LUNARC iris_ml.R You need to install caret , kernlab , and randomForest before running, as shown below. If it asks, agree to install in local directory. $ module load R/4.4.0-hpc1-gcc-11.3.0-bare $ R > install.packages ( 'caret' , repos = 'http://ftp.acc.umu.se/mirror/CRAN/' ) > install.packages ( 'kernlab' , repos = 'http://ftp.acc.umu.se/mirror/CRAN/' ) > install.packages ( 'randomForest' , repos = 'http://ftp.acc.umu.se/mirror/CRAN/' ) > quit () $ Rscript iris_ml.R All the needed packages are part of the R module. $ module load PDC/24.11 R/4.4.2-cpeGNU-24.11 $ Rscript iris_ml.R Use version 4.2.1 of R, as that version has caret , kernlab , and randomForest included (the newest, 4.3.3, does not have these packages). $ module load R/4.2.1-foss-2022a $ Rscript iris_ml.R Rackham : All the needed packages are part of the R_packages module. $ module load R_packages/4.1.1 $ Rscript iris_ml.R Pelle : The needed packages are in R, R-bundle-CRAN modules, and R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2. $ module load R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 $ Rscript iris_ml.R All the needed packages are part of the R-bundle-CRAN module. $ module load GCC/13.2.0 OpenMPI/4.1.6 R/4.4.1 R-bundle-CRAN/2024.06 $ Rscript iris_ml.R All the needed packages are part of the R-bundle-CRAN module. $ module load GCC/12.3.0 OpenMPI/4.1.5 R/4.4.1 R-bundle-CRAN/2023.12-R-4.4.1 $ Rscript iris_ml.R Simple example taken from https://machinelearningmastery.com/machine-learning-in-r-step-by-step/ library ( caret ) # COMMENT OUT THIS SECTION IF YOU CANNOT ACCESS REMOTE DATA-SETS # -------------------------------------------------------------- # attach the iris dataset to the environment data ( iris ) # rename the dataset dataset <- iris # --------------------------------------------------------------- # REMOVE THE COMMENTS ON THIS SECTION (except comments...) TO USE LOCAL DATA-SETS # ------------------------------------------------------------------------------- # define the filename #filename <- \"iris.csv\" # load the CSV file from the local directory #dataset <- read.csv(filename, header=FALSE) # ------------------------------------------------------------------------------- # set the column names in the dataset colnames ( dataset ) <- c ( \"Sepal.Length\" , \"Sepal.Width\" , \"Petal.Length\" , \"Petal.Width\" , \"Species\" ) # create a list of 80% of the rows in the original dataset we can use for training validation_index <- createDataPartition ( dataset $Species , p = 0 .80, list = FALSE ) # select 20% of the data for validation validation <- dataset [ -validation_index, ] # use the remaining 80% of data to training and testing the models dataset <- dataset [ validation_index, ] # Run algorithms using 10-fold cross validation control <- trainControl ( method = \"cv\" , number = 10 ) metric <- \"Accuracy\" # a) linear algorithms set.seed ( 7 ) fit.lda <- train ( Species~., data = dataset, method = \"lda\" , metric = metric, trControl = control ) # b) nonlinear algorithms # CART set.seed ( 7 ) fit.cart <- train ( Species~., data = dataset, method = \"rpart\" , metric = metric, trControl = control ) # kNN set.seed ( 7 ) fit.knn <- train ( Species~., data = dataset, method = \"knn\" , metric = metric, trControl = control ) # c) advanced algorithms # SVM set.seed ( 7 ) fit.svm <- train ( Species~., data = dataset, method = \"svmRadial\" , metric = metric, trControl = control ) # Random Forest set.seed ( 7 ) fit.rf <- train ( Species~., data = dataset, method = \"rf\" , metric = metric, trControl = control ) # summarize accuracy of models results <- resamples ( list ( lda = fit.lda, cart = fit.cart, knn = fit.knn, svm = fit.svm, rf = fit.rf )) summary ( results ) # summarize Best Model print ( fit.lda ) # estimate skill of LDA on the validation dataset predictions <- predict ( fit.lda, validation ) confusionMatrix ( predictions, validation $Species ) R batch scripts for ML \u00b6 Since most R codes for Machine Learning would run for a fairly long time, you would usually have to run them in a batch script. Serial jobs \u00b6 Type-along Short serial batch example for running the R code above, iris_ml.R NSC PDC C3SE UPPMAX HPC2N LUNARC Short serial example for running on Tetralith. Loading R/4.4.0-hpc1-gcc-11.3.0-bare NOTE: if you did not install the packages caret , kernlab , and randomForest above, you have to do so now before running the script. #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.0-hpc1-gcc-11.3.0-bare module load R/4.4.0-hpc1-gcc-11.3.0-bare # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Short serial example for running on Dardel. Loading PDC/24.11 R/4.4.2-cpeGNU-24.11 #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.2-cpeGNU-24.11 and prerequisites module load PDC/24.11 R/4.4.2-cpeGNU-24.11 # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Alvis is only for running GPU jobs Short serial example script for Pelle. Loading R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor #!/bin/bash #SBATCH -A uppmax2025-2-360 # Course project id. Change to your own project ID after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.2-gfbf-2024a and R-bundle-CRAN/2024.11-foss-2024a and R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 module load R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Short serial example for running on Kebnekaise. Loading R/4.4.1 and prerequisites, also R-bundle-Bioconductor/3.19-R-4.4.1 and R-bundle-CRAN/2024.06 #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.1 and prerequisites + R-bundle-CRAN/2024.06 and R-bundle-Bioconductor/3.19-R-4.4.1 module load GCC/13.2.0 R/4.4.1 module load OpenMPI/4.1.6 R-bundle-CRAN/2024.06 R-bundle-Bioconductor/3.19-R-4.4.1 # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Short serial example for running on Cosmos. Loading R/4.2.1 and prerequisites, also a suitable R-bundle-Bioconductor #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.2.1 and prerequisites + R-bundle-Bioconductor module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 R-bundle-Bioconductor/3.15-R-4.2.1 # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Send the script to the batch: $ sbatch <batch script> NOTE you could also run the R code inside the batch script with \u201cRscript program.R\u201d. Parallel jobs \u00b6 Type-along UPPMAX HPC2N LUNARC NSC PDC C3SE MyRscript.R Short ML example for running on Pelle. #!/bin/bash #SBATCH -A uppmax2025-2-360 #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH --exclusive #SBATCH -n 1 #SBATCH --cpus-per-task=8 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 srun Rscript MyRscript.R Short ML example for running on Kebnekaise. #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 1 #SBATCH --cpus-per-task=8 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 module load GCC/13.2.0 R/4.4.1 module load OpenMPI/4.1.6 R-bundle-CRAN/2024.06 module load R-bundle-Bioconductor/3.19-R-4.4.1 srun Rscript MyRscript.R Short ML example for running on Cosmos. #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 1 #SBATCH --cpus-per-task=8 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 srun Rscript MyRscript.R Short ML example for running on Tetralith. #!/bin/bash #SBATCH -A naiss2025-23-934 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 1 #SBATCH --cpus-per-task=8 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 module load R/4.4.0-hpc1-gcc-11.3.0-bare srun Rscript MyRscript.R Short ML example for running on Dardel. #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -N 1 #SBATCH --ntasks-per-node=8 #SBATCH -p main #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 module load PDC/24.11 R/4.4.2-cpeGNU-24.11 srun Rscript MyRscript.R Alvis is only for running GPU jobs Short ML example. #Example taken from https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md library ( mlbench ) data ( Sonar ) library ( caret ) set.seed ( 95014 ) # create training & testing data sets inTraining <- createDataPartition ( Sonar $Class , p = .75, list = FALSE ) training <- Sonar [ inTraining, ] testing <- Sonar [ -inTraining, ] # set up training run for x / y syntax because model format performs poorly x <- training [ ,-61 ] y <- training [ ,61 ] #Serial mode fitControl <- trainControl ( method = \"cv\" , number = 25 , allowParallel = FALSE ) stime <- system.time ( fit <- train ( x,y, method = \"rf\" ,data = Sonar,trControl = fitControl )) #Parallel mode library ( parallel ) library ( doParallel ) cluster <- makeCluster ( 1 ) registerDoParallel ( cluster ) fitControl <- trainControl ( method = \"cv\" , number = 25 , allowParallel = TRUE ) ptime <- system.time ( fit <- train ( x,y, method = \"rf\" ,data = Sonar,trControl = fitControl )) stopCluster ( cluster ) registerDoSEQ () fit fit $resample confusionMatrix.train ( fit ) #Timings timing <- rbind ( sequential = stime, parallel = ptime ) timing $ sbatch <batch script> GPU jobs \u00b6 Some packages are now able to use GPUs for ML jobs in R. One of them is xgboost <https://xgboost.readthedocs.io/en/latest/install.html> _. In the following demo you will find instructions to install this package and run a test case with GPUs. Demo Prerequisites Choose an R version > 4.1 and a CUDA module. NSC PDC C3SE HPC2N LUNARC UPPMAX There is no compatible CUDA and R, so the best option seems to be to be to install your own R with conda. It will take quite some space, so do it in your project storage: $ ml buildenv-gcccuda/11.6.2-gcc9-hpc1 $ ml buildtool-easybuild/4.9.4-hpc71cbb0050 $ ml Miniforge/24.7.1-2-hpc1 $ cd /proj/courses-fall-2025/users/<username> $ conda create -n myenv $ conda activate myenv $ mamba create -n R -c conda-forge r-base -y $ mamba activate R $ mamba install -c conda-forge r-essentials $ R --quiet --no-save --no-restore -e \"install.packages('tictoc', repos='http://ftp.acc.umu.se/mirror/CRAN/')\" The example with xgboost will unfortunately not work on Dardel, as it works only for CUDA-capable GPUs. $ ml R/4.2.1-foss-2022a $ ml CUDA/12.9.1 ml GCC/13.2.0 R/4.4.1 CUDA/12.1.1 ml OpenMPI/4.1.6 R-bundle-CRAN/2024.06 module load GCC/12.3.0 OpenMPI/4.1.5 R/4.4.1 module load R-bundle-CRAN/2023.12-R-4.4.1 CUDA ml R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 Get a release xgboost version with GPU support and place it in the package directory for your R version - if you are using the project storage, change to that - If you have not created a package directory for the version of R you will use here, then follow the steps under setup cd /home/u/username/R-packages-4.4.1 wget https://github.com/dmlc/xgboost/releases/download/v1.5.0rc1/xgboost_r_gpu_linux.tar.gz Note If on NSC, activate your newly created conda environment, if you are not already there. Create a suitable directory for installing (probably for R 4.5.1) and add to .bashrc if you have not already: mkdir -p /proj/courses-fall-2025/users/<username>/R-packages-4.5.1 echo R_LIBS_USER = \"<path-to-your-space-on-proj-storage>/R-packages-%V\" > ~/.Renviron Then, install the package R CMD INSTALL ./xgboost_r_gpu_linux.tar.gz Download a data set like the HIGGS data set for detecting Higgs particles that is large enough to benefit from GPU acceleration (it can take several minutes to download and uncompress). Do not put in the R-packages library: wget https://archive.ics.uci.edu/static/public/280/higgs.zip unzip higgs.zip gunzip HIGGS.csv.gz Warning HIGGS.csv is a big file. If this is done during the course, you will find the HIGGS.csv file in the top of the project storage for the course. Use that instead of your own copy, and instead create a soft link for the file in your working directory: $ cd /path/to/projdir/yourdir/workdir/ $ ln -s /path/to/projdir/HIGGS.csv HIGGS.csv Copy and paste the following R script for predicting if the detected particles in the data set are Higgs bosons or not: gpu-script-db-higgs.R # Inspired by the benchmarking of Anatoly Tsyplenkov: # https://anatolii.nz/posts/2024/xgboost-gpu-r # step 0: Install these packages if you haven't done it #install.packages(c(\"xgboost\", \"data.table\", \"tictoc\")) library ( xgboost ) library ( data.table ) library ( tictoc ) # step 1: Extract the ZIP file (if not already extracted) #unzip(\"higgs.zip\") # Extracts to the current working directory # step 2: Read the CSV file higgs_data <- fread ( \"HIGGS.csv\" ) # Reads large datasets efficiently # step 3: Preprocess Data # The first column is the target (0 or 1), the rest are features X <- as.matrix ( higgs_data [, -1 , with = FALSE ]) # Remove first column y <- as.integer ( higgs_data $ V1 ) # Target column # Train-test split (75% train, 25% test) set.seed ( 111 ) N <- nrow ( X ) train_idx <- sample.int ( N , N * 0.75 ) dtrain <- xgb.DMatrix ( X [ train_idx , ], label = y [ train_idx ]) dtest <- xgb.DMatrix ( X [ - train_idx , ], label = y [ - train_idx ]) evals <- list ( train = dtrain , test = dtest ) # step 4: Define XGBoost Parameters param <- list ( objective = \"binary:logistic\" , eval_metric = \"error\" , eval_metric = \"logloss\" , max_depth = 6 , eta = 0.1 ) # step 5: Train on CPU tic () xgb_cpu <- xgb.train ( params = param , data = dtrain , watchlist = evals , nrounds = 10000 , verbose = 0 , tree_method = \"hist\" ) toc () # step 6: Train on GPU tic () xgb_gpu <- xgb.train ( params = param , data = dtrain , watchlist = evals , nrounds = 10000 , verbose = 0 , tree_method = \"hist\" , device = \"cuda\" ) toc () # Print models print ( xgb_cpu ) print ( xgb_gpu ) You can use the following template for your batch script: job-gpu.sh NSC PDC C3SE HPC2N LUNARC UPPMAX #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A naiss2025-22-934 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=03:00:00 # Ask for resources, including GPU resources #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 # Remove any loaded modules and load the ones we need module purge > /dev/null 2 > & 1 ml buildenv-gcccuda/11.6.2-gcc9-hpc1 ml buildtool-easybuild/4.9.4-hpc71cbb0050 ml Miniforge/24.7.1-2-hpc1 conda activate myenv mamba activate R R --no-save --no-restore -f gpu-script-db-higgs.R Dardel has AMD GPUs so we cannot run the xgboost R package. #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A NAISS2025-22-934 #SBATCH -t 03:00:00 #SBATCH -p alvis #SBATCH -N 1 --gpus-per-node=T4:1 ml purge > /dev/null 2 > & 1 module load R/4.2.1-foss-2022a CUDA/12.9.1 R --no-save --no-restore -f gpu-script-db-higgs.R #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #Asking for 1 hour. #SBATCH -t 03:00:00 #SBATCH -n 1 #SBATCH --gpus=1 #SBATCH -C l40s #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 ml GCC/13.2.0 R/4.4.1 CUDA/12.1.1 ml OpenMPI/4.1.6 R-bundle-CRAN/2024.06 R --no-save --no-restore -f gpu-script-db-higgs.R #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A lu2025-2-94 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=03:00:00 # Ask for GPU resources - x is how many cards, 1 or 2 #SBATCH -p gpua100 #SBATCH --gres=gpu:x # Remove any loaded modules and load the ones we need module purge > /dev/null 2 > & 1 module load GCC/12.3.0 OpenMPI/4.1.5 R/4.4.1 module load R-bundle-CRAN/2023.12-R-4.4.1 CUDA R --no-save --no-restore -f gpu-script-db-higgs.R #!/bin/bash -l #SBATCH -A uppmax2025-2-360 # Change to your own project ID #Asking for 3 hours. #SBATCH -t 03:00:00 #SBATCH -p gpu #SBATCH --gpus:l40s:1 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 R --no-save --no-restore -f gpu-script-db-higgs.R Timings ```R step 5: Train on CPU \u00b6 tic() xgb_cpu <- xgb.train( params = param, data = dtrain, watchlist = evals, + nrounds = 10000, verbose = 0, tree_method = \u201chist\u201d) toc() 10337.386 sec elapsed step 6: Train on GPU \u00b6 tic() xgb_gpu <- xgb.train( params = param, data = dtrain, watchlist = evals, + nrounds = 10000, verbose = 0, tree_method = \u201chist\u201d, device = \u201ccuda\u201d) toc() 199.416 sec elapsed ``` Exercises \u00b6 Exercise Run validation.R with Rscript This example is taken from https://www.geeksforgeeks.org/cross-validation-in-r-programming/ . validation.R # R program to implement # validation set approach # Taken from https://www.geeksforgeeks.org/cross-validation-in-r-programming/ library ( tidyverse ) library ( caret ) library ( datarium ) # setting seed to generate a # reproducible random sampling set.seed ( 123 ) # creating training data as 80% of the dataset random_sample <- createDataPartition ( marketing $ sales , p = 0.8 , list = FALSE ) # generating training dataset # from the random_sample training_dataset <- marketing [ random_sample , ] # generating testing dataset # from rows which are not # included in random_sample testing_dataset <- marketing [ - random_sample , ] # Building the model # training the model by assigning sales column # as target variable and rest other columns # as independent variables model <- lm ( sales ~ . , data = training_dataset ) # predicting the target variable predictions <- predict ( model , testing_dataset ) # computing model performance metrics data.frame ( R2 = R2 ( predictions , testing_dataset $ sales ), RMSE = RMSE ( predictions , testing_dataset $ sales ), MAE = MAE ( predictions , testing_dataset $ sales )) Solution NOTE: you may or may not have to install the \u201cdatarium\u201d package. Check! Load R, R-bundle-CRAN, R-bundle-Bioconductor, etc. and test! $ Rscript validation.R Exercise Create a batch script to run validation.R You can find example batch scripts in the exercises/r directory and you can also look at the examples on this page.","title":"With R"},{"location":"advanced/MLR/#ml-with-r","text":"Questions Is R suitable for Machine Learning (ML)? Which machine learning tools are installed at HPCs? How to run R ML jobs on an HPC system (NSC, PDC, C3SE, UPPMAX, HPC2N, LUNARC) Objectives Short introduction to ML with R Overview of installed ML tools at Swedish HPC centres Workflow Show the structure of a suitable batch script Examples to try We will not learn about: How to write and optimize ML/DL code. How to use multi-node setup for training models on CPU and GPU. R provides many packages that are specifically designed for machine learning. R is also known for its statistical capabilities for analysis and interpretation of data. This all makes it easier to develop and deploy models, also without having to write a lot of code yourself. The R community has contributed many powerful packages, both for machine learning and data science. Some of the popular packages are: Package What it does Dplyr Enables dataframe manipulation in an intuitive, user-friendly way. One of the core packages of the popular tidyverse set of packages Tidyr Provides a bunch of tools to help tidy up your messy datasets. tidyr is a member of the core tidyverse Caret A set of functions that attempt to streamline the process for creating predictive models. Short for Classification And REgression Training MLR R has no standardized interface for its ML algorithms. MLR provides this infrastructure. The framework provides supervised methods like classification, regression and survival analysis along with their corresponding evaluation and optimization methods, as well as unsupervised methods like clustering. You can extend it yourself or deviate from the implemented convenience methods and construct your own complex experiments or algorithms ggplot2 A system for declaratively creating graphics. Part of tidyverse randomForest implements Breiman\u2019s random forest algorithm mlbench A collection of artificial and real-world machine learning benchmark problem stringr Provides a cohesive set of functions designed to make working with strings as easy as possible tidyverse A set of packages that work in harmony because they share common data representations and API design. Some of the popular packages in this set is: ggplot2, dplyr, tidyr, stringr, and many more and others.","title":"ML with R"},{"location":"advanced/MLR/#installed-ml-tools","text":"There are differences depending on the centre as well as minor differences depending on the version of R. This table is not exhaustive, but lists the more popular libraries/packages and what the module is called at the various centres. Please do module spider on them to see how to load them as well as which versions are available. Package NSC PDC C3SE UPPMAX (Rackham) UPPMAX (Pelle) HPC2N LUNARC dplyr N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN tidyr N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN caret N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN mlr N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN randomForest R R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN stringr N/A R R R_packages R R R kernlab N/A R R R_packages R-bundle-CRAN R-bundle-CRAN R-bundle-CRAN Some centes have several packages installed with R, some have module \u201cbundles\u201d of R packages installed at the various centres: NSC (R/4.4.0): ~30 packages installed with R. You will have to install the rest yourself. PDC (R/4.4.2): ~1250 packages installed with R. In addition, there are many Bioconductor packages installed with the Rbio module C3SE: R(R/4.2.1): ~1340 packages installed with R. (The R/4.3.3 only has ~100 packages installed with R). HPC2N (R/4.4.1): ~100 packages installed with R. In addition many installed with R-bundle-CRAN, R-bundle-CRAN-extra, R-bundle-Bioconductor UPPMAX - Rackham (R/4.1.1): Almost all packages in CRAN and BioConductor are contained in the R_packages module, as is a small number of other R packages not in CRAN/BioConductor. Total of 23476 R packages are installed. UPPMAX - Pelle (R/4.4.2): ~100 packages installed with R. In addition many installed with R-bundle-CRAN, R-bundle-Bioconductor LUNARC (R/4.4.1): ~100 packages installed with R. In addition many installed with R-bundle-CRAN and R-bundle-Bioconductor","title":"Installed ML tools"},{"location":"advanced/MLR/#running-your-code","text":"Workflow Determine if you need any R libraries that are not already installed (load R module and R_packages/R-bundle-CRAN/R-bundle-Bioconductor and check) Determine if you want to run on CPUs or GPUs - some of the R version modules are not CUDA-aware Install any missing R libraries in an isolated environment Possibly download any datasets Write a batch script Submit the batch script Example Type-along We will run a simple example taken from https://machinelearningmastery.com/machine-learning-in-r-step-by-step/ If you cannot access remote data-sets, change the R code as mentioned inside to use a local data-set, which has already been downloaded NOTE : normally we would not run this on the command line, but through a batch script, but since these are short examples we will run it on the command line. NSC PDC C3SE UPPMAX HPC2N LUNARC iris_ml.R You need to install caret , kernlab , and randomForest before running, as shown below. If it asks, agree to install in local directory. $ module load R/4.4.0-hpc1-gcc-11.3.0-bare $ R > install.packages ( 'caret' , repos = 'http://ftp.acc.umu.se/mirror/CRAN/' ) > install.packages ( 'kernlab' , repos = 'http://ftp.acc.umu.se/mirror/CRAN/' ) > install.packages ( 'randomForest' , repos = 'http://ftp.acc.umu.se/mirror/CRAN/' ) > quit () $ Rscript iris_ml.R All the needed packages are part of the R module. $ module load PDC/24.11 R/4.4.2-cpeGNU-24.11 $ Rscript iris_ml.R Use version 4.2.1 of R, as that version has caret , kernlab , and randomForest included (the newest, 4.3.3, does not have these packages). $ module load R/4.2.1-foss-2022a $ Rscript iris_ml.R Rackham : All the needed packages are part of the R_packages module. $ module load R_packages/4.1.1 $ Rscript iris_ml.R Pelle : The needed packages are in R, R-bundle-CRAN modules, and R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2. $ module load R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 $ Rscript iris_ml.R All the needed packages are part of the R-bundle-CRAN module. $ module load GCC/13.2.0 OpenMPI/4.1.6 R/4.4.1 R-bundle-CRAN/2024.06 $ Rscript iris_ml.R All the needed packages are part of the R-bundle-CRAN module. $ module load GCC/12.3.0 OpenMPI/4.1.5 R/4.4.1 R-bundle-CRAN/2023.12-R-4.4.1 $ Rscript iris_ml.R Simple example taken from https://machinelearningmastery.com/machine-learning-in-r-step-by-step/ library ( caret ) # COMMENT OUT THIS SECTION IF YOU CANNOT ACCESS REMOTE DATA-SETS # -------------------------------------------------------------- # attach the iris dataset to the environment data ( iris ) # rename the dataset dataset <- iris # --------------------------------------------------------------- # REMOVE THE COMMENTS ON THIS SECTION (except comments...) TO USE LOCAL DATA-SETS # ------------------------------------------------------------------------------- # define the filename #filename <- \"iris.csv\" # load the CSV file from the local directory #dataset <- read.csv(filename, header=FALSE) # ------------------------------------------------------------------------------- # set the column names in the dataset colnames ( dataset ) <- c ( \"Sepal.Length\" , \"Sepal.Width\" , \"Petal.Length\" , \"Petal.Width\" , \"Species\" ) # create a list of 80% of the rows in the original dataset we can use for training validation_index <- createDataPartition ( dataset $Species , p = 0 .80, list = FALSE ) # select 20% of the data for validation validation <- dataset [ -validation_index, ] # use the remaining 80% of data to training and testing the models dataset <- dataset [ validation_index, ] # Run algorithms using 10-fold cross validation control <- trainControl ( method = \"cv\" , number = 10 ) metric <- \"Accuracy\" # a) linear algorithms set.seed ( 7 ) fit.lda <- train ( Species~., data = dataset, method = \"lda\" , metric = metric, trControl = control ) # b) nonlinear algorithms # CART set.seed ( 7 ) fit.cart <- train ( Species~., data = dataset, method = \"rpart\" , metric = metric, trControl = control ) # kNN set.seed ( 7 ) fit.knn <- train ( Species~., data = dataset, method = \"knn\" , metric = metric, trControl = control ) # c) advanced algorithms # SVM set.seed ( 7 ) fit.svm <- train ( Species~., data = dataset, method = \"svmRadial\" , metric = metric, trControl = control ) # Random Forest set.seed ( 7 ) fit.rf <- train ( Species~., data = dataset, method = \"rf\" , metric = metric, trControl = control ) # summarize accuracy of models results <- resamples ( list ( lda = fit.lda, cart = fit.cart, knn = fit.knn, svm = fit.svm, rf = fit.rf )) summary ( results ) # summarize Best Model print ( fit.lda ) # estimate skill of LDA on the validation dataset predictions <- predict ( fit.lda, validation ) confusionMatrix ( predictions, validation $Species )","title":"Running your code"},{"location":"advanced/MLR/#r-batch-scripts-for-ml","text":"Since most R codes for Machine Learning would run for a fairly long time, you would usually have to run them in a batch script.","title":"R batch scripts for ML"},{"location":"advanced/MLR/#serial-jobs","text":"Type-along Short serial batch example for running the R code above, iris_ml.R NSC PDC C3SE UPPMAX HPC2N LUNARC Short serial example for running on Tetralith. Loading R/4.4.0-hpc1-gcc-11.3.0-bare NOTE: if you did not install the packages caret , kernlab , and randomForest above, you have to do so now before running the script. #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.0-hpc1-gcc-11.3.0-bare module load R/4.4.0-hpc1-gcc-11.3.0-bare # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Short serial example for running on Dardel. Loading PDC/24.11 R/4.4.2-cpeGNU-24.11 #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.2-cpeGNU-24.11 and prerequisites module load PDC/24.11 R/4.4.2-cpeGNU-24.11 # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Alvis is only for running GPU jobs Short serial example script for Pelle. Loading R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor #!/bin/bash #SBATCH -A uppmax2025-2-360 # Course project id. Change to your own project ID after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.2-gfbf-2024a and R-bundle-CRAN/2024.11-foss-2024a and R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 module load R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Short serial example for running on Kebnekaise. Loading R/4.4.1 and prerequisites, also R-bundle-Bioconductor/3.19-R-4.4.1 and R-bundle-CRAN/2024.06 #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.1 and prerequisites + R-bundle-CRAN/2024.06 and R-bundle-Bioconductor/3.19-R-4.4.1 module load GCC/13.2.0 R/4.4.1 module load OpenMPI/4.1.6 R-bundle-CRAN/2024.06 R-bundle-Bioconductor/3.19-R-4.4.1 # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Short serial example for running on Cosmos. Loading R/4.2.1 and prerequisites, also a suitable R-bundle-Bioconductor #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.2.1 and prerequisites + R-bundle-Bioconductor module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 R-bundle-Bioconductor/3.15-R-4.2.1 # Run your R script (here 'iris_ml.R') R --no-save --quiet < iris_ml.R Send the script to the batch: $ sbatch <batch script> NOTE you could also run the R code inside the batch script with \u201cRscript program.R\u201d.","title":"Serial jobs"},{"location":"advanced/MLR/#parallel-jobs","text":"Type-along UPPMAX HPC2N LUNARC NSC PDC C3SE MyRscript.R Short ML example for running on Pelle. #!/bin/bash #SBATCH -A uppmax2025-2-360 #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH --exclusive #SBATCH -n 1 #SBATCH --cpus-per-task=8 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 srun Rscript MyRscript.R Short ML example for running on Kebnekaise. #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 1 #SBATCH --cpus-per-task=8 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 module load GCC/13.2.0 R/4.4.1 module load OpenMPI/4.1.6 R-bundle-CRAN/2024.06 module load R-bundle-Bioconductor/3.19-R-4.4.1 srun Rscript MyRscript.R Short ML example for running on Cosmos. #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 1 #SBATCH --cpus-per-task=8 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 srun Rscript MyRscript.R Short ML example for running on Tetralith. #!/bin/bash #SBATCH -A naiss2025-23-934 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 1 #SBATCH --cpus-per-task=8 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 module load R/4.4.0-hpc1-gcc-11.3.0-bare srun Rscript MyRscript.R Short ML example for running on Dardel. #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -N 1 #SBATCH --ntasks-per-node=8 #SBATCH -p main #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 module load PDC/24.11 R/4.4.2-cpeGNU-24.11 srun Rscript MyRscript.R Alvis is only for running GPU jobs Short ML example. #Example taken from https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md library ( mlbench ) data ( Sonar ) library ( caret ) set.seed ( 95014 ) # create training & testing data sets inTraining <- createDataPartition ( Sonar $Class , p = .75, list = FALSE ) training <- Sonar [ inTraining, ] testing <- Sonar [ -inTraining, ] # set up training run for x / y syntax because model format performs poorly x <- training [ ,-61 ] y <- training [ ,61 ] #Serial mode fitControl <- trainControl ( method = \"cv\" , number = 25 , allowParallel = FALSE ) stime <- system.time ( fit <- train ( x,y, method = \"rf\" ,data = Sonar,trControl = fitControl )) #Parallel mode library ( parallel ) library ( doParallel ) cluster <- makeCluster ( 1 ) registerDoParallel ( cluster ) fitControl <- trainControl ( method = \"cv\" , number = 25 , allowParallel = TRUE ) ptime <- system.time ( fit <- train ( x,y, method = \"rf\" ,data = Sonar,trControl = fitControl )) stopCluster ( cluster ) registerDoSEQ () fit fit $resample confusionMatrix.train ( fit ) #Timings timing <- rbind ( sequential = stime, parallel = ptime ) timing $ sbatch <batch script>","title":"Parallel jobs"},{"location":"advanced/MLR/#gpu-jobs","text":"Some packages are now able to use GPUs for ML jobs in R. One of them is xgboost <https://xgboost.readthedocs.io/en/latest/install.html> _. In the following demo you will find instructions to install this package and run a test case with GPUs. Demo Prerequisites Choose an R version > 4.1 and a CUDA module. NSC PDC C3SE HPC2N LUNARC UPPMAX There is no compatible CUDA and R, so the best option seems to be to be to install your own R with conda. It will take quite some space, so do it in your project storage: $ ml buildenv-gcccuda/11.6.2-gcc9-hpc1 $ ml buildtool-easybuild/4.9.4-hpc71cbb0050 $ ml Miniforge/24.7.1-2-hpc1 $ cd /proj/courses-fall-2025/users/<username> $ conda create -n myenv $ conda activate myenv $ mamba create -n R -c conda-forge r-base -y $ mamba activate R $ mamba install -c conda-forge r-essentials $ R --quiet --no-save --no-restore -e \"install.packages('tictoc', repos='http://ftp.acc.umu.se/mirror/CRAN/')\" The example with xgboost will unfortunately not work on Dardel, as it works only for CUDA-capable GPUs. $ ml R/4.2.1-foss-2022a $ ml CUDA/12.9.1 ml GCC/13.2.0 R/4.4.1 CUDA/12.1.1 ml OpenMPI/4.1.6 R-bundle-CRAN/2024.06 module load GCC/12.3.0 OpenMPI/4.1.5 R/4.4.1 module load R-bundle-CRAN/2023.12-R-4.4.1 CUDA ml R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 Get a release xgboost version with GPU support and place it in the package directory for your R version - if you are using the project storage, change to that - If you have not created a package directory for the version of R you will use here, then follow the steps under setup cd /home/u/username/R-packages-4.4.1 wget https://github.com/dmlc/xgboost/releases/download/v1.5.0rc1/xgboost_r_gpu_linux.tar.gz Note If on NSC, activate your newly created conda environment, if you are not already there. Create a suitable directory for installing (probably for R 4.5.1) and add to .bashrc if you have not already: mkdir -p /proj/courses-fall-2025/users/<username>/R-packages-4.5.1 echo R_LIBS_USER = \"<path-to-your-space-on-proj-storage>/R-packages-%V\" > ~/.Renviron Then, install the package R CMD INSTALL ./xgboost_r_gpu_linux.tar.gz Download a data set like the HIGGS data set for detecting Higgs particles that is large enough to benefit from GPU acceleration (it can take several minutes to download and uncompress). Do not put in the R-packages library: wget https://archive.ics.uci.edu/static/public/280/higgs.zip unzip higgs.zip gunzip HIGGS.csv.gz Warning HIGGS.csv is a big file. If this is done during the course, you will find the HIGGS.csv file in the top of the project storage for the course. Use that instead of your own copy, and instead create a soft link for the file in your working directory: $ cd /path/to/projdir/yourdir/workdir/ $ ln -s /path/to/projdir/HIGGS.csv HIGGS.csv Copy and paste the following R script for predicting if the detected particles in the data set are Higgs bosons or not: gpu-script-db-higgs.R # Inspired by the benchmarking of Anatoly Tsyplenkov: # https://anatolii.nz/posts/2024/xgboost-gpu-r # step 0: Install these packages if you haven't done it #install.packages(c(\"xgboost\", \"data.table\", \"tictoc\")) library ( xgboost ) library ( data.table ) library ( tictoc ) # step 1: Extract the ZIP file (if not already extracted) #unzip(\"higgs.zip\") # Extracts to the current working directory # step 2: Read the CSV file higgs_data <- fread ( \"HIGGS.csv\" ) # Reads large datasets efficiently # step 3: Preprocess Data # The first column is the target (0 or 1), the rest are features X <- as.matrix ( higgs_data [, -1 , with = FALSE ]) # Remove first column y <- as.integer ( higgs_data $ V1 ) # Target column # Train-test split (75% train, 25% test) set.seed ( 111 ) N <- nrow ( X ) train_idx <- sample.int ( N , N * 0.75 ) dtrain <- xgb.DMatrix ( X [ train_idx , ], label = y [ train_idx ]) dtest <- xgb.DMatrix ( X [ - train_idx , ], label = y [ - train_idx ]) evals <- list ( train = dtrain , test = dtest ) # step 4: Define XGBoost Parameters param <- list ( objective = \"binary:logistic\" , eval_metric = \"error\" , eval_metric = \"logloss\" , max_depth = 6 , eta = 0.1 ) # step 5: Train on CPU tic () xgb_cpu <- xgb.train ( params = param , data = dtrain , watchlist = evals , nrounds = 10000 , verbose = 0 , tree_method = \"hist\" ) toc () # step 6: Train on GPU tic () xgb_gpu <- xgb.train ( params = param , data = dtrain , watchlist = evals , nrounds = 10000 , verbose = 0 , tree_method = \"hist\" , device = \"cuda\" ) toc () # Print models print ( xgb_cpu ) print ( xgb_gpu ) You can use the following template for your batch script: job-gpu.sh NSC PDC C3SE HPC2N LUNARC UPPMAX #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A naiss2025-22-934 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=03:00:00 # Ask for resources, including GPU resources #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 # Remove any loaded modules and load the ones we need module purge > /dev/null 2 > & 1 ml buildenv-gcccuda/11.6.2-gcc9-hpc1 ml buildtool-easybuild/4.9.4-hpc71cbb0050 ml Miniforge/24.7.1-2-hpc1 conda activate myenv mamba activate R R --no-save --no-restore -f gpu-script-db-higgs.R Dardel has AMD GPUs so we cannot run the xgboost R package. #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A NAISS2025-22-934 #SBATCH -t 03:00:00 #SBATCH -p alvis #SBATCH -N 1 --gpus-per-node=T4:1 ml purge > /dev/null 2 > & 1 module load R/4.2.1-foss-2022a CUDA/12.9.1 R --no-save --no-restore -f gpu-script-db-higgs.R #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #Asking for 1 hour. #SBATCH -t 03:00:00 #SBATCH -n 1 #SBATCH --gpus=1 #SBATCH -C l40s #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 ml GCC/13.2.0 R/4.4.1 CUDA/12.1.1 ml OpenMPI/4.1.6 R-bundle-CRAN/2024.06 R --no-save --no-restore -f gpu-script-db-higgs.R #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A lu2025-2-94 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=03:00:00 # Ask for GPU resources - x is how many cards, 1 or 2 #SBATCH -p gpua100 #SBATCH --gres=gpu:x # Remove any loaded modules and load the ones we need module purge > /dev/null 2 > & 1 module load GCC/12.3.0 OpenMPI/4.1.5 R/4.4.1 module load R-bundle-CRAN/2023.12-R-4.4.1 CUDA R --no-save --no-restore -f gpu-script-db-higgs.R #!/bin/bash -l #SBATCH -A uppmax2025-2-360 # Change to your own project ID #Asking for 3 hours. #SBATCH -t 03:00:00 #SBATCH -p gpu #SBATCH --gpus:l40s:1 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 R --no-save --no-restore -f gpu-script-db-higgs.R Timings ```R","title":"GPU jobs"},{"location":"advanced/MLR/#step-5-train-on-cpu","text":"tic() xgb_cpu <- xgb.train( params = param, data = dtrain, watchlist = evals, + nrounds = 10000, verbose = 0, tree_method = \u201chist\u201d) toc() 10337.386 sec elapsed","title":"step 5: Train on CPU"},{"location":"advanced/MLR/#step-6-train-on-gpu","text":"tic() xgb_gpu <- xgb.train( params = param, data = dtrain, watchlist = evals, + nrounds = 10000, verbose = 0, tree_method = \u201chist\u201d, device = \u201ccuda\u201d) toc() 199.416 sec elapsed ```","title":"step 6: Train on GPU"},{"location":"advanced/MLR/#exercises","text":"Exercise Run validation.R with Rscript This example is taken from https://www.geeksforgeeks.org/cross-validation-in-r-programming/ . validation.R # R program to implement # validation set approach # Taken from https://www.geeksforgeeks.org/cross-validation-in-r-programming/ library ( tidyverse ) library ( caret ) library ( datarium ) # setting seed to generate a # reproducible random sampling set.seed ( 123 ) # creating training data as 80% of the dataset random_sample <- createDataPartition ( marketing $ sales , p = 0.8 , list = FALSE ) # generating training dataset # from the random_sample training_dataset <- marketing [ random_sample , ] # generating testing dataset # from rows which are not # included in random_sample testing_dataset <- marketing [ - random_sample , ] # Building the model # training the model by assigning sales column # as target variable and rest other columns # as independent variables model <- lm ( sales ~ . , data = training_dataset ) # predicting the target variable predictions <- predict ( model , testing_dataset ) # computing model performance metrics data.frame ( R2 = R2 ( predictions , testing_dataset $ sales ), RMSE = RMSE ( predictions , testing_dataset $ sales ), MAE = MAE ( predictions , testing_dataset $ sales )) Solution NOTE: you may or may not have to install the \u201cdatarium\u201d package. Check! Load R, R-bundle-CRAN, R-bundle-Bioconductor, etc. and test! $ Rscript validation.R Exercise Create a batch script to run validation.R You can find example batch scripts in the exercises/r directory and you can also look at the examples on this page.","title":"Exercises"},{"location":"advanced/MLjulia/","text":"ML with Julia \u00b6 Questions Is Julia suitable for Machine Learning (ML)? Which machine learning tools are installed at HPCs? How to run Julia ML jobs on an HPC system (NSC, PDC, C3SE, UPPMAX, HPC2N, LUNARC) Objectives Short introduction to ML with Julia Overview of installed ML tools at Swedish HPC centres Workflow Show the structure of a suitable batch script Examples to try We will not learn about: How to write and optimize ML/DL code. How to use multi-node setup for training models on CPU and GPU. Machine Learning job on GPUs \u00b6 Julia has already several packages for ML, one of them is Flux . We will work with one of the test cases provided by Flux which deals with a data set of tiny images (CIFAR10). Follow this steps: Create an environment called ML , move to that environment directory and activate it Fetch the vgg_cifar10.jl test case from Flux repo ( wget https://raw.githubusercontent.com/FluxML/model-zoo/master/vision/vgg_cifar10/vgg_cifar10.jl ) Load CUDA toolkit 11.4.1 Install (add) the following packages: CUDA, MLDatasets, MLUtils The first time you use the data set CIFAR10, it will ask you to download it and accept. Do this in Julian mode: julia > using MLDatasets : CIFAR10 julia > x , y = CIFAR10 ( :train )[ : ] Change the number of epochs in the vgg_cifar10.jl script from 50 to something shorter like 5. Submit the job with the script: #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #remove this line for UPPMAX #SBATCH --time=00:15:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #SBATCH --gres=gpu:v100:1 # 1 GPU v100 card #remove this line for UPPMAX # On Rackham use the follwing lines instead (rm one #) by subsituting the related HPC2N lines, se above ##SBATCH -M snowy ##SBATCH -p node ##SBATCH --gres=gpu:1 ##SBATCH -N 1 ##SBATCH --qos=short ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia <fix-activate-environment> <fix-name-script>.jl Answer UPPMAX HPC2N ml julia/1.8.5 mkdir ML cd ML wget https://raw.githubusercontent.com/FluxML/model-zoo/master/vision/vgg_cifar10/vgg_cifar10.jl julia ( v1.8 ) pkg> activate . ( ML ) pkg> add CUDA ( ML ) pkg> add Flux ( ML ) pkg> add MLDatasets ( ML ) pkg> add MLUtils julia> using MLDatasets: CIFAR10 julia> x, y = CIFAR10 ( :train )[ : ] The batch script looks like: #!/bin/bash -l #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -M snowy #SBATCH -p node #SBATCH --gres=gpu:1 #SBATCH -N 1 #SBATCH --time=00:15:00 # requested time #SBATCH --qos=short #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.8.5 julia --project = . vgg_cifar10.jl ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 mkdir ML cd ML wget https://raw.githubusercontent.com/FluxML/model-zoo/master/vision/vgg_cifar10/vgg_cifar10.jl julia ( v1.8 ) pkg> activate . ( ML ) pkg> add CUDA ( ML ) pkg> add Flux ( ML ) pkg> add MLDatasets ( ML ) pkg> add MLUtils julia> using MLDatasets: CIFAR10 julia> x, y = CIFAR10 ( :train )[ : ] The batch script looks like: #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #SBATCH --gres=gpu:v100:1 # 1 GPU v100 card ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia --project = . vgg_cifar10.jl At HPC2N you can use the tool job-usage on the command line: job-usage job_ID # job_ID number you get upon using sbatch This will give you a URL that you can paste on your local browser. It would display statistics after a couple of minutes the job started.","title":"With Julia"},{"location":"advanced/MLjulia/#ml-with-julia","text":"Questions Is Julia suitable for Machine Learning (ML)? Which machine learning tools are installed at HPCs? How to run Julia ML jobs on an HPC system (NSC, PDC, C3SE, UPPMAX, HPC2N, LUNARC) Objectives Short introduction to ML with Julia Overview of installed ML tools at Swedish HPC centres Workflow Show the structure of a suitable batch script Examples to try We will not learn about: How to write and optimize ML/DL code. How to use multi-node setup for training models on CPU and GPU.","title":"ML with Julia"},{"location":"advanced/MLjulia/#machine-learning-job-on-gpus","text":"Julia has already several packages for ML, one of them is Flux . We will work with one of the test cases provided by Flux which deals with a data set of tiny images (CIFAR10). Follow this steps: Create an environment called ML , move to that environment directory and activate it Fetch the vgg_cifar10.jl test case from Flux repo ( wget https://raw.githubusercontent.com/FluxML/model-zoo/master/vision/vgg_cifar10/vgg_cifar10.jl ) Load CUDA toolkit 11.4.1 Install (add) the following packages: CUDA, MLDatasets, MLUtils The first time you use the data set CIFAR10, it will ask you to download it and accept. Do this in Julian mode: julia > using MLDatasets : CIFAR10 julia > x , y = CIFAR10 ( :train )[ : ] Change the number of epochs in the vgg_cifar10.jl script from 50 to something shorter like 5. Submit the job with the script: #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #remove this line for UPPMAX #SBATCH --time=00:15:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #SBATCH --gres=gpu:v100:1 # 1 GPU v100 card #remove this line for UPPMAX # On Rackham use the follwing lines instead (rm one #) by subsituting the related HPC2N lines, se above ##SBATCH -M snowy ##SBATCH -p node ##SBATCH --gres=gpu:1 ##SBATCH -N 1 ##SBATCH --qos=short ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia <fix-activate-environment> <fix-name-script>.jl Answer UPPMAX HPC2N ml julia/1.8.5 mkdir ML cd ML wget https://raw.githubusercontent.com/FluxML/model-zoo/master/vision/vgg_cifar10/vgg_cifar10.jl julia ( v1.8 ) pkg> activate . ( ML ) pkg> add CUDA ( ML ) pkg> add Flux ( ML ) pkg> add MLDatasets ( ML ) pkg> add MLUtils julia> using MLDatasets: CIFAR10 julia> x, y = CIFAR10 ( :train )[ : ] The batch script looks like: #!/bin/bash -l #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -M snowy #SBATCH -p node #SBATCH --gres=gpu:1 #SBATCH -N 1 #SBATCH --time=00:15:00 # requested time #SBATCH --qos=short #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.8.5 julia --project = . vgg_cifar10.jl ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 mkdir ML cd ML wget https://raw.githubusercontent.com/FluxML/model-zoo/master/vision/vgg_cifar10/vgg_cifar10.jl julia ( v1.8 ) pkg> activate . ( ML ) pkg> add CUDA ( ML ) pkg> add Flux ( ML ) pkg> add MLDatasets ( ML ) pkg> add MLUtils julia> using MLDatasets: CIFAR10 julia> x, y = CIFAR10 ( :train )[ : ] The batch script looks like: #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #SBATCH --gres=gpu:v100:1 # 1 GPU v100 card ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia --project = . vgg_cifar10.jl At HPC2N you can use the tool job-usage on the command line: job-usage job_ID # job_ID number you get upon using sbatch This will give you a URL that you can paste on your local browser. It would display statistics after a couple of minutes the job started.","title":"Machine Learning job on GPUs"},{"location":"advanced/MLmatlab/","text":"ML with MATLAB \u00b6 Here is an example for running a ML script in Matlab using a Neural Network model. The dataset is present in the exercises/matlab/wine.csv file. To display the graphics for the performance of the training process an interactive session for instance with Open onDemand is recommended. % Data for Wines can be obtained from: https://archive.ics.uci.edu/dataset/109/wine % Read the csv file data = readtable ( \"wine.csv\" , 'PreserveVariableNames' , true ); % Features to be considered as input data X = [ data . Alcohol , data . MalicAcid , data . Ash , data . Acl , data . Mg , data . Phenols , ... data . NonflavanoidPhenols , data . Proanth , data . ColorInt , data . Hue , data . OD , data . Proline ]; % Feature to be predicted Y = data . Flavanoids ; % Split data: 80% for training, 10% for validation, 10% for testing idx = randperm ( height ( data )); trainIdx = idx ( 1 : round ( 0.8 * end )); valIdx = idx ( round ( 0.8 * end ) + 1 : round ( 0.9 * end )); testIdx = idx ( round ( 0.9 * end ) + 1 : end ); XTrain = X ( trainIdx ,:); YTrain = Y ( trainIdx ); XValidation = X ( valIdx ,:); YValidation = Y ( valIdx ); XTest = X ( testIdx ,:); YTest = Y ( testIdx ); % Define a GPU-capable regression network layers = [ featureInputLayer ( 12 ) % 12 features fullyConnectedLayer ( 20 ) % 3 hidden layers with 30 nodes each reluLayer fullyConnectedLayer ( 20 ) reluLayer fullyConnectedLayer ( 20 ) reluLayer fullyConnectedLayer ( 1 ) % Final layer is for regression regressionLayer ]; % Options for training and displaying the progress of the training process options = trainingOptions ( 'adam' , ... 'ValidationData' ,{ XValidation , YValidation }, ... 'MaxEpochs' , 600 , ... 'MiniBatchSize' , 32 , ... 'Plots' , 'training-progress' , ... 'ExecutionEnvironment' , 'gpu' ); %Use GPU automatically net = trainNetwork ( XTrain , YTrain , layers , options ); % Predicting values based on the trained model YPred = predict ( net , XTest ); % Plot the predicted values vs. the true values figure plot ( YTest , YPred , 'bo' ) hold on plot ([ min ( YTest ), max ( YTest )], [ min ( YTest ), max ( YTest )], 'r--' ) % perfect line xlabel ( 'True Value' ) ylabel ( 'Predicted Value' ) title ( 'Predicted vs. True Values' ) grid on % Obtain the mean square error between predicted and true values mseTest = mean (( YTest - YPred ) .^ 2 ) % If you want to perform a Classification analysis, an example for the Wine % data set can be found here: % https://se.mathworks.com/help/deeplearning/ug/wine-classification.html One can monitor the training process in Matlab: Links to MATLAB courses https://matlabacademy.mathworks.com/details/machine-learning-techniques-in-matlab/lpmlmlt https://matlabacademy.mathworks.com/en/details/machine-learning-with-matlab/mlml","title":"With MATLAB"},{"location":"advanced/MLmatlab/#ml-with-matlab","text":"Here is an example for running a ML script in Matlab using a Neural Network model. The dataset is present in the exercises/matlab/wine.csv file. To display the graphics for the performance of the training process an interactive session for instance with Open onDemand is recommended. % Data for Wines can be obtained from: https://archive.ics.uci.edu/dataset/109/wine % Read the csv file data = readtable ( \"wine.csv\" , 'PreserveVariableNames' , true ); % Features to be considered as input data X = [ data . Alcohol , data . MalicAcid , data . Ash , data . Acl , data . Mg , data . Phenols , ... data . NonflavanoidPhenols , data . Proanth , data . ColorInt , data . Hue , data . OD , data . Proline ]; % Feature to be predicted Y = data . Flavanoids ; % Split data: 80% for training, 10% for validation, 10% for testing idx = randperm ( height ( data )); trainIdx = idx ( 1 : round ( 0.8 * end )); valIdx = idx ( round ( 0.8 * end ) + 1 : round ( 0.9 * end )); testIdx = idx ( round ( 0.9 * end ) + 1 : end ); XTrain = X ( trainIdx ,:); YTrain = Y ( trainIdx ); XValidation = X ( valIdx ,:); YValidation = Y ( valIdx ); XTest = X ( testIdx ,:); YTest = Y ( testIdx ); % Define a GPU-capable regression network layers = [ featureInputLayer ( 12 ) % 12 features fullyConnectedLayer ( 20 ) % 3 hidden layers with 30 nodes each reluLayer fullyConnectedLayer ( 20 ) reluLayer fullyConnectedLayer ( 20 ) reluLayer fullyConnectedLayer ( 1 ) % Final layer is for regression regressionLayer ]; % Options for training and displaying the progress of the training process options = trainingOptions ( 'adam' , ... 'ValidationData' ,{ XValidation , YValidation }, ... 'MaxEpochs' , 600 , ... 'MiniBatchSize' , 32 , ... 'Plots' , 'training-progress' , ... 'ExecutionEnvironment' , 'gpu' ); %Use GPU automatically net = trainNetwork ( XTrain , YTrain , layers , options ); % Predicting values based on the trained model YPred = predict ( net , XTest ); % Plot the predicted values vs. the true values figure plot ( YTest , YPred , 'bo' ) hold on plot ([ min ( YTest ), max ( YTest )], [ min ( YTest ), max ( YTest )], 'r--' ) % perfect line xlabel ( 'True Value' ) ylabel ( 'Predicted Value' ) title ( 'Predicted vs. True Values' ) grid on % Obtain the mean square error between predicted and true values mseTest = mean (( YTest - YPred ) .^ 2 ) % If you want to perform a Classification analysis, an example for the Wine % data set can be found here: % https://se.mathworks.com/help/deeplearning/ug/wine-classification.html One can monitor the training process in Matlab: Links to MATLAB courses https://matlabacademy.mathworks.com/details/machine-learning-techniques-in-matlab/lpmlmlt https://matlabacademy.mathworks.com/en/details/machine-learning-with-matlab/mlml","title":"ML with MATLAB"},{"location":"advanced/big_data_managers/","text":"Big data and cluster managers \u00b6 Learning outcomes I understand how I can work with big data I know where to find more information about big data I understand how cluster managers work I know where to find more information about cluster managers Big data \u00b6 Sometimes the workflow you are targeting doesn\u2019t require extensive computations but mainly dealing with big pieces of data. An example can be, reading a column-structured file and doing some transformation per-column. Fortunately, all languages covered in this course have already several tools to deal with big data. We list some of these tools in what follows but notice that other tools doing similar jobs can be available for each language. Language-specific tools for big data R Matlab Julia Arrow (previously disk.frame ) can deal with big arrays. Other tools include data.table and bigmemory . Tall Arrays and Distributed Arrays will assist you when dealing with large arrays. According to the developers of this framework, Dagger is heavily inspired on Dask. It support distributed arrays so that they could fit the memory and also the possibility of parallelizing the computations on these arrays. Memory-mapping with Mmap Big-Arrays Effective storage \u00b6 Under construction See Python material for inspiration https://uppmax.github.io/HPC-python/day3/big_data.html#high-performance-data-analytics-hpda Allocating memory (RAM) \u00b6 Storing the data in an efficient way is one thing! Using the data in a program is another. How much is actually loaded into the working memory (RAM) Is more data in variables created during the run or work? Important Allocate many cores or a full node! You do not have to explicitely run threads or other parallelism. Note that shared memory among the cores works within node only. Tip On some clusters you do not have to request additional CPUs to get additional memory. You can use the Slurm options --mem or --mem-per-cpu Discussion Take some time to find out the answers on the questions below, using the table of hardware I\u2019ll ask around in a few minutes Table of hardware Technology Kebnekaise Pelle Bianca Cosmos Tetralith Dardel Cores/compute node 28 (72 for largemem, 128/256 for AMD Zen3/Zen4) 20 16 48 32 128 Memory/compute node 128-3072 GB 128-1024 GB 128-512 GB 256-512 GB 96-384 GB 256-2048 GB GPU NVidia V100, A100, A6000, L40s, H100, A40, AMD MI100 L40s, H100 NVidia A100 NVidia A100 NVidia T4 4 AMD Instinct\u2122 MI250X \u00e1 2 GCDs How much memory do I get per core? Divide GB RAM of the booked node with number of cores. Example: 128 GB node with 20 cores ~6.4 GB per core How much memory do I get with 5 cores? Multiply the RAM per core with number of allocated cores.. Example: 6.4 GB per core ~32 GB Do you remember how to allocate several cores? Slurm flag -n <number of cores> Tip Choose, if necessary a node with more RAM See local HPC center documentation in how to do so! Extra: Cluster Managers \u00b6 Julia \u00b6 The package ClusterManagers.jl allows you to submit expensive parts of your simulation to the batch queue in a more interactive manner than by using batch scripts. ClusterManagers.jl needs to be installed through Pkg . This can useful, for instance if you are developing some code where just specific parts are computationally heavy while the rest is related to data analysis or visualization. In order to use this package, you should add it in a Julia session. using Distributed , ClusterManagers # Adapted from: https://github.com/JuliaParallel/ClusterManagers.jl # Arguments to the Slurm srun(1) command can be given as keyword # arguments to addprocs. The argument name and value is translated to # a srun(1) command line argument as follows: # 1) If the length of the argument is 1 => \"-arg value\", # e.g. t=\"0:1:0\" => \"-t 0:1:0\" # 2) If the length of the argument is > 1 => \"--arg=value\" # e.g. time=\"0:1:0\" => \"--time=0:1:0\" # 3) If the value is the empty string, it becomes a flag value, # e.g. exclusive=\"\" => \"--exclusive\" # 4) If the argument contains \"_\", they are replaced with \"-\", # e.g. mem_per_cpu=100 => \"--mem-per-cpu=100\" # Example: add 2 processes, with your project ID, allocated 5 min, and 2 cores addprocs ( SlurmManager ( 2 ), A = \"project_ID\" , partition = \"name-of-partition\" , t = \"00:05:00\" , c = \"2\" ) # Define a function that computes the square of a number @everywhere function square ( x ) return x ^ 2 end hosts = [] result = [] for i in workers () println ( i ) host = fetch ( @spawnat i gethostname ()) push! ( hosts , host ) result_partial = fetch ( @spawnat i square ( i )) push! ( result , result_partial ) end println ( hosts ) println ( result ) # The Slurm resource allocation is released when all the workers have # exited for i in workers () rmprocs ( i ) end","title":"Big data"},{"location":"advanced/big_data_managers/#big-data-and-cluster-managers","text":"Learning outcomes I understand how I can work with big data I know where to find more information about big data I understand how cluster managers work I know where to find more information about cluster managers","title":"Big data and cluster managers"},{"location":"advanced/big_data_managers/#big-data","text":"Sometimes the workflow you are targeting doesn\u2019t require extensive computations but mainly dealing with big pieces of data. An example can be, reading a column-structured file and doing some transformation per-column. Fortunately, all languages covered in this course have already several tools to deal with big data. We list some of these tools in what follows but notice that other tools doing similar jobs can be available for each language. Language-specific tools for big data R Matlab Julia Arrow (previously disk.frame ) can deal with big arrays. Other tools include data.table and bigmemory . Tall Arrays and Distributed Arrays will assist you when dealing with large arrays. According to the developers of this framework, Dagger is heavily inspired on Dask. It support distributed arrays so that they could fit the memory and also the possibility of parallelizing the computations on these arrays. Memory-mapping with Mmap Big-Arrays","title":"Big data"},{"location":"advanced/big_data_managers/#effective-storage","text":"Under construction See Python material for inspiration https://uppmax.github.io/HPC-python/day3/big_data.html#high-performance-data-analytics-hpda","title":"Effective storage"},{"location":"advanced/big_data_managers/#allocating-memory-ram","text":"Storing the data in an efficient way is one thing! Using the data in a program is another. How much is actually loaded into the working memory (RAM) Is more data in variables created during the run or work? Important Allocate many cores or a full node! You do not have to explicitely run threads or other parallelism. Note that shared memory among the cores works within node only. Tip On some clusters you do not have to request additional CPUs to get additional memory. You can use the Slurm options --mem or --mem-per-cpu Discussion Take some time to find out the answers on the questions below, using the table of hardware I\u2019ll ask around in a few minutes Table of hardware Technology Kebnekaise Pelle Bianca Cosmos Tetralith Dardel Cores/compute node 28 (72 for largemem, 128/256 for AMD Zen3/Zen4) 20 16 48 32 128 Memory/compute node 128-3072 GB 128-1024 GB 128-512 GB 256-512 GB 96-384 GB 256-2048 GB GPU NVidia V100, A100, A6000, L40s, H100, A40, AMD MI100 L40s, H100 NVidia A100 NVidia A100 NVidia T4 4 AMD Instinct\u2122 MI250X \u00e1 2 GCDs How much memory do I get per core? Divide GB RAM of the booked node with number of cores. Example: 128 GB node with 20 cores ~6.4 GB per core How much memory do I get with 5 cores? Multiply the RAM per core with number of allocated cores.. Example: 6.4 GB per core ~32 GB Do you remember how to allocate several cores? Slurm flag -n <number of cores> Tip Choose, if necessary a node with more RAM See local HPC center documentation in how to do so!","title":"Allocating memory (RAM)"},{"location":"advanced/big_data_managers/#extra-cluster-managers","text":"","title":"Extra: Cluster Managers"},{"location":"advanced/big_data_managers/#julia","text":"The package ClusterManagers.jl allows you to submit expensive parts of your simulation to the batch queue in a more interactive manner than by using batch scripts. ClusterManagers.jl needs to be installed through Pkg . This can useful, for instance if you are developing some code where just specific parts are computationally heavy while the rest is related to data analysis or visualization. In order to use this package, you should add it in a Julia session. using Distributed , ClusterManagers # Adapted from: https://github.com/JuliaParallel/ClusterManagers.jl # Arguments to the Slurm srun(1) command can be given as keyword # arguments to addprocs. The argument name and value is translated to # a srun(1) command line argument as follows: # 1) If the length of the argument is 1 => \"-arg value\", # e.g. t=\"0:1:0\" => \"-t 0:1:0\" # 2) If the length of the argument is > 1 => \"--arg=value\" # e.g. time=\"0:1:0\" => \"--time=0:1:0\" # 3) If the value is the empty string, it becomes a flag value, # e.g. exclusive=\"\" => \"--exclusive\" # 4) If the argument contains \"_\", they are replaced with \"-\", # e.g. mem_per_cpu=100 => \"--mem-per-cpu=100\" # Example: add 2 processes, with your project ID, allocated 5 min, and 2 cores addprocs ( SlurmManager ( 2 ), A = \"project_ID\" , partition = \"name-of-partition\" , t = \"00:05:00\" , c = \"2\" ) # Define a function that computes the square of a number @everywhere function square ( x ) return x ^ 2 end hosts = [] result = [] for i in workers () println ( i ) host = fetch ( @spawnat i gethostname ()) push! ( hosts , host ) result_partial = fetch ( @spawnat i square ( i )) push! ( result , result_partial ) end println ( hosts ) println ( result ) # The Slurm resource allocation is released when all the workers have # exited for i in workers () rmprocs ( i ) end","title":"Julia"},{"location":"advanced/evaluation/","text":"Evaluation \u00b6 This is the page for evaluating the current iteration of the course. The evaluation form for the Advanced part can be found here . It takes into account that one may need to leave early too. Where can I find the results of earlier evaluations? At the \u2018Evaluations\u2019 page . Any feedback during the day \u00b6 Form to submit any feedback during the day For teachers: what is in that form? Thanks for your feedback. This feedback will be published as-is at the end of the day, if and only if there are no personal details (email, address, etc.) in the feedback. Do mention the teachers, assistants, etc by name! Evaluation questions \u00b6 Evaluation questions form Why do you evaluate under lesson hours? Because we value your time: your free time should be your free time. We think the time lost teaching is worth it to improve our teaching. For teachers: what is in that form? This is not yet decided Question 1: Overall, how would you rate today\u2019s training event? A grade from 1 to (and including) 10 Question 2: What do you think about the pace of teaching overall? [Free text] Question 3: Confidences I can name and describe three types of parallel computation I can explain at least 1 advantage of parallel computation I can explain at least 2 disadvantages of parallel computation I can explain how to use my computational resources effectively I can schedule jobs with thread parallelism I can explain how jobs with thread parallelism are scheduled I can explain how Julia/MATLAB/R code makes use of thread parallelism I can explain the results of a correct benchmark I can explain the results of an incorrect benchmark I can explain the difference between a CPU and a GPU I can use GPUs with R I can use GPUs with MATLAB I can use GPUs with Julia I can check if an ML package is installed with a module I can run an ML code with R Coding scheme: Column text Description NA I did not attend that session 0 I have no confidence I can do this 1 I have low confidence I can do this 2 I have some confidence I can do this 3 I have good confidence I can do this 4 I can absolutely do this! Question 4: Would you recommend this course to someone else? Multiple choice: Yes, no, not sure Question 5: Which future training topics would you like to be provided by the training host(s)? [Free text] Question 6: Do you have any additional comments? [Free text]","title":"Evaluation"},{"location":"advanced/evaluation/#evaluation","text":"This is the page for evaluating the current iteration of the course. The evaluation form for the Advanced part can be found here . It takes into account that one may need to leave early too. Where can I find the results of earlier evaluations? At the \u2018Evaluations\u2019 page .","title":"Evaluation"},{"location":"advanced/evaluation/#any-feedback-during-the-day","text":"Form to submit any feedback during the day For teachers: what is in that form? Thanks for your feedback. This feedback will be published as-is at the end of the day, if and only if there are no personal details (email, address, etc.) in the feedback. Do mention the teachers, assistants, etc by name!","title":"Any feedback during the day"},{"location":"advanced/evaluation/#evaluation-questions","text":"Evaluation questions form Why do you evaluate under lesson hours? Because we value your time: your free time should be your free time. We think the time lost teaching is worth it to improve our teaching. For teachers: what is in that form? This is not yet decided Question 1: Overall, how would you rate today\u2019s training event? A grade from 1 to (and including) 10 Question 2: What do you think about the pace of teaching overall? [Free text] Question 3: Confidences I can name and describe three types of parallel computation I can explain at least 1 advantage of parallel computation I can explain at least 2 disadvantages of parallel computation I can explain how to use my computational resources effectively I can schedule jobs with thread parallelism I can explain how jobs with thread parallelism are scheduled I can explain how Julia/MATLAB/R code makes use of thread parallelism I can explain the results of a correct benchmark I can explain the results of an incorrect benchmark I can explain the difference between a CPU and a GPU I can use GPUs with R I can use GPUs with MATLAB I can use GPUs with Julia I can check if an ML package is installed with a module I can run an ML code with R Coding scheme: Column text Description NA I did not attend that session 0 I have no confidence I can do this 1 I have low confidence I can do this 2 I have some confidence I can do this 3 I have good confidence I can do this 4 I can absolutely do this! Question 4: Would you recommend this course to someone else? Multiple choice: Yes, no, not sure Question 5: Which future training topics would you like to be provided by the training host(s)? [Free text] Question 6: Do you have any additional comments? [Free text]","title":"Evaluation questions"},{"location":"advanced/gpus/","text":"Using GPUs \u00b6 Questions What is GPU acceleration? How to enable GPUs? How to deploy GPUs at HPC2N, UPPMAX, LUNARC, NSC, PDC and C3SE? Objectives Get an intro to common schemes for GPU code acceleration Learn about the GPU nodes at HPC2N, UPPMAX, LUNARC, NSC, PDC, and C3SE Learn how to make a batch script asking for GPU nodes at HPC2N, UPPMAX, LUNARC, NSC, PDC, and C3SE Introduction \u00b6 In order to understand the capabilities of a GPU, it is instructive to compare a pure CPU architecture with a GPU based architecture. Here, there is a schematics of the former: Pure CPU architecture (single node). In the present case there are 256 cores, each with its own cache memory (LX). There is a shared memory (~378 GB/NUMA node) for all these cores. This is an AMD Zen4 node. The base frequency is 2.25 GHz, but it can boost up to 3.1 GHz. As for the GPU architecture, a GPU card of type Ada Lovelace (like the L40s) looks like this: Note: The AD102 GPU also includes 288 FP64 Cores (2 per SM) which are not depicted in the above diagram. The FP64 TFLOP rate is 1/64th the TFLOP rate of FP32 operations. The small number of FP64 Cores are included to ensure any programs with FP64 code operate correctly, including FP64 Tensor Core code. This is a single GPU engine of a L40s card. There are 12 Graphics Processing Clusters (GPCs), 72 Texture Processing Clusters (TPCs), 144 Streaming Multiprocessors (SMs), and a 384-bit memory interface with 12 32-bit memory controllers). On the diagram, each green dot represents a CUDA core (single precision), while the yellow are RT cores and blue Tensor cores. The cores are arranged in the slots called SMs in the figure. Cores in the same SM share some local and fast cache memory. GPCs The GPC is the dominant high-level hardware block. Each GPC includes a dedicated Raster Engine, two Raster Operations (ROPs) partitions, with each partition containing eight individual ROP units, and six TPCs. Each TPC includes one PolyMorph Engine and two SMs. Each SM contain 128 CUDA Cores, one Ada Third-Generation RT Core, four Ada Fourth-Generation Tensor Cores, four Texture Units, a 256 KB Register File, and 128 KB of L1/Shared Memory, which can be configured for different memory sizes depending on the needs of the graphics or compute workload. In a typical cluster, some GPUs are attached to a single node resulting in a CPU-GPU hybrid architecture. The CPU component is called the host and the GPU part the device. One possible layout Kebnekaise, AMD Zen4 node with L40s GPU Schematics of a hybrid CPU-GPU architecture. A GPU L40s card is attached to a NUMA island which in turn contains 24 cores (AMD Zen4 CPU node with 48 cores total). The NUMA island and the GPUs are connected through a PCI-E interconnect which makes the data transfer between both components rather slow. We can characterize the CPU and GPU performance with two quantities: the latency and the throughput . Latency refers to the time spent in a sole computation. Throughput denotes the number of computations that can be performed in parallel. Then, we can say that a CPU has low latency (able to do fast computations) but low throughput (only a few computations simultaneously). In the case of GPUs, the latency is high and the throughput is also high. We can visualize the behavior of the CPUs and GPUs with cars as in the figure below. A CPU would be compact road where only a few racing cars can drive whereas a GPU would be a broader road where plenty of slow cars can drive. Cars and roads analogy for the CPU and GPU behavior. The compact road is analogous to the CPU (low latency, low throughput) and the broader road is analogous to the GPU (high latency, high throughput). Not every program is suitable for GPU acceleration. GPUs process simple functions rapidly, and are best suited for repetitive and highly-parallel computing tasks. GPUs were originally designed to render high-resolution images and video concurrently and fast, but since they can perform parallel operations on multiple sets of data, they are also often used for other, non-graphical tasks. Common uses are machine learning and scientific computation were the GPUs can take advantage of massive parallelism. Many R packages are not CUDA aware, but some have been written specifically with GPUs in mind. Many Julia packages are not CUDA aware. The CUDA.jl package is the main programming interface for working with Nvidia GPUs. Matlab does have support for computing on GPUs, but you need to write functions that support GPU execution. Many functions in Matlab run automatically on a GPU if you supply a gpuArray data argument. GPU computing in MATLAB requires Parallel Computing Toolbox. Information here: Run MATLAB Functions on a GPU . One of the most common use of GPUs with and Julia is for machine learning or deep learning. GPUs on C3SE, UPPMAX, HPC2N, LUNARC, NSC, and PDC systems \u00b6 There are generally either not GPUs on the login nodes or they cannot be accessed for computations. To use them you need to either launch an interactive job or submit a batch job. C3SE NSC PDC UPPMAX HPC2N LUNARC Alvis is meant for GPU jobs. There is no node-sharing on multi-node jobs ( --exclusive is automatic). NOTE: Requesting -N 1 does not mean 1 full node You would need to add this to your batch script: #SBATCH -p alvis #SBATCH -N <nodes> #SBATCH --gpus-per-node=<type>:x where <type> is one of V100 T4 A100 and x is number of GPU cards 1-4 for V100 1-8 for T4 1-4 for A100 Tetralith has Nvidia T4 GPUs. In order to access them, add this to your batch script or interactive job: #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 Dardel has AMD Instinct\u2122 MI250X GPU chips. In order to access them, add this to your batch script or interactive job: You need to add this to your batch script (or interactive job) in order to use them: #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu NOTE: the fact that Dardel has AMD GPUs means that CUDA-enabled packages will not run! You need something like hip. Rackham\u2019s compute nodes do not have GPUs. You need to use Snowy for that. The new cluster Pelle has GPUs. On Pelle, you need to use this batch command: for L40s GPUs (up to 10 GPU cards) #SBATCH -p gpu #SBATCH --gpus:l40s:<number of GPUs> or for H100 GPUs (up to 2 GPU cards) #SBATCH -p gpu #SBATCH --gpus=h100:<number of GPUs> Kebnekaise\u2019s GPU nodes are considered a separate resource, and the regular compute nodes do not have GPUs. Kebnekaise has a great many different types of GPUs: V100 (2 cards/node) A40 (8 cards/node) A6000 (2 cards/node) L40s (2 or 6 cards/node) A100 (2 cards/node) H100 (4 cards/node) MI100 (2 cards/node) To access them, you need to use this to the batch system: #SBATCH --gpus=x where x is the number of GPU cards you want. Above are given how many are on each type, so you can ask for up to that number. In addition, you need to add this to the batch system: #SBATCH -C <type> where type is v100 a40 a6000 l40s a100 h100 mi100 For more information, see HPC2N\u2019s guide to the different parts of the batch system: https://docs.hpc2n.umu.se/documentation/batchsystem/resources/ LUNARC has Nvidia A100 GPUs and Nvidia A40 GPUs, but the latter ones are reserved for interactive graphics work on the on-demand system, and Slurm jobs should not be submitted to them. Thus in order to use the A100 GPUs on Cosmos, add this to your batch script: A100 GPUs on AMD nodes: #SBATCH -p gpua100 #SBATCH --gres=gpu:1 These nodes are configured as exclusive access and will not be shared between users. User projects will be charged for the entire node (48 cores). A job on a node will also have access to all memory on the node. A100 GPUs on Intel nodes: #SBATCH -p gpua100i #SBATCH --gres=gpu:<number> where <number> is 1 or 2 (Two of the nodes have 1 GPU and two have 2 GPUs). Deploying GPUs \u00b6 As mentioned, you need a batch job or an interactive job (could also be through Open OnDemand or from inside MATLAB) to use the GPUs. R \u00b6 Here follows an example of a batch script that allocates GPUs. This is for an R job, but it is done similarly for Julia and Matlab. NSC PDC C3SE UPPMAX HPC2N LUNARC #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A naiss2025-22-934 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for resources, including GPU resources #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 # Remove any loaded modules and load the ones we need module purge > /dev/null 2 > & 1 module load R/4.4.0-hpc1-gcc-11.3.0-bare R --no-save --no-restore -f MY-R-GPU-SCRIPT.R #!/bin/bash -l # Remember to change this to your own project ID after the course! #SBATCH -A naiss2025-22-934 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for resources, including GPU resources #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu module load PDC/23.12 R/4.4.1-cpeGNU-23.12 module load rocm/5.7.0 #module load craype-accel-amd-gfx90a #module load cpeGNU/23.12 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A NAISS2025-22-934 #SBATCH -t HHH:MM:SS #SBATCH -p alvis #SBATCH -N 1 --gpus-per-node=T4:4 ml purge > /dev/null 2 > & 1 module load R/4.2.1-foss-2022a CUDA/12.9.1 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R Rackham/Snowy : #!/bin/bash -l #SBATCH -A uppmax2025-Y-ZZZ #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS #SBATCH --exclusive #SBATCH -p node #SBATCH -N 1 #SBATCH -M snowy #SBATCH --gpus=1 #SBATCH --gpus-per-node=1 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 ml uppmax R/4.1.1 R_packages/4.1.1 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R Pelle (1 L40s) : #!/bin/bash -l #SBATCH -A uppmax2025-Y-ZZZ #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS #SBATCH -p gpu #SBATCH --gpus:l40s:1 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 # Reloading a module that got removed with purge ml Java/17 ml R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS #Ask for GPU resources. You pick type as one of the ones shown above #x is how many cards you want, at most as many as shown above #SBATCH --gpus:x #SBATCH -C type #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 #R version 4.4.1 ml GCC/13.2.0 R/4.4.1 OpenMPI/4.1.6 R-bundle-CRAN/2024.06 ml CUDA/12.6.0 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A lu2025-2-94 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for GPU resources - x is how many cards, 1 or 2 #SBATCH -p gpua100 #SBATCH --gres=gpu:x # Remove any loaded modules and load the ones we need module purge > /dev/null 2 > & 1 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 CUDA/12.1.1 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R MATLAB \u00b6 Here we cover how to use GPUs with MATLAB Inside MATLAB In a batch script In order to use GPUs, you have to ask for them. Inside MATLAB \u00b6 In order to use GPUs from inside MATLAB, you must add them as additional properties to your profile. This is done the same whether you use MATLAB on the command line or inside the GUI. If you work inside the GUI it is also possible to set these GPU values in the Cluster Profile Manager. Remember, after it is saved to your profile it will use GPUs again next time you submit a job, even if you don\u2019t want GPUs there. To reset this, do: c.AdditionalProperties.GpuCard = '' ; c.AdditionalProperties.GpusPerNode = '' ; This is how you add GPUs to use in batch jobs submitted from inside MATLAB: Important Ask for a GPU and enough time to do what you need. UPPMAX HPC2N LUNARC NSC PDC C3SE interactive -A uppmax2025-2-360 -p gpu --gpus:l40s:1 -t 2 :00:00 Load MATLAB ml MATLAB/2024a Then run MATLAB either as GUI\u2026 matlab -singleCompThread \u2026Or on the terminal matlab -singleCompThread -nodisplay -nosplash -nodesktop Finally, inside MATLAB, add this to your profile (remember the c=parcluster; after you start MATLAB again, to get a handle): c . AdditionalProperties . GpusPerNode = 1 ; c . saveProfile Load and start MATLAB, then do (remember the c=parcluster; after you start MATLAB again, to get a handle) c . AdditionalProperties . GpuCard = 'card-type' ; c . AdditionalProperties . GpusPerNode = '#gpus' ; c . saveProfile where card-type is one of v100 , a40 , a6000 , l40s , a100 , h100 , or mi100 , and #gpus depends on the card-type: V100 (2 cards/node) A40 (8 cards/node) A6000 (2 cards/node) L40s (2 or 6 cards/node) A100 (2 cards/node) H100 (4 cards/node) MI100 (2 cards/node) Load and start MATLAB, then do (remember the c=parcluster; after you start MATLAB again, to get a handle) c . AdditionalProperties . GpusPerNode = # GPUs ; c . saveProfile where #GPUs is 1 or 2. Load and start MATLAB, then do (remember the c=parcluster; after you start MATLAB again, to get a handle) c . AdditionalProperties . GPUsPerNode = # GPUs ; c . saveProfile where #GPUs is 1 or 2. Remember, here you cannot set AdditionalProperties . Instead, you do the following: Start an interactive session on the GPU partition: salloc -N 1 --ntasks-per-node=1 --t 1:00:00 -A naiss2025-22-262 -p gpu Load MATLAB: module load PDCOLD/23.12 matlab/r2024a-ps Start MATLAB: matlab -nodisplay -nodesktop -nosplash You are now ready to run your GPU MATLAB scripts. Remember, here you cannot set AdditionalProperties . Instead, you do the following: Start am interactive sesson asking for a GPU: srun --account=naiss2025-22-934 --gpus-per-node=T4:1 --time=01:00:00 --pty /bin/bash Load MATLAB: module load MATLAB/2024b Start MATLAB: matlab -singleCompThread -nodisplay -nosplash -nodesktop Challenge 5. Add/remove GPUs in your cluster profile Try to add GPUs to your cluster profile and save it. Run c.AdditionalProperties to see what was added. Then do c.AdditionalProperties.GpusPerNode = ''; to remove it. See that it was removed. Matlab with GPUs in batch scripts \u00b6 How to request a GPU node varies somewhat between clusters. Refer to the following templates: UPPMAX HPC2N LUNARC NSC PDC C3SE #!/bin/bash # Change to your actual project number #SBATCH -A naiss2025-22-360 #SBATCH -p gpu #SBATCH --gpus:l40s:1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Change depending on resource and MATLAB version # to find out available versions: module spider MATLAB module add MATLAB/2024a # Executing a GPU matlab program matlab -nodisplay -nosplash -r \"gpu-matlab-script.m\" #!/bin/bash # Change to your actual project number #SBATCH -A hpc2n2025-151 #SBATCH -n 1 #SBATCH --gpus=<#gpus> #SBATCH -C <gpu-type> # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module load MATLAB/2023a.Update4 # Executing a GPU matlab program matlab -nodisplay -nosplash -r \"gpu-matlab-script.m\" where card-type is one of v100 , a40 , a6000 , l40s , a100 , h100 , or mi100 , and #gpus depends on the card-type: V100 (2 cards/node) A40 (8 cards/node) A6000 (2 cards/node) L40s (2 or 6 cards/node) A100 (2 cards/node) H100 (4 cards/node) MI100 (2 cards/node) #!/bin/bash # Change to your actual project number #SBATCH -A lu2025-7-94 #SBATCH -n 1 #SBATCH -p gpua100 # The number of GPUs.#gpus, can be 1 or 2 #SBATCH --gpus=<#gpus> # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module load matlab/2023b # Executing a GPU matlab program matlab -nodisplay -nosplash -r \"gpu-matlab-script.m\" #!/bin/bash # Change to your actual project number #SBATCH -A naiss2025-22-934 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-core=1 # The number of GPUs.#gpus, can be 1 or 2 #SBATCH --gpus-per-task=1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module load MATLAB/2024a-hpc1-bdist # Executing a GPU matlab program matlab -singleCompThread -nodisplay -nosplash -r \"gpu-matlab-script.m\" #!/bin/bash # Change to your actual project number #SBATCH -A naiss2025-22-934 #SBATCH --ntasks-per-node=1 #SBATCH -N 1 # Ask for GPUs #SBATCH -p gpu # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module load PDC/24.11 matlab/r2024b rocm/5.7.0 # Executing a GPU matlab program matlab -singleCompThread -nodisplay -nosplash -r \"gpu-matlab-script.m\" #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A NAISS2025-22-934 #SBATCH -t 00:30:00 #SBATCH -p alvis #You always need to ask for GPUs on Alvis! And you should not use it for anything but GPU jobs! #SBATCH -N 1 --gpus-per-node=T4:1 ml purge > /dev/null 2 > & 1 module load MATLAB/2024b matlab -singleCompThread -nodisplay -nosplash -r \"gpu-matlab-script.m\" Julia \u00b6 In order to use the NVIDIA GPUs with Julia (UPPMAX, HPC2N, and LUNARC), you will need to load a CUDA toolkit module on the cluster and install the CUDA package in Julia. In the case of AMD GPUs for Julia (PDC and HPC2N), you will need to load a ROCM toolkit module on the cluster and install the AMDGPU package in Julia as in the next sequence of commands. Prerequisites UPPMAX HPC2N LUNARC PDC NSC This can only be done on Snowy or Bianca. Then either create an interactive session or make a batch job CUDA is installed at system level so they do not need to be loaded. On snowy $ interactive -A <proj> -n 1 -M snowy --gres = gpu:1 -t 3 :00:00 $ ml Julia/1.8.5 # Julia version $ julia ( v1.8 ) pkg> add CUDA Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... Installed CEnum \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 v0.4.2 ... $ ml Julia/1.8.5-linux-x86_64 # Julia version $ ml CUDA/11.4.1 # CUDA toolkit module $ julia ( v1.8 ) pkg> add CUDA Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... Installed CEnum \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 v0.4.2 ... $ ml Julia/1.8.5-linux-x86_64 # Julia version $ ml CUDA/11.4.1 # CUDA toolkit module $ julia ( v1.8 ) pkg> add CUDA Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... Installed CEnum \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 v0.4.2 ... $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # Julia version $ ml rocm/5.7.0 craype-accel-amd-gfx90a # ROCM toolkit module $ julia ( v1.10 ) pkg> add AMDGPU Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... Installed CEnum \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 v0.4.2 $ interactive -A <proj> -n 1 -c 32 --gpus-per-task = 1 -t 1 :00:00 $ ml buildenv-gcccuda/11.6.2-gcc9-hpc1 # Load tool chain with CUDA $ ml julia/1.9.4-bdist # Julia version $ julia ( v1.9 ) pkg> add LinearAlgebra ( v1.9 ) pkg> add CUDA Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... ... Once this initial setting is completed, you will be able to use the GPUs available on the cluster. Here, there is a simple example for computing a matrix-matrix multiplication. As a reference point, we show the simulation on CPUs as well. You can call the batch script job-gpu.sh , for instance. Running on NVIDIA GPUs UPPMAX HPC2N LUNARC NSC script-gpu.jl Short GPU example for running on Snowy. #!/bin/bash -l #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -M snowy #SBATCH -p node #SBATCH --gres=gpu:1 #SBATCH -N 1 #SBATCH --job-name=job-gpu # create a short name for your job #SBATCH --time=00:15:00 # total run time limit (HH:MM:SS) #SBATCH --qos=short # if test run t<15 min #SBATCH --mail-type=begin # send email when job begins #SBATCH --mail-type=end # send email when job ends module load julia/1.8.5 # system CUDA works as of today julia script-gpu.jl #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-gpu # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #SBATCH --gres=gpu:v100:1 # 1 GPU v100 card ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia script-gpu.jl #!/bin/bash #SBATCH -A lu202w-x-yz # your project_ID #SBATCH -J job-gpu # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #Asking for one A100 GPU #SBATCH -p gpua100 #SBATCH --gres=gpu:1 ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia script-gpu.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job-gpu # name of the job #SBATCH -n 1 # nr. tasks #SBATCH -c 32 # nr. cores #SBATCH --gpus-per-task=1 # nr. GPU cards #SBATCH --time=00:04:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml buildenv-gcccuda/11.6.2-gcc9-hpc1 ml julia/1.9.4-bdist julia script-gpu.jl Julia GPU example code. using CUDA CUDA . versioninfo () N = 2 ^ 8 x = rand ( N , N ) y = rand ( N , N ) A = CuArray ( x ) B = CuArray ( y ) # Calculation on CPU @time x * y # Calculation on GPU @time A * B # Calculation on CPU @time x * y # Calculation on GPU @time A * B Running on AMD GPUs PDC script-gpu.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p gpu # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # ROCM toolkit module ml rocm/5.7.0 craype-accel-amd-gfx90a julia script-gpu.jl Julia AMD GPU example code. using AMDGPU AMDGPU . versioninfo () # Display AMD GPU information N = 2 ^ 8 x = rand ( N , N ) y = rand ( N , N ) A = ROCArray ( x ) # Transfer data to AMD GPU B = ROCArray ( y ) # Calculation on CPU @time x * y # Calculation on AMD GPU @time A * B # Calculation on CPU (again) @time x * y # Calculation on AMD GPU (again) @time A * B Julia Exercises \u00b6 Exercise 1. Run the GPU script Run the following script script-gpu.jl . Why are we running the simulations twice? Note that at UPPMAX you will need a project will access to Snowy. Remember that at PDC we will use AMD GPUs. Answer HPC2N UPPMAX LUNARC PDC NSC This batch script is for Kebnekaise. We run the simulation twice because in this way, the reported time is more reliable for the computing time as in the first simulation, data transfer and other settings could be added to the reported time. #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #SBATCH --gres=gpu:v100:1 # 1 GPU v100 card ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia script-gpu.jl Output: 0 .689096 seconds ( 2 .72 M allocations: 132 .617 MiB, 6 .27% gc time, 99 .62% compilation time ) 1 .194153 seconds ( 1 .24 M allocations: 62 .487 MiB, 3 .41% gc time, 55 .13% compilation time ) 0 .000933 seconds ( 2 allocations: 512 .047 KiB ) 0 .000311 seconds ( 5 allocations: 192 bytes ) This batch script is for UPPMAX. Adding the numbers 2 and 3. #!/bin/bash -l #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -M snowy #SBATCH -p node #SBATCH --gres=gpu:1 #SBATCH -N 1 #SBATCH --job-name=juliaGPU # create a short name for your job #SBATCH --time=00:15:00 # total run time limit (HH:MM:SS) #SBATCH --qos=short # if test run t<15 min ml julia/1.8.5 julia script-gpu.jl Output: Downloading artifact: CUDNN Downloading artifact: CUTENSOR CUDA toolkit 11 .7, artifact installation NVIDIA driver 525 .85.12, for CUDA 12 .0 CUDA driver 12 .0 Libraries: - CUBLAS: 11 .10.1 - CURAND: 10 .2.10 - CUFFT: 10 .7.2 - CUSOLVER: 11 .3.5 - CUSPARSE: 11 .7.3 - CUPTI: 17 .0.0 - NVML: 12 .0.0+525.85.12 - CUDNN: 8 .30.2 ( for CUDA 11 .5.0 ) - CUTENSOR: 1 .4.0 ( for CUDA 11 .5.0 ) Toolchain: - Julia: 1 .8.5 - LLVM: 13 .0.1 - PTX ISA support: 3 .2, 4 .0, 4 .1, 4 .2, 4 .3, 5 .0, 6 .0, 6 .1, 6 .3, 6 .4, 6 .5, 7 .0, 7 .1, 7 .2 - Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80, sm_86 1 device: 0 : Tesla T4 ( sm_75, 14 .605 GiB / 15 .000 GiB available ) 0 .988437 seconds ( 2 .72 M allocations: 132 .556 MiB, 4 .72% gc time, 99 .10% compilation time ) 5 .707402 seconds ( 1 .30 M allocations: 65 .564 MiB, 0 .72% gc time, 19 .70% compilation time ) 0 .000813 seconds ( 2 allocations: 512 .047 KiB ) 0 .000176 seconds ( 16 allocations: 384 bytes ) This batch script is for Cosmos. #!/bin/bash #SBATCH -A lu202w-x-yz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #Asking for one A100 GPU #SBATCH -p gpua100 #SBATCH --gres=gpu:1 ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia script-gpu.jl This batch script is for Dardel. #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p gpu # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # ROCM toolkit module ml rocm/5.7.0 craype-accel-amd-gfx90a julia script-gpu.jl OUTPUT: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500... \u2502 Available \u2502 Name \u2502 ... \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500... \u2502 + \u2502 LLD \u2502 ... \u2502 + \u2502 Device Libraries \u2502 ... \u2502 + \u2502 HIP \u2502 ... \u2502 + \u2502 rocBLAS \u2502 ... \u2502 + \u2502 rocSOLVER \u2502 ... \u2502 + \u2502 rocSPARSE \u2502 ... \u2502 + \u2502 rocRAND \u2502 ... \u2502 + \u2502 rocFFT \u2502 ... \u2502 + \u2502 MIOpen \u2502 ... \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500... \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500... \u2502 Id \u2502 Name \u2502 ... \u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500... \u2502 1 \u2502 AMD Instinct MI250X \u2502 gfx9... \u2502 2 \u2502 AMD Instinct MI250X \u2502 gfx9... \u2502 3 \u2502 AMD Instinct MI250X \u2502 gfx9... \u2502 4 \u2502 AMD Instinct MI250X \u2502 gfx9... \u2502 5 \u2502 AMD Instinct MI250X \u2502 gfx9... \u2502 6 \u2502 AMD Instinct MI250X \u2502 gfx9... \u2502 7 \u2502 AMD Instinct MI250X \u2502 gfx9... \u2502 8 \u2502 AMD Instinct MI250X \u2502 gfx9... \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500... 1 .241600 seconds ( 2 .27 M allocations: 152 .229 MiB, 8 .28% gc time, 91 .71% compilation time ) 0 .604009 seconds ( 624 .95 k allocations: 38 .360 MiB, 68 .01% compilation time ) 0 .001051 seconds ( 2 allocations: 512 .047 KiB ) 0 .000077 seconds ( 13 allocations: 352 bytes ) This batch script is for NSC. #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job-gpu # name of the job #SBATCH -n 1 # nr. tasks #SBATCH -c 32 # nr. cores #SBATCH --gpus-per-task=1 # nr. GPU cards #SBATCH --time=00:04:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml buildenv-gcccuda/11.6.2-gcc9-hpc1 ml julia/1.9.4-bdist julia script-gpu.jl Summary GPUs process simple functions rapidly, and are best suited for repetitive and highly-parallel computing tasks There are GPUs on NSC/Tetralith, PDC/Dardel, C3SE/Alvis, HPC2N/Kebnekaise, LUNARC/Cosmos, UPPMAX/Pelle, but they are different It varies between centres how you allocate a GPU You need to use either batch or interactive/OpenOnDemand to use GPUs","title":"Introduction to GPUs"},{"location":"advanced/gpus/#using-gpus","text":"Questions What is GPU acceleration? How to enable GPUs? How to deploy GPUs at HPC2N, UPPMAX, LUNARC, NSC, PDC and C3SE? Objectives Get an intro to common schemes for GPU code acceleration Learn about the GPU nodes at HPC2N, UPPMAX, LUNARC, NSC, PDC, and C3SE Learn how to make a batch script asking for GPU nodes at HPC2N, UPPMAX, LUNARC, NSC, PDC, and C3SE","title":"Using GPUs"},{"location":"advanced/gpus/#introduction","text":"In order to understand the capabilities of a GPU, it is instructive to compare a pure CPU architecture with a GPU based architecture. Here, there is a schematics of the former: Pure CPU architecture (single node). In the present case there are 256 cores, each with its own cache memory (LX). There is a shared memory (~378 GB/NUMA node) for all these cores. This is an AMD Zen4 node. The base frequency is 2.25 GHz, but it can boost up to 3.1 GHz. As for the GPU architecture, a GPU card of type Ada Lovelace (like the L40s) looks like this: Note: The AD102 GPU also includes 288 FP64 Cores (2 per SM) which are not depicted in the above diagram. The FP64 TFLOP rate is 1/64th the TFLOP rate of FP32 operations. The small number of FP64 Cores are included to ensure any programs with FP64 code operate correctly, including FP64 Tensor Core code. This is a single GPU engine of a L40s card. There are 12 Graphics Processing Clusters (GPCs), 72 Texture Processing Clusters (TPCs), 144 Streaming Multiprocessors (SMs), and a 384-bit memory interface with 12 32-bit memory controllers). On the diagram, each green dot represents a CUDA core (single precision), while the yellow are RT cores and blue Tensor cores. The cores are arranged in the slots called SMs in the figure. Cores in the same SM share some local and fast cache memory. GPCs The GPC is the dominant high-level hardware block. Each GPC includes a dedicated Raster Engine, two Raster Operations (ROPs) partitions, with each partition containing eight individual ROP units, and six TPCs. Each TPC includes one PolyMorph Engine and two SMs. Each SM contain 128 CUDA Cores, one Ada Third-Generation RT Core, four Ada Fourth-Generation Tensor Cores, four Texture Units, a 256 KB Register File, and 128 KB of L1/Shared Memory, which can be configured for different memory sizes depending on the needs of the graphics or compute workload. In a typical cluster, some GPUs are attached to a single node resulting in a CPU-GPU hybrid architecture. The CPU component is called the host and the GPU part the device. One possible layout Kebnekaise, AMD Zen4 node with L40s GPU Schematics of a hybrid CPU-GPU architecture. A GPU L40s card is attached to a NUMA island which in turn contains 24 cores (AMD Zen4 CPU node with 48 cores total). The NUMA island and the GPUs are connected through a PCI-E interconnect which makes the data transfer between both components rather slow. We can characterize the CPU and GPU performance with two quantities: the latency and the throughput . Latency refers to the time spent in a sole computation. Throughput denotes the number of computations that can be performed in parallel. Then, we can say that a CPU has low latency (able to do fast computations) but low throughput (only a few computations simultaneously). In the case of GPUs, the latency is high and the throughput is also high. We can visualize the behavior of the CPUs and GPUs with cars as in the figure below. A CPU would be compact road where only a few racing cars can drive whereas a GPU would be a broader road where plenty of slow cars can drive. Cars and roads analogy for the CPU and GPU behavior. The compact road is analogous to the CPU (low latency, low throughput) and the broader road is analogous to the GPU (high latency, high throughput). Not every program is suitable for GPU acceleration. GPUs process simple functions rapidly, and are best suited for repetitive and highly-parallel computing tasks. GPUs were originally designed to render high-resolution images and video concurrently and fast, but since they can perform parallel operations on multiple sets of data, they are also often used for other, non-graphical tasks. Common uses are machine learning and scientific computation were the GPUs can take advantage of massive parallelism. Many R packages are not CUDA aware, but some have been written specifically with GPUs in mind. Many Julia packages are not CUDA aware. The CUDA.jl package is the main programming interface for working with Nvidia GPUs. Matlab does have support for computing on GPUs, but you need to write functions that support GPU execution. Many functions in Matlab run automatically on a GPU if you supply a gpuArray data argument. GPU computing in MATLAB requires Parallel Computing Toolbox. Information here: Run MATLAB Functions on a GPU . One of the most common use of GPUs with and Julia is for machine learning or deep learning.","title":"Introduction"},{"location":"advanced/gpus/#gpus-on-c3se-uppmax-hpc2n-lunarc-nsc-and-pdc-systems","text":"There are generally either not GPUs on the login nodes or they cannot be accessed for computations. To use them you need to either launch an interactive job or submit a batch job. C3SE NSC PDC UPPMAX HPC2N LUNARC Alvis is meant for GPU jobs. There is no node-sharing on multi-node jobs ( --exclusive is automatic). NOTE: Requesting -N 1 does not mean 1 full node You would need to add this to your batch script: #SBATCH -p alvis #SBATCH -N <nodes> #SBATCH --gpus-per-node=<type>:x where <type> is one of V100 T4 A100 and x is number of GPU cards 1-4 for V100 1-8 for T4 1-4 for A100 Tetralith has Nvidia T4 GPUs. In order to access them, add this to your batch script or interactive job: #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 Dardel has AMD Instinct\u2122 MI250X GPU chips. In order to access them, add this to your batch script or interactive job: You need to add this to your batch script (or interactive job) in order to use them: #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu NOTE: the fact that Dardel has AMD GPUs means that CUDA-enabled packages will not run! You need something like hip. Rackham\u2019s compute nodes do not have GPUs. You need to use Snowy for that. The new cluster Pelle has GPUs. On Pelle, you need to use this batch command: for L40s GPUs (up to 10 GPU cards) #SBATCH -p gpu #SBATCH --gpus:l40s:<number of GPUs> or for H100 GPUs (up to 2 GPU cards) #SBATCH -p gpu #SBATCH --gpus=h100:<number of GPUs> Kebnekaise\u2019s GPU nodes are considered a separate resource, and the regular compute nodes do not have GPUs. Kebnekaise has a great many different types of GPUs: V100 (2 cards/node) A40 (8 cards/node) A6000 (2 cards/node) L40s (2 or 6 cards/node) A100 (2 cards/node) H100 (4 cards/node) MI100 (2 cards/node) To access them, you need to use this to the batch system: #SBATCH --gpus=x where x is the number of GPU cards you want. Above are given how many are on each type, so you can ask for up to that number. In addition, you need to add this to the batch system: #SBATCH -C <type> where type is v100 a40 a6000 l40s a100 h100 mi100 For more information, see HPC2N\u2019s guide to the different parts of the batch system: https://docs.hpc2n.umu.se/documentation/batchsystem/resources/ LUNARC has Nvidia A100 GPUs and Nvidia A40 GPUs, but the latter ones are reserved for interactive graphics work on the on-demand system, and Slurm jobs should not be submitted to them. Thus in order to use the A100 GPUs on Cosmos, add this to your batch script: A100 GPUs on AMD nodes: #SBATCH -p gpua100 #SBATCH --gres=gpu:1 These nodes are configured as exclusive access and will not be shared between users. User projects will be charged for the entire node (48 cores). A job on a node will also have access to all memory on the node. A100 GPUs on Intel nodes: #SBATCH -p gpua100i #SBATCH --gres=gpu:<number> where <number> is 1 or 2 (Two of the nodes have 1 GPU and two have 2 GPUs).","title":"GPUs on C3SE, UPPMAX, HPC2N, LUNARC, NSC, and PDC systems"},{"location":"advanced/gpus/#deploying-gpus","text":"As mentioned, you need a batch job or an interactive job (could also be through Open OnDemand or from inside MATLAB) to use the GPUs.","title":"Deploying GPUs"},{"location":"advanced/gpus/#r","text":"Here follows an example of a batch script that allocates GPUs. This is for an R job, but it is done similarly for Julia and Matlab. NSC PDC C3SE UPPMAX HPC2N LUNARC #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A naiss2025-22-934 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for resources, including GPU resources #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 # Remove any loaded modules and load the ones we need module purge > /dev/null 2 > & 1 module load R/4.4.0-hpc1-gcc-11.3.0-bare R --no-save --no-restore -f MY-R-GPU-SCRIPT.R #!/bin/bash -l # Remember to change this to your own project ID after the course! #SBATCH -A naiss2025-22-934 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for resources, including GPU resources #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu module load PDC/23.12 R/4.4.1-cpeGNU-23.12 module load rocm/5.7.0 #module load craype-accel-amd-gfx90a #module load cpeGNU/23.12 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A NAISS2025-22-934 #SBATCH -t HHH:MM:SS #SBATCH -p alvis #SBATCH -N 1 --gpus-per-node=T4:4 ml purge > /dev/null 2 > & 1 module load R/4.2.1-foss-2022a CUDA/12.9.1 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R Rackham/Snowy : #!/bin/bash -l #SBATCH -A uppmax2025-Y-ZZZ #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS #SBATCH --exclusive #SBATCH -p node #SBATCH -N 1 #SBATCH -M snowy #SBATCH --gpus=1 #SBATCH --gpus-per-node=1 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 ml uppmax R/4.1.1 R_packages/4.1.1 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R Pelle (1 L40s) : #!/bin/bash -l #SBATCH -A uppmax2025-Y-ZZZ #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS #SBATCH -p gpu #SBATCH --gpus:l40s:1 #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 # Reloading a module that got removed with purge ml Java/17 ml R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS #Ask for GPU resources. You pick type as one of the ones shown above #x is how many cards you want, at most as many as shown above #SBATCH --gpus:x #SBATCH -C type #Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error ml purge > /dev/null 2 > & 1 #R version 4.4.1 ml GCC/13.2.0 R/4.4.1 OpenMPI/4.1.6 R-bundle-CRAN/2024.06 ml CUDA/12.6.0 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A lu2025-2-94 # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for GPU resources - x is how many cards, 1 or 2 #SBATCH -p gpua100 #SBATCH --gres=gpu:x # Remove any loaded modules and load the ones we need module purge > /dev/null 2 > & 1 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 CUDA/12.1.1 R --no-save --no-restore -f MY-R-GPU-SCRIPT.R","title":"R"},{"location":"advanced/gpus/#matlab","text":"Here we cover how to use GPUs with MATLAB Inside MATLAB In a batch script In order to use GPUs, you have to ask for them.","title":"MATLAB"},{"location":"advanced/gpus/#julia","text":"In order to use the NVIDIA GPUs with Julia (UPPMAX, HPC2N, and LUNARC), you will need to load a CUDA toolkit module on the cluster and install the CUDA package in Julia. In the case of AMD GPUs for Julia (PDC and HPC2N), you will need to load a ROCM toolkit module on the cluster and install the AMDGPU package in Julia as in the next sequence of commands. Prerequisites UPPMAX HPC2N LUNARC PDC NSC This can only be done on Snowy or Bianca. Then either create an interactive session or make a batch job CUDA is installed at system level so they do not need to be loaded. On snowy $ interactive -A <proj> -n 1 -M snowy --gres = gpu:1 -t 3 :00:00 $ ml Julia/1.8.5 # Julia version $ julia ( v1.8 ) pkg> add CUDA Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... Installed CEnum \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 v0.4.2 ... $ ml Julia/1.8.5-linux-x86_64 # Julia version $ ml CUDA/11.4.1 # CUDA toolkit module $ julia ( v1.8 ) pkg> add CUDA Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... Installed CEnum \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 v0.4.2 ... $ ml Julia/1.8.5-linux-x86_64 # Julia version $ ml CUDA/11.4.1 # CUDA toolkit module $ julia ( v1.8 ) pkg> add CUDA Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... Installed CEnum \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 v0.4.2 ... $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # Julia version $ ml rocm/5.7.0 craype-accel-amd-gfx90a # ROCM toolkit module $ julia ( v1.10 ) pkg> add AMDGPU Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... Installed CEnum \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 v0.4.2 $ interactive -A <proj> -n 1 -c 32 --gpus-per-task = 1 -t 1 :00:00 $ ml buildenv-gcccuda/11.6.2-gcc9-hpc1 # Load tool chain with CUDA $ ml julia/1.9.4-bdist # Julia version $ julia ( v1.9 ) pkg> add LinearAlgebra ( v1.9 ) pkg> add CUDA Updating registry at ` ~/.julia/registries/General.toml ` Resolving package versions... ... Once this initial setting is completed, you will be able to use the GPUs available on the cluster. Here, there is a simple example for computing a matrix-matrix multiplication. As a reference point, we show the simulation on CPUs as well. You can call the batch script job-gpu.sh , for instance. Running on NVIDIA GPUs UPPMAX HPC2N LUNARC NSC script-gpu.jl Short GPU example for running on Snowy. #!/bin/bash -l #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -M snowy #SBATCH -p node #SBATCH --gres=gpu:1 #SBATCH -N 1 #SBATCH --job-name=job-gpu # create a short name for your job #SBATCH --time=00:15:00 # total run time limit (HH:MM:SS) #SBATCH --qos=short # if test run t<15 min #SBATCH --mail-type=begin # send email when job begins #SBATCH --mail-type=end # send email when job ends module load julia/1.8.5 # system CUDA works as of today julia script-gpu.jl #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-gpu # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #SBATCH --gres=gpu:v100:1 # 1 GPU v100 card ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia script-gpu.jl #!/bin/bash #SBATCH -A lu202w-x-yz # your project_ID #SBATCH -J job-gpu # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #Asking for one A100 GPU #SBATCH -p gpua100 #SBATCH --gres=gpu:1 ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml CUDA/11.4.1 julia script-gpu.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job-gpu # name of the job #SBATCH -n 1 # nr. tasks #SBATCH -c 32 # nr. cores #SBATCH --gpus-per-task=1 # nr. GPU cards #SBATCH --time=00:04:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml buildenv-gcccuda/11.6.2-gcc9-hpc1 ml julia/1.9.4-bdist julia script-gpu.jl Julia GPU example code. using CUDA CUDA . versioninfo () N = 2 ^ 8 x = rand ( N , N ) y = rand ( N , N ) A = CuArray ( x ) B = CuArray ( y ) # Calculation on CPU @time x * y # Calculation on GPU @time A * B # Calculation on CPU @time x * y # Calculation on GPU @time A * B Running on AMD GPUs PDC script-gpu.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p gpu # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # ROCM toolkit module ml rocm/5.7.0 craype-accel-amd-gfx90a julia script-gpu.jl Julia AMD GPU example code. using AMDGPU AMDGPU . versioninfo () # Display AMD GPU information N = 2 ^ 8 x = rand ( N , N ) y = rand ( N , N ) A = ROCArray ( x ) # Transfer data to AMD GPU B = ROCArray ( y ) # Calculation on CPU @time x * y # Calculation on AMD GPU @time A * B # Calculation on CPU (again) @time x * y # Calculation on AMD GPU (again) @time A * B","title":"Julia"},{"location":"advanced/parallelism_extra/","text":"Extra reading about parallelism \u00b6 Threaded programming \u00b6 To take advantage of the shared memory of the cores, threaded mechanisms can be used. Low-level programming languages, such as Fortran/C/C++, use OpenMP as the standard application programming interface (API) to parallelize programs by using a threaded mechanism. Here, all threads have access to the same data and can do computations simultaneously. From this we infer that without doing any modification to our code we can get the benefits from parallel computing by turning-on/off external libraries, by setting environment variables such as OMP_NUM_THREADS . Higher-level languages have their own mechanisms to generate threads and this can be confusing especially if the code is using external libraries, linear algebra for instance (LAPACK, BLAS, \u2026). These libraries have their own threads (OpenMP for example) and the code you are writing (R, Julia, Python, or Matlab) can also have some internal threded mechanism. - Here there are some examples (of many) of what you will need to pay attention when porting a parallel code from your laptop (or another HPC center) to our clusters: A common issue with shared memory programming is data racing which happens when different threads write on the same memory address. Language-specific === Julia The mechanism here is called `Julia threads` which is performant and can be activated by executing a script as follows ``julia --threads X script.jl``, where *X* is the number of threads. Code modifications are required to support the threads. === R R doesn't have a threaded mechanism as the other languages discussed in this course. Some functions provided by certain packages (parallel, doParallel, etc.), for instance, *foreach*, offer parallel features but memory is not shared across the workers. This could lead to [data replication](https://hpc2n.github.io/intro-course/software/#recommendations). === Matlab Starting from version 2020a, Matlab offers the [ThreadPool](https://se.mathworks.com/help/parallel-computing/parallel.threadpool.html) functionality that can leverage the power of threads sharing a common memory. This could potentially lead to a faster code compared to other schemes (Distributed discussed below) but notice that the code is not expected to support multi-node simulations. Demonstrations The idea is to parallelize a simple for loop (language-agnostic): for i start at 1 end at 4 wait 1 second end the for loop The waiting step is used to simulate a task without writing too much code. In this way, one can realize how faster the loop can be executed when threads are added: === Julia In the following example ``sleep-threads.jl`` the `sleep()` function is called `n` times first in serial mode and then by using `n` threads. The *BenchmarkTools* package help us to time the code (as this package is not in the base Julia installation you will need to install it). ```julia using BenchmarkTools using .Threads n = 4 # number of iterations function sleep_serial(n) #Serial version for i in 1:n sleep(1) end end @btime sleep_serial(n) evals=1 samples=1 function sleep_threaded(n) #Parallel version @threads for i = 1:n sleep(1) end end @btime sleep_threaded(n) evals=1 samples=1 ``` First load the Julia module ``ml Julia/1.8.5-linux-x86_64`` and then run the script with the command ``srun -A \"your-project\" -n 1 -c 4 -t 00:05:00 julia --threads 4 sleep-threads.jl`` to use 4 Julia threads. We can also use the *Distributed* package that allows the scaling of simulations beyond a single node (call the script ``sleep-distributed.jl``): ```julia using BenchmarkTools using Distributed n = 4 # number of iterations function sleep_parallel(n) @sync @distributed for i in 1:n sleep(1) end end @btime sleep_parallel(n) evals=1 samples=1 ``` Run the script with the command ``srun -A \"your-project\" -n 1 -c 4 -t 00:05:00 julia -p 4 sleep-distributed.jl`` to use 4 Julia processes. === R In the following example ``sleep.R`` the `Sys.sleep()` function is called `n` times first in serial mode and then by using `n` processes. Start by loading the modules ``ml GCC/12.2.0 OpenMPI/4.1.4 R/4.2.2`` ```r library(doParallel) # number of iterations = number of processes n <- 4 sleep_serial <- function(n) { for (i in 1:n) { Sys.sleep(1) } } serial_time <- system.time( sleep_serial(n) )[3] serial_time sleep_parallel <- function(n) { r <- foreach(i=1:n) %dopar% Sys.sleep(1) } cl <- makeCluster(n) registerDoParallel(cl) parallel_time <- system.time( sleep_parallel(n) )[3] stopCluster(cl) parallel_time ``` Run the script with the command ``srun -A \"your-project\" -n 1 -c 4 -t 00:05:00 Rscript --no-save --no-restore sleep.R``. === Matlab In Matlab one can use the function `pause()` to wait for some number of secods. The Matlab module we tested can be loaded as ``ml MATLAB/2023a.Update4``. ```matlab % Get a handler for the cluster c=parcluster('kebnekaise'); n = 4; % Number of iterations % Run the job with 1 worker and submit the job to the batch queue j = c.batch(@sleep_serial, 1, {4}, 'pool', 1); % Wait till the job has finished j.wait; % Fetch the result after the job has finished t = j.fetchOutputs{:}; fprintf('Time taken for serial version: %.2f seconds\\n', t); % Run the job with 4 worker and submit the job to the batch queue j = c.batch(@sleep_parallel, 1, {4}, 'pool', 4); % Wait till the job has finished j.wait; % Fetch the result after the job has finished t = j.fetchOutputs{:}; fprintf('Time taken for parallel version: %.2f seconds\\n', t); % Serial version function t_serial = sleep_serial(n) % Start timming tic; for i = 1:n pause(1); end t_serial = toc; % stop timing end % Parallel version function t_parallel = sleep_parallel(n) % Start timing tic; parfor i = 1:n pause(1); end t_parallel = toc; % stop timing end ``` You can run this code directly in the Matlab GUI. Distributed programming \u00b6 Although threaded programming is convenient because one can achieve considerable initial speedups with little code modifications, this approach does not scale for more than hundreds of cores. Scalability can be achieved with distributed programming. Here, there is not a common shared memory but the individual processes (notice the different terminology with threads in shared memory) have their own memory space. Then, if a process requires data from or should transfer data to another process, it can do that by using send and receive to transfer messages. A standard API for distributed computing is the Message Passing Interface (MPI). In general, MPI requires refactoring of your code.","title":"Extra reading about parallelism"},{"location":"advanced/parallelism_extra/#extra-reading-about-parallelism","text":"","title":"Extra reading about parallelism"},{"location":"advanced/parallelism_extra/#threaded-programming","text":"To take advantage of the shared memory of the cores, threaded mechanisms can be used. Low-level programming languages, such as Fortran/C/C++, use OpenMP as the standard application programming interface (API) to parallelize programs by using a threaded mechanism. Here, all threads have access to the same data and can do computations simultaneously. From this we infer that without doing any modification to our code we can get the benefits from parallel computing by turning-on/off external libraries, by setting environment variables such as OMP_NUM_THREADS . Higher-level languages have their own mechanisms to generate threads and this can be confusing especially if the code is using external libraries, linear algebra for instance (LAPACK, BLAS, \u2026). These libraries have their own threads (OpenMP for example) and the code you are writing (R, Julia, Python, or Matlab) can also have some internal threded mechanism. - Here there are some examples (of many) of what you will need to pay attention when porting a parallel code from your laptop (or another HPC center) to our clusters: A common issue with shared memory programming is data racing which happens when different threads write on the same memory address. Language-specific === Julia The mechanism here is called `Julia threads` which is performant and can be activated by executing a script as follows ``julia --threads X script.jl``, where *X* is the number of threads. Code modifications are required to support the threads. === R R doesn't have a threaded mechanism as the other languages discussed in this course. Some functions provided by certain packages (parallel, doParallel, etc.), for instance, *foreach*, offer parallel features but memory is not shared across the workers. This could lead to [data replication](https://hpc2n.github.io/intro-course/software/#recommendations). === Matlab Starting from version 2020a, Matlab offers the [ThreadPool](https://se.mathworks.com/help/parallel-computing/parallel.threadpool.html) functionality that can leverage the power of threads sharing a common memory. This could potentially lead to a faster code compared to other schemes (Distributed discussed below) but notice that the code is not expected to support multi-node simulations. Demonstrations The idea is to parallelize a simple for loop (language-agnostic): for i start at 1 end at 4 wait 1 second end the for loop The waiting step is used to simulate a task without writing too much code. In this way, one can realize how faster the loop can be executed when threads are added: === Julia In the following example ``sleep-threads.jl`` the `sleep()` function is called `n` times first in serial mode and then by using `n` threads. The *BenchmarkTools* package help us to time the code (as this package is not in the base Julia installation you will need to install it). ```julia using BenchmarkTools using .Threads n = 4 # number of iterations function sleep_serial(n) #Serial version for i in 1:n sleep(1) end end @btime sleep_serial(n) evals=1 samples=1 function sleep_threaded(n) #Parallel version @threads for i = 1:n sleep(1) end end @btime sleep_threaded(n) evals=1 samples=1 ``` First load the Julia module ``ml Julia/1.8.5-linux-x86_64`` and then run the script with the command ``srun -A \"your-project\" -n 1 -c 4 -t 00:05:00 julia --threads 4 sleep-threads.jl`` to use 4 Julia threads. We can also use the *Distributed* package that allows the scaling of simulations beyond a single node (call the script ``sleep-distributed.jl``): ```julia using BenchmarkTools using Distributed n = 4 # number of iterations function sleep_parallel(n) @sync @distributed for i in 1:n sleep(1) end end @btime sleep_parallel(n) evals=1 samples=1 ``` Run the script with the command ``srun -A \"your-project\" -n 1 -c 4 -t 00:05:00 julia -p 4 sleep-distributed.jl`` to use 4 Julia processes. === R In the following example ``sleep.R`` the `Sys.sleep()` function is called `n` times first in serial mode and then by using `n` processes. Start by loading the modules ``ml GCC/12.2.0 OpenMPI/4.1.4 R/4.2.2`` ```r library(doParallel) # number of iterations = number of processes n <- 4 sleep_serial <- function(n) { for (i in 1:n) { Sys.sleep(1) } } serial_time <- system.time( sleep_serial(n) )[3] serial_time sleep_parallel <- function(n) { r <- foreach(i=1:n) %dopar% Sys.sleep(1) } cl <- makeCluster(n) registerDoParallel(cl) parallel_time <- system.time( sleep_parallel(n) )[3] stopCluster(cl) parallel_time ``` Run the script with the command ``srun -A \"your-project\" -n 1 -c 4 -t 00:05:00 Rscript --no-save --no-restore sleep.R``. === Matlab In Matlab one can use the function `pause()` to wait for some number of secods. The Matlab module we tested can be loaded as ``ml MATLAB/2023a.Update4``. ```matlab % Get a handler for the cluster c=parcluster('kebnekaise'); n = 4; % Number of iterations % Run the job with 1 worker and submit the job to the batch queue j = c.batch(@sleep_serial, 1, {4}, 'pool', 1); % Wait till the job has finished j.wait; % Fetch the result after the job has finished t = j.fetchOutputs{:}; fprintf('Time taken for serial version: %.2f seconds\\n', t); % Run the job with 4 worker and submit the job to the batch queue j = c.batch(@sleep_parallel, 1, {4}, 'pool', 4); % Wait till the job has finished j.wait; % Fetch the result after the job has finished t = j.fetchOutputs{:}; fprintf('Time taken for parallel version: %.2f seconds\\n', t); % Serial version function t_serial = sleep_serial(n) % Start timming tic; for i = 1:n pause(1); end t_serial = toc; % stop timing end % Parallel version function t_parallel = sleep_parallel(n) % Start timing tic; parfor i = 1:n pause(1); end t_parallel = toc; % stop timing end ``` You can run this code directly in the Matlab GUI.","title":"Threaded programming"},{"location":"advanced/parallelism_extra/#distributed-programming","text":"Although threaded programming is convenient because one can achieve considerable initial speedups with little code modifications, this approach does not scale for more than hundreds of cores. Scalability can be achieved with distributed programming. Here, there is not a common shared memory but the individual processes (notice the different terminology with threads in shared memory) have their own memory space. Then, if a process requires data from or should transfer data to another process, it can do that by using send and receive to transfer messages. A standard API for distributed computing is the Message Passing Interface (MPI). In general, MPI requires refactoring of your code.","title":"Distributed programming"},{"location":"advanced/schedule/","text":"Schedule \u00b6 Time Topic Teacher(s) 9:00 Parallel computing: general R . Parallel computing: Thread parallelism R 10:00 Break . 10:15 Parallel computing: Thread parallelism R 11:00 Break . 11:15 Parallel computing: Distributed parallelism Bj 12:00 Break . 13:00 Big data and cluster managers Bj 13:20 Introduction to GPUs B 14:00 Break . 14:15 Machine learning (R, MATLAB, Julia) B+? 15:00 Break . 15:15 Machine learning cont\u2019d (R, MATLAB, Julia) B+? c. 15:30 Wrap-up + Q&A Several Teachers: B : Birgitte Bryds\u00f6, Bj : Bj\u00f6rn Claremar, R : Rich\u00e8l Bilderbeek","title":"Schedule"},{"location":"advanced/schedule/#schedule","text":"Time Topic Teacher(s) 9:00 Parallel computing: general R . Parallel computing: Thread parallelism R 10:00 Break . 10:15 Parallel computing: Thread parallelism R 11:00 Break . 11:15 Parallel computing: Distributed parallelism Bj 12:00 Break . 13:00 Big data and cluster managers Bj 13:20 Introduction to GPUs B 14:00 Break . 14:15 Machine learning (R, MATLAB, Julia) B+? 15:00 Break . 15:15 Machine learning cont\u2019d (R, MATLAB, Julia) B+? c. 15:30 Wrap-up + Q&A Several Teachers: B : Birgitte Bryds\u00f6, Bj : Bj\u00f6rn Claremar, R : Rich\u00e8l Bilderbeek","title":"Schedule"},{"location":"advanced/summary/","text":"Summary advanced day \u00b6 Parallel computing \u00b6 flowchart TD subgraph hpc_cluster[HPC cluster] subgraph node_1[Node] subgraph cpu_1_1[CPU] core_1_1_1[Core] end end end Simplified HPC cluster architecture Extent Parallelism Core Single-threaded Node Thread parallelism HPC cluster Distributed parallelism Types of parallelism, relevant to this courses Amdahl\u2019s law: the maximum speedup of code by parallelization is constrained by the code that cannot be run in parallel. Thread parallelism \u00b6 Language Keyword to indicate a parallel calculation Julia Threads.@threads MATLAB parfor R %dopar% Total core seconds Efficiency Speedup Benchmark: the total core seconds per number of workers Benchmark: Efficiency per number of workers Benchmark: Speedup per number of workers Singlethreaded code makes the best use of your computational resources HPC clusters differ Languages differ MATLAB does not use multithreading at all: it shows how a misconfigured job looks like No need to learn a different \u2018faster\u2019 programming language, as the variance within a programming language is bigger than variance between languages (adapted fig 2, from [Prechelt, 2000] ). Instead, get good in the one you already know Distributed parallelism \u00b6 Learning outcomes I can schedule jobs with distributed parallelism I know the basic difference between threads and distributed memory in terms of memory share I can explain how Julia/MATLAB/R code makes use of distributed parallelism Summary Memory and processes are distributed among tasks Information sent between tasks only on demand Native packages own syntax and run with -p MATLAB MPI external library MPI-like syntax and run with mpirun -np command or similar. Big data \u00b6 Learning outcomes I understand how I can work with big data I know where to find more information about big data Summary There are packages for all languages Distributing parts of arrays \u201cLazy computing\u201d Effective storage Different file formats suits diffeent types of data HDF5/NetCDF lets you load parts of the file into memory Allocating memory Allocate more cores Use Slurm commands --mem=<size>GB or --mem-per-gpu=<size>GB Introduction to GPUs \u00b6 Learning outcomes I can explain the difference between a CPU and a GPU I can use GPUs with my language Summary GPUs process simple functions rapidly, and are best suited for repetitive and highly-parallel computing tasks There are GPUs on NSC/Tetralith, PDC/Dardel, C3SE/Alvis, HPC2N/Kebnekaise, LUNARC/Cosmos, UPPMAX/Pelle, but they are different It varies between centres how you allocate a GPU You need to use either batch or interactive/OpenOnDemand to use GPUs Machine Learning \u00b6 Learning outcomes I can check if an ML package is installed with a module I can run ML code with my language Summary ML approaches Supervised learning (with training examples) classification regression Unsupervised learning (find structures in data)) clustering dimensionality reduction Reinforcement learning (take actions in different environment) real-time decisions game AIs Robot navigation When to use When tasks are too complex or dynamic for a traditional algorithm When you cannot define a set of rules to solve a problem, like image recognition and spam detection When you have tasks involving large amounts of unstructured data (images, audio, etc.) When you need to be able to easily adapt to new information over time","title":"Summary"},{"location":"advanced/summary/#summary-advanced-day","text":"","title":"Summary advanced day"},{"location":"advanced/summary/#parallel-computing","text":"flowchart TD subgraph hpc_cluster[HPC cluster] subgraph node_1[Node] subgraph cpu_1_1[CPU] core_1_1_1[Core] end end end Simplified HPC cluster architecture Extent Parallelism Core Single-threaded Node Thread parallelism HPC cluster Distributed parallelism Types of parallelism, relevant to this courses Amdahl\u2019s law: the maximum speedup of code by parallelization is constrained by the code that cannot be run in parallel.","title":"Parallel computing"},{"location":"advanced/summary/#thread-parallelism","text":"Language Keyword to indicate a parallel calculation Julia Threads.@threads MATLAB parfor R %dopar% Total core seconds Efficiency Speedup Benchmark: the total core seconds per number of workers Benchmark: Efficiency per number of workers Benchmark: Speedup per number of workers Singlethreaded code makes the best use of your computational resources HPC clusters differ Languages differ MATLAB does not use multithreading at all: it shows how a misconfigured job looks like No need to learn a different \u2018faster\u2019 programming language, as the variance within a programming language is bigger than variance between languages (adapted fig 2, from [Prechelt, 2000] ). Instead, get good in the one you already know","title":"Thread parallelism"},{"location":"advanced/summary/#distributed-parallelism","text":"Learning outcomes I can schedule jobs with distributed parallelism I know the basic difference between threads and distributed memory in terms of memory share I can explain how Julia/MATLAB/R code makes use of distributed parallelism Summary Memory and processes are distributed among tasks Information sent between tasks only on demand Native packages own syntax and run with -p MATLAB MPI external library MPI-like syntax and run with mpirun -np command or similar.","title":"Distributed parallelism"},{"location":"advanced/summary/#big-data","text":"Learning outcomes I understand how I can work with big data I know where to find more information about big data Summary There are packages for all languages Distributing parts of arrays \u201cLazy computing\u201d Effective storage Different file formats suits diffeent types of data HDF5/NetCDF lets you load parts of the file into memory Allocating memory Allocate more cores Use Slurm commands --mem=<size>GB or --mem-per-gpu=<size>GB","title":"Big data"},{"location":"advanced/summary/#introduction-to-gpus","text":"Learning outcomes I can explain the difference between a CPU and a GPU I can use GPUs with my language Summary GPUs process simple functions rapidly, and are best suited for repetitive and highly-parallel computing tasks There are GPUs on NSC/Tetralith, PDC/Dardel, C3SE/Alvis, HPC2N/Kebnekaise, LUNARC/Cosmos, UPPMAX/Pelle, but they are different It varies between centres how you allocate a GPU You need to use either batch or interactive/OpenOnDemand to use GPUs","title":"Introduction to GPUs"},{"location":"advanced/summary/#machine-learning","text":"Learning outcomes I can check if an ML package is installed with a module I can run ML code with my language Summary ML approaches Supervised learning (with training examples) classification regression Unsupervised learning (find structures in data)) clustering dimensionality reduction Reinforcement learning (take actions in different environment) real-time decisions game AIs Robot navigation When to use When tasks are too complex or dynamic for a traditional algorithm When you cannot define a set of rules to solve a problem, like image recognition and spam detection When you have tasks involving large amounts of unstructured data (images, audio, etc.) When you need to be able to easily adapt to new information over time","title":"Machine Learning"},{"location":"advanced/distributed_parallelism/","text":"Distributed parallelism \u00b6 Learning outcomes I can schedule jobs with distributed parallelism I know the basic difference between threads and distributed memory in terms of memory share I can explain how Julia/MATLAB/R code makes use of distributed parallelism Why distributed parallelism is important \u00b6 Type of parallelism Number of cores Number of nodes Memory Library Single-threaded 1 1 As given by operating system None Threaded/shared memory Multiple 1 Shared by all cores OpenMP Distributed Multiple 1 or Multiple Distributed Language package or MPI (Example OpenMPI/MPICH) What is distributed parallelism \u00b6 Although threaded programming is convenient because one can achieve considerable initial speedups with little code modifications, this approach does not scale for more than hundreds of cores. Scalability can be achieved with distributed programming. Here, there is not a common shared memory but the individual processes (notice the different terminology with threads in shared memory) have their own memory space. Then, if a process requires data from or should transfer data to another process, it can do that by using send and receive to transfer messages. Must be used for a job that use many different nodes, for example, a weather prediction. Can also be used within a node. Types \u00b6 A standard API for distributed computing is the Message Passing Interface (MPI) . In general, MPI requires refactoring of your code. External libraries (loaded as modules on the cluster) There are two common versions OpenMPI MPICH (on Dardel) In the distributed parallelization scheme the workers (processes) can share some common memory but they can also exchange information by sending and receiving messages for instance. This is often built-in or written in the actual language (R, MATLAB, Julia Summary \u00b6 Distributed memory Tasks doing individual work Memory sent \u201con-demand\u201d between the tasks with rather small \u201cpackages\u201d Suitable for not very memory-dependent tasks Distributed programming, 2 implementations Native language, packages called \u201cdistributed\u201d or \u201cparallel\u201d (or similar) Message Passing Interface (MPI) relying on external libraries Key words tasks processes workers Important Cores are physical units on a node. In some clusters where with \u201chyperthreading\u201d is activated one can have 2 threads per core In theses cases Slurm counts these \u201csub-cores\u201d On a core (physical or hyper) you can define 1 thread (shared memory) or 1 task (distributed memory) Hybrid parallelism combines threading and distributed (not covered here) Example: One node with 20 cores uses 5 tasks \u00e1 4 threads, each thread occupying 1 core. Read more Courses from Aalto University, Finland MPI parallelism: multi-task programs Older explanation (video) Newer explanation (video) More details for the MPI parallelization scheme in Python can be found in a previous MPI course offered by some of us. MATLAB: choose between threads and processes Arnold (at the left): a robot that was controlled by MPI How it is used in programming languages of this course? \u00b6 Language-specific nuances for distributed programming R Matlab Julia R doesn\u2019t have a multiprocessing mechanism as the other languages discussed in this course. Some functions provided by certain packages (parallel, doParallel, etc.), for instance, foreach , offer parallel features. The processes generated by these functions have their own workspace which could lead to data replication . MPI is supported in R through the Rmpi package. In Matlab one can use the parpool('my-cluster',X) where X is the number of workers. The total number of processes spawned will always be X+1 where the extra process handles the overhead for the rest. See the documentation for parpool from MatWorks. Matlab doesn\u2019t support MPI function calls in Matlab code, it could be used indirectly through mex functions though. The mechanism here is called Julia processes which can be activated by executing a script as follows julia -p X script.jl , where X is the number of processes. Code modifications are required to support the workers. Julia also supports MPI through the package MPI.jl . Packages and syntax R MATLAB Julia Packages parallel doParallel Rmpi pdbMPI on Dardel Syntax for parallel/doParallel makeCluster registerDoParallel foreach stopCluster Some syntax for MPI mpi.universe Syntax parpool parcluster parfor parfeval spmd Packages Distributed (native to Julia) Convenient Not difficult to code SharedArrays MPI Syntax for distributed/shared arrays addprocs(nworkers) sharedVector @everywhere @sync @distributed @spawn Allocating distributed jobs \u00b6 Use --ntasks=<number> or -n=<number> Use srun if you run a script. Use -p <number of processes> together with julia or R to start a language shell using having access to several cores. Use an MPI or \u201cdistributed\u201d version of your software: a \u2018regular\u2019 non-MPI/distributed version will just run the same command serially but perhaps many times at once! Use mpirun if it is real MPI. Warning Check if the resources that you allocated are being used properly. Monitor the usage of hardware resources with tools offered at your HPC center, for instance job-usage at HPC2N . Here there are some examples (of many) of what you will need to pay attention when porting a parallel code from your laptop (or another HPC center) to our clusters: HPC2N Other We have a tool to monitor the usage of resources called: job-usage at HPC2N . If you are in a interactive node session the top command will give you information of the resources usage. Exercises \u00b6 Important Compute allocations in this workshop Pelle/Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934 Storage space for this workshop Rackham: /proj/r-matlab-julia-pelle Kebnekaise: /proj/nobackup/fall-courses Tetralith: /proj/courses-fall-2025/users/ Dardel: /cfs/klemming/projects/snic/courses-fall-2025 Alvis: /mimer/NOBACKUP/groups/courses-fall-2025/ Running parallel with distributed libraries In this exercise we will run a parallelized code that performs a 2D integration: \\int^{\\pi}_{0}\\int^{\\pi}_{0}\\sin(x+y)dxdy = 0 One way to perform the integration is by creating a grid in the x and y directions. More specifically, one divides the integration range in both directions into n bins. Julia R Matlab Here is a parallel code using the Distributed package in Julia (call it integration2d_distributed.jl ): integration2d_distributed.jl using Distributed using SharedArrays using LinearAlgebra using Printf using Dates # Add worker processes (replace with actual number of cores you want to use) nworkers = * FIXME * addprocs ( nworkers ) # Grid size n = 20000 # Number of processes numprocesses = nworkers # Shared array to store partial sums for each process partial_integrals = SharedVector { Float64 }( numprocesses ) # Function for 2D integration using multiprocessing # the decorator @everywher instruct Julia to transfer this function to all workers @everywhere function integration2d_multiprocessing ( n , numprocesses , processindex , partial_integrals ) # Interval size (same for X and Y) h = \u03c0 / n # Cumulative variable mysum = 0.0 # Workload for each process workload = div ( n , numprocesses ) # Define the range of work for each process according to index begin_index = workload * ( processindex - 1 ) + 1 end_index = workload * processindex # Regular integration in the X axis for i in begin_index : end_index x = h * ( i - 0.5 ) # Regular integration in the Y axis for j in 1 : n y = h * ( j - 0.5 ) mysum += sin ( x + y ) end end # Store the result in the shared array partial_integrals [ processindex ] = h ^ 2 * mysum end # function for main function main () # Start the timer starttime = now () # Distribute tasks to processes @sync for i in 1 : numprocesses @spawnat i integration2d_multiprocessing ( n , numprocesses , i , partial_integrals ) end # Calculate the total integral by summing over partial integrals integral = sum ( partial_integrals ) # end timing endtime = now () # Output results println ( \"Integral value is $ ( integral ) , Error is $ ( abs ( integral - 0.0 )) \" ) println ( \"Time spent: $ ( Dates . value ( endtime - starttime ) / 1000 ) sec\" ) end # Run the main function main () Run the code with the following batch script. job.sh UPPMAX HPC2N LUNARC PDC NSC #!/bin/bash -l #SBATCH -A naiss202X-XY-XYZ # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks/coresw #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.8.5 julia integration2d_distributed.jl #!/bin/bash #SBATCH -A hpc2n202x-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 ml Julia/1.9.3-linux-x86_64 julia integration2d_distributed.jl #!/bin/bash #SBATCH -A lu202X-XX-XX # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # reservation (optional) #SBATCH --reservation=RPJM-course*FIXME* ml purge > /dev/null 2 > & 1 ml Julia/1.9.3-linux-x86_64 julia integration2d_distributed.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=*FIXME* # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 julia integration2d_distributed.jl ```bash #!/bin/bash #SBATCH -A naiss202t-uv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load any modules you need, here for Julia ml julia/1.9.4-bdist julia integration2d_distributed.jl ``` Try different number of cores for this batch script ( FIXME string) using the sequence: 1, 2, 4, 8, 12, and 14. Note: this number should match the number of processes (also a FIXME string) in the Julia script. Collect the timings that are printed out in the job.*.out . According to these execution times what would be the number of cores that gives the optimal (fastest) simulation? Challenge: Increase the grid size ( n ) to 100000 and submit the batch job with 4 workers (in the Julia script) and request 5 cores in the batch script. Monitor the usage of resources with tools available at your center, for instance top (UPPMAX) or job-usage (HPC2N). Here is a parallel code using the parallel and doParallel packages in R (call it integration2d.R ). Note: check if those packages are already installed for the required R version, otherwise install them with install.packages() . The recommended R version for this exercise is ml GCC/12.2.0 OpenMPI/4.1.4 R/4.2.2 (HPC2N). integration2d.R library ( parallel ) library ( doParallel ) # nr. of workers/cores that will solve the tasks nworkers <- * FIXME * # grid size n <- 840 # Function for 2D integration (non-optimal implementation) integration2d <- function ( n , numprocesses , processindex ) { # Interval size (same for X and Y) h <- pi / n # Cumulative variable mysum <- 0.0 # Workload for each process workload <- floor ( n / numprocesses ) # Define the range of work for each process according to index begin_index <- workload * ( processindex - 1 ) + 1 end_index <- workload * processindex # Regular integration in the X axis for ( i in begin_index : end_index ) { x <- h * ( i - 0.5 ) # Regular integration in the Y axis for ( j in 1 : n ) { y <- h * ( j - 0.5 ) mysum <- mysum + sin ( x + y ) } } # Return the result return ( h ^ 2 * mysum ) } # Set up the cluster for doParallel cl <- makeCluster ( nworkers ) registerDoParallel ( cl ) # Start the timer starttime <- Sys.time () # Distribute tasks to processes and combine the outputs into the results list results <- foreach ( i = 1 : nworkers , .combine = c ) %dopar% { integration2d ( n , nworkers , i ) } # Calculate the total integral by summing over partial integrals integral <- sum ( results ) # End the timing endtime <- Sys.time () # Print out the result print ( paste ( \"Integral value is\" , integral , \"Error is\" , abs ( integral - 0.0 ))) print ( paste ( \"Time spent:\" , difftime ( endtime , starttime , units = \"secs\" ), \"seconds\" )) # Stop the cluster after computation stopCluster ( cl ) Run the code with the following batch script. job.sh UPPMAX HPC2N #!/bin/bash -l #SBATCH -A naiss202u-wv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks/coresw #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml R_packages/4.1.1 Rscript --no-save --no-restore integration2d.R #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 ml GCC/12.2.0 OpenMPI/4.1.4 R/4.2.2 Rscript --no-save --no-restore integration2d.R === \u201cLUNARC ```sh #!/bin/bash #SBATCH -A lu202u-wy-yz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #SBATCH --reservation=RPJM-course*FIXME* # reservation (optional) ml purge > /dev/null 2>&1 ml GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 Rscript --no-save --no-restore integration2d.R ``` === \u201cPDC ```bash #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=*FIXME* # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and R version ml ... Rscript --no-save --no-restore integration2d.R ``` === \u201cNSC ```bash #!/bin/bash #SBATCH -A naiss202t-uv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load any modules you need, here for R ml R/4.4.0-hpc1-gcc-11.3.0-bare Rscript --no-save --no-restore integration2d.R ``` Try different number of cores for this batch script ( FIXME string) using the sequence: 1,2,4,8,12, and 14. Note: this number should match the number of processes (also a FIXME string) in the R script. Collect the timings that are printed out in the job.*.out . According to these execution times what would be the number of cores that gives the optimal (fastest) simulation? Challenge: Increase the grid size ( n ) to 10000 and submit the batch job with 4 workers (in the R script) and request 5 cores in the batch script. Monitor the usage of resources with tools available at your center, for instance top (UPPMAX) or job-usage (HPC2N). Here is a parallel code using the parfor tool from Matlab (call it integration2d.m ). integration2d.m % Number of workers/processes num_workers = * FIXME * ; % Use parallel pool with 'parfor' parpool ( 'profile-name' , num_workers ); % Start parallel pool with num_workers workers % Grid size n = 6720 ; % bin size h = pi / n ; tic ; % Start timer % Shared variable to collect partial sums partial_integrals = 0.0 ; % In Matlab one can use parfor to parallelize loops parfor i = 1 : n partial_integrals = partial_integrals + integration2d_partial ( n , i ); end % Compute the integrals by multilpying by the bin size integral = partial_integrals * h ^ 2 ; elapsedTime = toc ; % Stop timer fprintf ( \"Integral value is %e\\n\" , integral ); fprintf ( \"Error is %e\\n\" , abs ( integral - 0.0 )); fprintf ( \"Time spent: %.2f sec\\n\" , elapsedTime ); % Clean up the parallel pool delete ( gcp ( 'nocreate' )); % Function for the 2D integration only computes a single bin function mysum = integration2d_partial ( n,i ) % bin size h = pi / n ; % Partial summation mysum = 0.0 ; % A single bin is computed x = h * ( i - 0.5 ); % Regular integration in the Y axis for j = 1 : n y = h * ( j - 0.5 ); mysum = mysum + sin ( x + y ); end end You can run directly this script from the Matlab GUI. Try different number of cores for this batch script ( FIXME string) using the sequence: 1, 2, 4, 8, 12, and 14. Collect the timings that are printed out in the Matlab command window. According to these execution times what would be the number of cores that gives the optimal (fastest) simulation? Challenge: Increase the grid size ( n ) to 100000 and submit the batch job with 4 workers. Monitor the usage of resources with tools available at your center, for instance top (UPPMAX), job-usage (HPC2N), or if you\u2019re working in the GUI (e.g. on LUNARC), you can click Parallel and then Monitor Jobs . For job-usage , you can see the job ID if you type squeue --me on a terminal on Kebnekaise. Extra exercises covering MPI \u00b6 Julia: Find the difference in coding \u00b6 In this exercise we will run a parallelized code that performs the 2D integration used above: \\int^{\\pi}_{0}\\int^{\\pi}_{0}\\sin(x+y)dxdy = 0 Note the differences in writing the code MPI usually needs more rewriting Julia scripts serial.jl threaded.jl distributed.jl mpi.jl # nr. of grid points n = 100000 function integration2d_julia ( n ) # interval size h = \u03c0 / n # cummulative variable mysum = 0.0 # regular integration in the X axis for i in 0 : n - 1 x = h * ( i + 0.5 ) # regular integration in the Y axis for j in 0 : n - 1 y = h * ( j + 0.5 ) mysum = mysum + sin ( x + y ) end end return mysum * h * h end res = integration2d_julia ( n ) println ( res ) using . Threads # nr. of grid points n = 100000 # nr. of threads numthreads = nthreads () # array for storing partial sums from threads partial_integrals = zeros ( Float64 , numthreads ) function integration2d_julia_threaded ( n , numthreads , threadindex ) # interval size h = \u03c0 / convert ( Float64 , n ) # cummulative variable mysum = 0.0 # workload for each thread workload = convert ( Int64 , n / numthreads ) # lower and upper integration limits for each thread lower_lim = workload * ( threadindex - 1 ) upper_lim = workload * threadindex - 1 ## regular integration in the X axis for i in lower_lim : upper_lim x = h * ( i + 0.5 ) # regular integration in the Y axis for j in 0 : n - 1 y = h * ( j + 0.5 ) mysum = mysum + sin ( x + y ) end end partial_integrals [ threadindex ] = mysum * h * h return end # The threads can compute now the partial summations @threads for i in 1 : numthreads integration2d_julia_threaded ( n , numthreads , threadid ()) end # The main thread now reduces the array total_sum = sum ( partial_integrals ) println ( \"The integral value is $total_sum \" ) @everywhere begin using Distributed using SharedArrays end # nr. of grid points n = 100000 # nr. of workers numworkers = nworkers () # array for storing partial sums from workers partial_integrals = SharedArray ( zeros ( Float64 , numworkers ) ) @everywhere function integration2d_julia_distributed ( n , numworkers , workerid , A :: SharedArray ) # interval size h = \u03c0 / convert ( Float64 , n ) # cummulative variable mysum = 0.0 # workload for each worker workload = convert ( Int64 , n / numworkers ) # lower and upper integration limits for each thread lower_lim = workload * ( workerid - 2 ) upper_lim = workload * ( workerid - 1 ) - 1 # regular integration in the X axis for i in lower_lim : upper_lim x = h * ( i + 0.5 ) # regular integration in the Y axis for j in 0 : n - 1 y = h * ( j + 0.5 ) mysum = mysum + sin ( x + y ) end end A [ workerid - 1 ] = mysum * h * h return end # The workers can compute now the partial summations @sync @distributed for i in 1 : numworkers integration2d_julia_distributed ( n , numworkers , myid (), partial_integrals ) end # The main process now reduces the array total_sum = sum ( partial_integrals ) println ( \"The integral value is $total_sum \" ) using MPI MPI . Init () # Initialize the communicator comm = MPI . COMM_WORLD # Get the ranks of the processes rank = MPI . Comm_rank ( comm ) # Get the size of the communicator size = MPI . Comm_size ( comm ) # root process root = 0 # nr. of grid points n = 100000 function integration2d_julia_mpi ( n , numworkers , workerid ) # interval size h = \u03c0 / convert ( Float64 , n ) # cummulative variable mysum = 0.0 # workload for each worker workload = div ( n , numworkers ) # lower and upper integration limits for each thread lower_lim = workload * workerid upper_lim = workload * ( workerid + 1 ) - 1 # regular integration in the X axis for i in lower_lim : upper_lim x = h * ( i + 0.5 ) # regular integration in the Y axis for j in 0 : n - 1 y = h * ( j + 0.5 ) mysum = mysum + sin ( x + y ) end end partial_integrals = mysum * h * h return partial_integrals end # The workers can compute now the partial summations p = integration2d_julia_mpi ( n , size , rank ) # The root process now reduces the array integral = MPI . Reduce ( p , + , root , comm ) if rank == root println ( \"The integral value is $integral \" ) end MPI . Finalize () Batch scripts The corresponding batch scripts for these examples are given here: UPPMAX HPC2N LUNARC PDC NSC serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash -l #SBATCH -A naiss202t-uv-wxyz #SBATCH -J job #SBATCH -n 1 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml julia/1.8.5 # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml julia/1.8.5 # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml julia/1.8.5 # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml julia/1.8.5 ml gcc/11.3.0 openmpi/4.1.3 # \"time\" command is optional # export the PATH of the Julia MPI wrapper export PATH = ~/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash #SBATCH -A hpc2n202w-xyz #SBATCH -J job #SBATCH -n 1 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A hpc2n202w-xyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A hpc2n202w-xyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A hpc2n202w-xyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml foss/2021b # export the PATH of the Julia MPI wrapper export PATH = /home/u/username/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash #SBATCH -A lu202w-x-yz #SBATCH -J job #SBATCH -n 1 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A lu202w-x-yz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A lu202w-x-yz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A lu202w-x-yz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml foss/2021b # export the PATH of the Julia MPI wrapper export PATH = /home/u/username/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=8 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=8 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=8 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # export the PATH of the Julia MPI wrapper export PATH = /cfs/klemming/home/u/username/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 1 # nr. of tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml julia/1.9.4-bdist # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 8 # nr. of tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml julia/1.9.4-bdist # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 8 # nr. of tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml julia/1.9.4-bdist # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 8 # nr. of tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml buildtool-easybuild/4.8.0-hpce082752a2 foss/2023b ml julia/1.9.4-bdist # export the PATH of the Julia MPI wrapper export PATH = /home/username/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl R: RMPI \u00b6 RMPI Short parallel example using package \u201cRmpi\u201d (\u201cpbdMPI on Dardel\u201d) UPPMAX HPC2N LUNARC NSC PDC Rmpi.R pbdMPI.R Short parallel example (using package \u201cRmpi\u201d, so we need to load both the module R/4.4.2-gfbf-2024a and the module R-bundle-CRAN/2024.11-foss-2024a. A suitable openmpi module, OpenMPI/5.0.3-GCC-13.3.0, is loaded with these.) #!/bin/bash -l #SBATCH -A uppmax2025-2-360 #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 8 export OMPI_MCA_mpi_warn_on_fork = 0 export OMPI_MCA_btl_openib_allow_ib = 1 ml purge > /dev/null 2 > & 1 ml R/4.4.2-gfbf-2024a ml OpenMPI/5.0.3-GCC-13.3.0 R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 mpirun -np 1 R CMD BATCH --no-save --no-restore Rmpi.R output.out Short parallel example (using packages \u201cRmpi\u201d). Loading R/4.4.1 and its prerequisites, as well as R-bundle-CRAN/2024.06 and its prerequisites. #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 8 export OMPI_MCA_mpi_warn_on_fork = 0 ml purge > /dev/null 2 > & 1 ml GCC/13.2.0 R/4.4.1 ml OpenMPI/4.1.6 R-bundle-CRAN/2024.06 mpirun -np 1 Rscript Rmpi.R Short parallel example (using packages \u201cRmpi\u201d). Loading R/4.2.1 and its prerequisites. #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID # Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 8 export OMPI_MCA_mpi_warn_on_fork = 0 ml purge > /dev/null 2 > & 1 ml GCC/11.3.0 OpenMPI/4.1.4 ml R/4.2.1 mpirun -np 1 R CMD BATCH --no-save --no-restore Rmpi.R output.out Short parallel example (using packages \u201cpbdMPI as \u201cRmpi\u201d does not work correctly on NSC). Loading R/4.2.2. Note: for NSC you first need to install \u201cpdbMPI\u201d ( module load R/4.2.2-hpc1-gcc-11.3.0-bare , start R , install.packages('pbdMPI') , pick CRAN mirror (Denmark, Finland, Sweden or other closeby)) #!/bin/bash #SBATCH -A naiss2025-22-934 # Asking for 15 min. #SBATCH -t 00:15:00 #SBATCH -n 8 #SBATCH --exclusive ml purge > /dev/null 2 > & 1 ml R/4.2.2-hpc1-gcc-11.3.0-bare srun --mpi = pmix Rscript pbdMPI.R Short parallel example (using packages \u201cpbdMPI\u201d). Loading R/4.4.1. Note: for PDC you first need to install \u201cpbdMPI\u201d (\u201cRmpi\u201d does not work). You can find the tarball in /cfs/klemming/projects/supr/courses-fall-2025/pbdMPI_0.5-4.tar.gz . Copy it to your own subdirectory under the project directory and then do: module load PDC/24.11 R/4.4.2-cpeGNU-24.11 R CMD INSTALL pbdMPI_0.5-4.tar.gz --configure-args=\" --with-mpi-include=/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/include --with-mpi-libpath=/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib --with-mpi-type=MPICH2\" --no-test-load #!/bin/bash -l #SBATCH -A naiss2025-22-934 # Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH --nodes 2 #SBATCH --ntasks-per-node=8 #SBATCH -p main #SBATCH --output=pbdMPI-test_%J.out # If you do ml purge you also need to restore the preloaded modules which you should have saved # when you logged in. Otherwise leave the two following lines outcommented. #ml purge > /dev/null 2>&1 #ml restore preload ml PDC/24.11 ml R/4.4.2-cpeGNU-24.11 srun -n 4 Rscript pbdMPI.R This R script uses package \u201cRmpi\u201d. # Load the R MPI package if it is not already loaded. if ( ! is.loaded ( \"mpi_initialize\" )) { library ( \"Rmpi\" ) } print ( mpi.universe.size ()) ns <- mpi.universe.size () - 1 mpi.spawn.Rslaves ( nslaves = ns ) # # In case R exits unexpectedly, have it automatically clean up # resources taken up by Rmpi (slaves, memory, etc...) .Last <- function (){ if ( is.loaded ( \"mpi_initialize\" )){ if ( mpi.comm.size ( 1 ) > 0 ){ print ( \"Please use mpi.close.Rslaves() to close slaves.\" ) mpi.close.Rslaves () } print ( \"Please use mpi.quit() to quit R\" ) .Call ( \"mpi_finalize\" ) } } # Tell all slaves to return a message identifying themselves mpi.remote.exec ( paste ( \"I am\" , mpi.comm.rank (), \"of\" , mpi.comm.size (), system ( \"hostname\" , intern = T ))) # Test computations x <- 5 x <- mpi.remote.exec ( rnorm , x ) length ( x ) x # Tell all slaves to close down, and exit the program mpi.close.Rslaves () mpi.quit () This R script uses package \u201cpbdMPI\u201d. library ( pbdMPI ) ns <- comm.size () # Tell all R sessions to return a message identifying themselves id <- comm.rank () ns <- comm.size () host <- system ( \"hostname\" , intern = TRUE ) comm.cat ( \"I am\" , id , \"on\" , host , \"of\" , ns , \"\\n\" , all.rank = TRUE ) # Test computations x <- 5 x <- rnorm ( x ) comm.print ( length ( x )) comm.print ( x , all.rank = TRUE ) finalize () Send the script to the batch system: sbatch <batch script> MATLAB: parfor and parfeval \u00b6 Challenge 1. Create and run a parallel code We have the following code in MATLAB that generates an array of 10000 random numbers and then the sum of all elements is stored in a variable called s : r = rand ( 1 , 10000 ); s = sum ( r ); We want now to repeat these steps (generating the numbers and taking the sum) 6 times so that the steps are run at the same time. Use parfor to parallelize these steps. Once your code is parallelized enclose it in a parpool section and send the job to the queue. Solution % Nr. of workers nworkers = 6 ; % Use parallel pool with 'parfor' parpool ( 'name-of-your-cluster' , nworkers ); % Start parallel pool with nworkers workers myarray = []; % Optional in this exercise to store partial results parfor i = 1 : nworkers r = rand ( 1 , 10000 ); s = sum ( r ); myarray = [ myarray , s ]; end myarray % print out the results from the workers % Clean up the parallel pool delete ( gcp ( 'nocreate' )); Challenge 2. Run a parallel code with batch MATLAB function The following function uses parfeval to do some computation (specifically it takes the average per-column of a matrix with a size nsize equal to 1000): function results = parfeval_mean ( nsize ) results = parfeval (@ mean , 1 , rand ( nsize )) end Place this function in a file called parfeval_mean.m and submit this function with the MATLAB batch command. Solution c = parcluster ( 'name-of-your-cluster' ); j = c . batch (@ parfeval_mean , 1 ,{ 1000 }, 'pool' , 1 ); j . wait ; % wait for the results t = j . fetchOutputs {:}; % fetch the results fprintf ( 'Name of host: %.5f \\n' , t ); % Print out the results More info HPC2N Julia documentation . White paper on Julia parallel computing . HPC2N R documentation . Wikipedias\u2019 article on Parallel Computing .","title":"Distributed parallelism"},{"location":"advanced/distributed_parallelism/#distributed-parallelism","text":"Learning outcomes I can schedule jobs with distributed parallelism I know the basic difference between threads and distributed memory in terms of memory share I can explain how Julia/MATLAB/R code makes use of distributed parallelism","title":"Distributed parallelism"},{"location":"advanced/distributed_parallelism/#why-distributed-parallelism-is-important","text":"Type of parallelism Number of cores Number of nodes Memory Library Single-threaded 1 1 As given by operating system None Threaded/shared memory Multiple 1 Shared by all cores OpenMP Distributed Multiple 1 or Multiple Distributed Language package or MPI (Example OpenMPI/MPICH)","title":"Why distributed parallelism is important"},{"location":"advanced/distributed_parallelism/#what-is-distributed-parallelism","text":"Although threaded programming is convenient because one can achieve considerable initial speedups with little code modifications, this approach does not scale for more than hundreds of cores. Scalability can be achieved with distributed programming. Here, there is not a common shared memory but the individual processes (notice the different terminology with threads in shared memory) have their own memory space. Then, if a process requires data from or should transfer data to another process, it can do that by using send and receive to transfer messages. Must be used for a job that use many different nodes, for example, a weather prediction. Can also be used within a node.","title":"What is distributed parallelism"},{"location":"advanced/distributed_parallelism/#types","text":"A standard API for distributed computing is the Message Passing Interface (MPI) . In general, MPI requires refactoring of your code. External libraries (loaded as modules on the cluster) There are two common versions OpenMPI MPICH (on Dardel) In the distributed parallelization scheme the workers (processes) can share some common memory but they can also exchange information by sending and receiving messages for instance. This is often built-in or written in the actual language (R, MATLAB, Julia","title":"Types"},{"location":"advanced/distributed_parallelism/#summary","text":"Distributed memory Tasks doing individual work Memory sent \u201con-demand\u201d between the tasks with rather small \u201cpackages\u201d Suitable for not very memory-dependent tasks Distributed programming, 2 implementations Native language, packages called \u201cdistributed\u201d or \u201cparallel\u201d (or similar) Message Passing Interface (MPI) relying on external libraries Key words tasks processes workers Important Cores are physical units on a node. In some clusters where with \u201chyperthreading\u201d is activated one can have 2 threads per core In theses cases Slurm counts these \u201csub-cores\u201d On a core (physical or hyper) you can define 1 thread (shared memory) or 1 task (distributed memory) Hybrid parallelism combines threading and distributed (not covered here) Example: One node with 20 cores uses 5 tasks \u00e1 4 threads, each thread occupying 1 core. Read more Courses from Aalto University, Finland MPI parallelism: multi-task programs Older explanation (video) Newer explanation (video) More details for the MPI parallelization scheme in Python can be found in a previous MPI course offered by some of us. MATLAB: choose between threads and processes Arnold (at the left): a robot that was controlled by MPI","title":"Summary"},{"location":"advanced/distributed_parallelism/#how-it-is-used-in-programming-languages-of-this-course","text":"Language-specific nuances for distributed programming R Matlab Julia R doesn\u2019t have a multiprocessing mechanism as the other languages discussed in this course. Some functions provided by certain packages (parallel, doParallel, etc.), for instance, foreach , offer parallel features. The processes generated by these functions have their own workspace which could lead to data replication . MPI is supported in R through the Rmpi package. In Matlab one can use the parpool('my-cluster',X) where X is the number of workers. The total number of processes spawned will always be X+1 where the extra process handles the overhead for the rest. See the documentation for parpool from MatWorks. Matlab doesn\u2019t support MPI function calls in Matlab code, it could be used indirectly through mex functions though. The mechanism here is called Julia processes which can be activated by executing a script as follows julia -p X script.jl , where X is the number of processes. Code modifications are required to support the workers. Julia also supports MPI through the package MPI.jl . Packages and syntax R MATLAB Julia Packages parallel doParallel Rmpi pdbMPI on Dardel Syntax for parallel/doParallel makeCluster registerDoParallel foreach stopCluster Some syntax for MPI mpi.universe Syntax parpool parcluster parfor parfeval spmd Packages Distributed (native to Julia) Convenient Not difficult to code SharedArrays MPI Syntax for distributed/shared arrays addprocs(nworkers) sharedVector @everywhere @sync @distributed @spawn","title":"How it is used in programming languages of this course?"},{"location":"advanced/distributed_parallelism/#allocating-distributed-jobs","text":"Use --ntasks=<number> or -n=<number> Use srun if you run a script. Use -p <number of processes> together with julia or R to start a language shell using having access to several cores. Use an MPI or \u201cdistributed\u201d version of your software: a \u2018regular\u2019 non-MPI/distributed version will just run the same command serially but perhaps many times at once! Use mpirun if it is real MPI. Warning Check if the resources that you allocated are being used properly. Monitor the usage of hardware resources with tools offered at your HPC center, for instance job-usage at HPC2N . Here there are some examples (of many) of what you will need to pay attention when porting a parallel code from your laptop (or another HPC center) to our clusters: HPC2N Other We have a tool to monitor the usage of resources called: job-usage at HPC2N . If you are in a interactive node session the top command will give you information of the resources usage.","title":"Allocating distributed jobs"},{"location":"advanced/distributed_parallelism/#exercises","text":"Important Compute allocations in this workshop Pelle/Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934 Storage space for this workshop Rackham: /proj/r-matlab-julia-pelle Kebnekaise: /proj/nobackup/fall-courses Tetralith: /proj/courses-fall-2025/users/ Dardel: /cfs/klemming/projects/snic/courses-fall-2025 Alvis: /mimer/NOBACKUP/groups/courses-fall-2025/ Running parallel with distributed libraries In this exercise we will run a parallelized code that performs a 2D integration: \\int^{\\pi}_{0}\\int^{\\pi}_{0}\\sin(x+y)dxdy = 0 One way to perform the integration is by creating a grid in the x and y directions. More specifically, one divides the integration range in both directions into n bins. Julia R Matlab Here is a parallel code using the Distributed package in Julia (call it integration2d_distributed.jl ): integration2d_distributed.jl using Distributed using SharedArrays using LinearAlgebra using Printf using Dates # Add worker processes (replace with actual number of cores you want to use) nworkers = * FIXME * addprocs ( nworkers ) # Grid size n = 20000 # Number of processes numprocesses = nworkers # Shared array to store partial sums for each process partial_integrals = SharedVector { Float64 }( numprocesses ) # Function for 2D integration using multiprocessing # the decorator @everywher instruct Julia to transfer this function to all workers @everywhere function integration2d_multiprocessing ( n , numprocesses , processindex , partial_integrals ) # Interval size (same for X and Y) h = \u03c0 / n # Cumulative variable mysum = 0.0 # Workload for each process workload = div ( n , numprocesses ) # Define the range of work for each process according to index begin_index = workload * ( processindex - 1 ) + 1 end_index = workload * processindex # Regular integration in the X axis for i in begin_index : end_index x = h * ( i - 0.5 ) # Regular integration in the Y axis for j in 1 : n y = h * ( j - 0.5 ) mysum += sin ( x + y ) end end # Store the result in the shared array partial_integrals [ processindex ] = h ^ 2 * mysum end # function for main function main () # Start the timer starttime = now () # Distribute tasks to processes @sync for i in 1 : numprocesses @spawnat i integration2d_multiprocessing ( n , numprocesses , i , partial_integrals ) end # Calculate the total integral by summing over partial integrals integral = sum ( partial_integrals ) # end timing endtime = now () # Output results println ( \"Integral value is $ ( integral ) , Error is $ ( abs ( integral - 0.0 )) \" ) println ( \"Time spent: $ ( Dates . value ( endtime - starttime ) / 1000 ) sec\" ) end # Run the main function main () Run the code with the following batch script. job.sh UPPMAX HPC2N LUNARC PDC NSC #!/bin/bash -l #SBATCH -A naiss202X-XY-XYZ # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks/coresw #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.8.5 julia integration2d_distributed.jl #!/bin/bash #SBATCH -A hpc2n202x-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 ml Julia/1.9.3-linux-x86_64 julia integration2d_distributed.jl #!/bin/bash #SBATCH -A lu202X-XX-XX # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # reservation (optional) #SBATCH --reservation=RPJM-course*FIXME* ml purge > /dev/null 2 > & 1 ml Julia/1.9.3-linux-x86_64 julia integration2d_distributed.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=*FIXME* # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 julia integration2d_distributed.jl ```bash #!/bin/bash #SBATCH -A naiss202t-uv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load any modules you need, here for Julia ml julia/1.9.4-bdist julia integration2d_distributed.jl ``` Try different number of cores for this batch script ( FIXME string) using the sequence: 1, 2, 4, 8, 12, and 14. Note: this number should match the number of processes (also a FIXME string) in the Julia script. Collect the timings that are printed out in the job.*.out . According to these execution times what would be the number of cores that gives the optimal (fastest) simulation? Challenge: Increase the grid size ( n ) to 100000 and submit the batch job with 4 workers (in the Julia script) and request 5 cores in the batch script. Monitor the usage of resources with tools available at your center, for instance top (UPPMAX) or job-usage (HPC2N). Here is a parallel code using the parallel and doParallel packages in R (call it integration2d.R ). Note: check if those packages are already installed for the required R version, otherwise install them with install.packages() . The recommended R version for this exercise is ml GCC/12.2.0 OpenMPI/4.1.4 R/4.2.2 (HPC2N). integration2d.R library ( parallel ) library ( doParallel ) # nr. of workers/cores that will solve the tasks nworkers <- * FIXME * # grid size n <- 840 # Function for 2D integration (non-optimal implementation) integration2d <- function ( n , numprocesses , processindex ) { # Interval size (same for X and Y) h <- pi / n # Cumulative variable mysum <- 0.0 # Workload for each process workload <- floor ( n / numprocesses ) # Define the range of work for each process according to index begin_index <- workload * ( processindex - 1 ) + 1 end_index <- workload * processindex # Regular integration in the X axis for ( i in begin_index : end_index ) { x <- h * ( i - 0.5 ) # Regular integration in the Y axis for ( j in 1 : n ) { y <- h * ( j - 0.5 ) mysum <- mysum + sin ( x + y ) } } # Return the result return ( h ^ 2 * mysum ) } # Set up the cluster for doParallel cl <- makeCluster ( nworkers ) registerDoParallel ( cl ) # Start the timer starttime <- Sys.time () # Distribute tasks to processes and combine the outputs into the results list results <- foreach ( i = 1 : nworkers , .combine = c ) %dopar% { integration2d ( n , nworkers , i ) } # Calculate the total integral by summing over partial integrals integral <- sum ( results ) # End the timing endtime <- Sys.time () # Print out the result print ( paste ( \"Integral value is\" , integral , \"Error is\" , abs ( integral - 0.0 ))) print ( paste ( \"Time spent:\" , difftime ( endtime , starttime , units = \"secs\" ), \"seconds\" )) # Stop the cluster after computation stopCluster ( cl ) Run the code with the following batch script. job.sh UPPMAX HPC2N #!/bin/bash -l #SBATCH -A naiss202u-wv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks/coresw #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml R_packages/4.1.1 Rscript --no-save --no-restore integration2d.R #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 ml GCC/12.2.0 OpenMPI/4.1.4 R/4.2.2 Rscript --no-save --no-restore integration2d.R === \u201cLUNARC ```sh #!/bin/bash #SBATCH -A lu202u-wy-yz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file #SBATCH --reservation=RPJM-course*FIXME* # reservation (optional) ml purge > /dev/null 2>&1 ml GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 Rscript --no-save --no-restore integration2d.R ``` === \u201cPDC ```bash #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=*FIXME* # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and R version ml ... Rscript --no-save --no-restore integration2d.R ``` === \u201cNSC ```bash #!/bin/bash #SBATCH -A naiss202t-uv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load any modules you need, here for R ml R/4.4.0-hpc1-gcc-11.3.0-bare Rscript --no-save --no-restore integration2d.R ``` Try different number of cores for this batch script ( FIXME string) using the sequence: 1,2,4,8,12, and 14. Note: this number should match the number of processes (also a FIXME string) in the R script. Collect the timings that are printed out in the job.*.out . According to these execution times what would be the number of cores that gives the optimal (fastest) simulation? Challenge: Increase the grid size ( n ) to 10000 and submit the batch job with 4 workers (in the R script) and request 5 cores in the batch script. Monitor the usage of resources with tools available at your center, for instance top (UPPMAX) or job-usage (HPC2N). Here is a parallel code using the parfor tool from Matlab (call it integration2d.m ). integration2d.m % Number of workers/processes num_workers = * FIXME * ; % Use parallel pool with 'parfor' parpool ( 'profile-name' , num_workers ); % Start parallel pool with num_workers workers % Grid size n = 6720 ; % bin size h = pi / n ; tic ; % Start timer % Shared variable to collect partial sums partial_integrals = 0.0 ; % In Matlab one can use parfor to parallelize loops parfor i = 1 : n partial_integrals = partial_integrals + integration2d_partial ( n , i ); end % Compute the integrals by multilpying by the bin size integral = partial_integrals * h ^ 2 ; elapsedTime = toc ; % Stop timer fprintf ( \"Integral value is %e\\n\" , integral ); fprintf ( \"Error is %e\\n\" , abs ( integral - 0.0 )); fprintf ( \"Time spent: %.2f sec\\n\" , elapsedTime ); % Clean up the parallel pool delete ( gcp ( 'nocreate' )); % Function for the 2D integration only computes a single bin function mysum = integration2d_partial ( n,i ) % bin size h = pi / n ; % Partial summation mysum = 0.0 ; % A single bin is computed x = h * ( i - 0.5 ); % Regular integration in the Y axis for j = 1 : n y = h * ( j - 0.5 ); mysum = mysum + sin ( x + y ); end end You can run directly this script from the Matlab GUI. Try different number of cores for this batch script ( FIXME string) using the sequence: 1, 2, 4, 8, 12, and 14. Collect the timings that are printed out in the Matlab command window. According to these execution times what would be the number of cores that gives the optimal (fastest) simulation? Challenge: Increase the grid size ( n ) to 100000 and submit the batch job with 4 workers. Monitor the usage of resources with tools available at your center, for instance top (UPPMAX), job-usage (HPC2N), or if you\u2019re working in the GUI (e.g. on LUNARC), you can click Parallel and then Monitor Jobs . For job-usage , you can see the job ID if you type squeue --me on a terminal on Kebnekaise.","title":"Exercises"},{"location":"advanced/distributed_parallelism/#extra-exercises-covering-mpi","text":"","title":"Extra exercises covering MPI"},{"location":"advanced/distributed_parallelism/#julia-find-the-difference-in-coding","text":"In this exercise we will run a parallelized code that performs the 2D integration used above: \\int^{\\pi}_{0}\\int^{\\pi}_{0}\\sin(x+y)dxdy = 0 Note the differences in writing the code MPI usually needs more rewriting Julia scripts serial.jl threaded.jl distributed.jl mpi.jl # nr. of grid points n = 100000 function integration2d_julia ( n ) # interval size h = \u03c0 / n # cummulative variable mysum = 0.0 # regular integration in the X axis for i in 0 : n - 1 x = h * ( i + 0.5 ) # regular integration in the Y axis for j in 0 : n - 1 y = h * ( j + 0.5 ) mysum = mysum + sin ( x + y ) end end return mysum * h * h end res = integration2d_julia ( n ) println ( res ) using . Threads # nr. of grid points n = 100000 # nr. of threads numthreads = nthreads () # array for storing partial sums from threads partial_integrals = zeros ( Float64 , numthreads ) function integration2d_julia_threaded ( n , numthreads , threadindex ) # interval size h = \u03c0 / convert ( Float64 , n ) # cummulative variable mysum = 0.0 # workload for each thread workload = convert ( Int64 , n / numthreads ) # lower and upper integration limits for each thread lower_lim = workload * ( threadindex - 1 ) upper_lim = workload * threadindex - 1 ## regular integration in the X axis for i in lower_lim : upper_lim x = h * ( i + 0.5 ) # regular integration in the Y axis for j in 0 : n - 1 y = h * ( j + 0.5 ) mysum = mysum + sin ( x + y ) end end partial_integrals [ threadindex ] = mysum * h * h return end # The threads can compute now the partial summations @threads for i in 1 : numthreads integration2d_julia_threaded ( n , numthreads , threadid ()) end # The main thread now reduces the array total_sum = sum ( partial_integrals ) println ( \"The integral value is $total_sum \" ) @everywhere begin using Distributed using SharedArrays end # nr. of grid points n = 100000 # nr. of workers numworkers = nworkers () # array for storing partial sums from workers partial_integrals = SharedArray ( zeros ( Float64 , numworkers ) ) @everywhere function integration2d_julia_distributed ( n , numworkers , workerid , A :: SharedArray ) # interval size h = \u03c0 / convert ( Float64 , n ) # cummulative variable mysum = 0.0 # workload for each worker workload = convert ( Int64 , n / numworkers ) # lower and upper integration limits for each thread lower_lim = workload * ( workerid - 2 ) upper_lim = workload * ( workerid - 1 ) - 1 # regular integration in the X axis for i in lower_lim : upper_lim x = h * ( i + 0.5 ) # regular integration in the Y axis for j in 0 : n - 1 y = h * ( j + 0.5 ) mysum = mysum + sin ( x + y ) end end A [ workerid - 1 ] = mysum * h * h return end # The workers can compute now the partial summations @sync @distributed for i in 1 : numworkers integration2d_julia_distributed ( n , numworkers , myid (), partial_integrals ) end # The main process now reduces the array total_sum = sum ( partial_integrals ) println ( \"The integral value is $total_sum \" ) using MPI MPI . Init () # Initialize the communicator comm = MPI . COMM_WORLD # Get the ranks of the processes rank = MPI . Comm_rank ( comm ) # Get the size of the communicator size = MPI . Comm_size ( comm ) # root process root = 0 # nr. of grid points n = 100000 function integration2d_julia_mpi ( n , numworkers , workerid ) # interval size h = \u03c0 / convert ( Float64 , n ) # cummulative variable mysum = 0.0 # workload for each worker workload = div ( n , numworkers ) # lower and upper integration limits for each thread lower_lim = workload * workerid upper_lim = workload * ( workerid + 1 ) - 1 # regular integration in the X axis for i in lower_lim : upper_lim x = h * ( i + 0.5 ) # regular integration in the Y axis for j in 0 : n - 1 y = h * ( j + 0.5 ) mysum = mysum + sin ( x + y ) end end partial_integrals = mysum * h * h return partial_integrals end # The workers can compute now the partial summations p = integration2d_julia_mpi ( n , size , rank ) # The root process now reduces the array integral = MPI . Reduce ( p , + , root , comm ) if rank == root println ( \"The integral value is $integral \" ) end MPI . Finalize () Batch scripts The corresponding batch scripts for these examples are given here: UPPMAX HPC2N LUNARC PDC NSC serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash -l #SBATCH -A naiss202t-uv-wxyz #SBATCH -J job #SBATCH -n 1 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml julia/1.8.5 # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml julia/1.8.5 # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml julia/1.8.5 # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml julia/1.8.5 ml gcc/11.3.0 openmpi/4.1.3 # \"time\" command is optional # export the PATH of the Julia MPI wrapper export PATH = ~/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash #SBATCH -A hpc2n202w-xyz #SBATCH -J job #SBATCH -n 1 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A hpc2n202w-xyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A hpc2n202w-xyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A hpc2n202w-xyz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml foss/2021b # export the PATH of the Julia MPI wrapper export PATH = /home/u/username/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash #SBATCH -A lu202w-x-yz #SBATCH -J job #SBATCH -n 1 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A lu202w-x-yz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A lu202w-x-yz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A lu202w-x-yz #SBATCH -J job #SBATCH -n 8 #SBATCH --time=00:10:00 #SBATCH --error=job.%J.err #SBATCH --output=job.%J.out ml purge > /dev/null 2 > & 1 ml Julia/1.8.5-linux-x86_64 ml foss/2021b # export the PATH of the Julia MPI wrapper export PATH = /home/u/username/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=8 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=8 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=8 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # export the PATH of the Julia MPI wrapper export PATH = /cfs/klemming/home/u/username/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl serial.sh threaded.sh distributed.sh mpi.sh #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 1 # nr. of tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml julia/1.9.4-bdist # \"time\" command is optional time julia serial.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 8 # nr. of tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml julia/1.9.4-bdist # \"time\" command is optional time julia -t 8 threaded.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 8 # nr. of tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml julia/1.9.4-bdist # \"time\" command is optional time julia -p 8 distributed.jl #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 8 # nr. of tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml buildtool-easybuild/4.8.0-hpce082752a2 foss/2023b ml julia/1.9.4-bdist # export the PATH of the Julia MPI wrapper export PATH = /home/username/.julia/bin: $PATH time mpiexecjl -np 8 julia mpi.jl","title":"Julia: Find the difference in coding"},{"location":"advanced/distributed_parallelism/#r-rmpi","text":"RMPI Short parallel example using package \u201cRmpi\u201d (\u201cpbdMPI on Dardel\u201d) UPPMAX HPC2N LUNARC NSC PDC Rmpi.R pbdMPI.R Short parallel example (using package \u201cRmpi\u201d, so we need to load both the module R/4.4.2-gfbf-2024a and the module R-bundle-CRAN/2024.11-foss-2024a. A suitable openmpi module, OpenMPI/5.0.3-GCC-13.3.0, is loaded with these.) #!/bin/bash -l #SBATCH -A uppmax2025-2-360 #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 8 export OMPI_MCA_mpi_warn_on_fork = 0 export OMPI_MCA_btl_openib_allow_ib = 1 ml purge > /dev/null 2 > & 1 ml R/4.4.2-gfbf-2024a ml OpenMPI/5.0.3-GCC-13.3.0 R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 mpirun -np 1 R CMD BATCH --no-save --no-restore Rmpi.R output.out Short parallel example (using packages \u201cRmpi\u201d). Loading R/4.4.1 and its prerequisites, as well as R-bundle-CRAN/2024.06 and its prerequisites. #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 8 export OMPI_MCA_mpi_warn_on_fork = 0 ml purge > /dev/null 2 > & 1 ml GCC/13.2.0 R/4.4.1 ml OpenMPI/4.1.6 R-bundle-CRAN/2024.06 mpirun -np 1 Rscript Rmpi.R Short parallel example (using packages \u201cRmpi\u201d). Loading R/4.2.1 and its prerequisites. #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID # Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 8 export OMPI_MCA_mpi_warn_on_fork = 0 ml purge > /dev/null 2 > & 1 ml GCC/11.3.0 OpenMPI/4.1.4 ml R/4.2.1 mpirun -np 1 R CMD BATCH --no-save --no-restore Rmpi.R output.out Short parallel example (using packages \u201cpbdMPI as \u201cRmpi\u201d does not work correctly on NSC). Loading R/4.2.2. Note: for NSC you first need to install \u201cpdbMPI\u201d ( module load R/4.2.2-hpc1-gcc-11.3.0-bare , start R , install.packages('pbdMPI') , pick CRAN mirror (Denmark, Finland, Sweden or other closeby)) #!/bin/bash #SBATCH -A naiss2025-22-934 # Asking for 15 min. #SBATCH -t 00:15:00 #SBATCH -n 8 #SBATCH --exclusive ml purge > /dev/null 2 > & 1 ml R/4.2.2-hpc1-gcc-11.3.0-bare srun --mpi = pmix Rscript pbdMPI.R Short parallel example (using packages \u201cpbdMPI\u201d). Loading R/4.4.1. Note: for PDC you first need to install \u201cpbdMPI\u201d (\u201cRmpi\u201d does not work). You can find the tarball in /cfs/klemming/projects/supr/courses-fall-2025/pbdMPI_0.5-4.tar.gz . Copy it to your own subdirectory under the project directory and then do: module load PDC/24.11 R/4.4.2-cpeGNU-24.11 R CMD INSTALL pbdMPI_0.5-4.tar.gz --configure-args=\" --with-mpi-include=/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/include --with-mpi-libpath=/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib --with-mpi-type=MPICH2\" --no-test-load #!/bin/bash -l #SBATCH -A naiss2025-22-934 # Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH --nodes 2 #SBATCH --ntasks-per-node=8 #SBATCH -p main #SBATCH --output=pbdMPI-test_%J.out # If you do ml purge you also need to restore the preloaded modules which you should have saved # when you logged in. Otherwise leave the two following lines outcommented. #ml purge > /dev/null 2>&1 #ml restore preload ml PDC/24.11 ml R/4.4.2-cpeGNU-24.11 srun -n 4 Rscript pbdMPI.R This R script uses package \u201cRmpi\u201d. # Load the R MPI package if it is not already loaded. if ( ! is.loaded ( \"mpi_initialize\" )) { library ( \"Rmpi\" ) } print ( mpi.universe.size ()) ns <- mpi.universe.size () - 1 mpi.spawn.Rslaves ( nslaves = ns ) # # In case R exits unexpectedly, have it automatically clean up # resources taken up by Rmpi (slaves, memory, etc...) .Last <- function (){ if ( is.loaded ( \"mpi_initialize\" )){ if ( mpi.comm.size ( 1 ) > 0 ){ print ( \"Please use mpi.close.Rslaves() to close slaves.\" ) mpi.close.Rslaves () } print ( \"Please use mpi.quit() to quit R\" ) .Call ( \"mpi_finalize\" ) } } # Tell all slaves to return a message identifying themselves mpi.remote.exec ( paste ( \"I am\" , mpi.comm.rank (), \"of\" , mpi.comm.size (), system ( \"hostname\" , intern = T ))) # Test computations x <- 5 x <- mpi.remote.exec ( rnorm , x ) length ( x ) x # Tell all slaves to close down, and exit the program mpi.close.Rslaves () mpi.quit () This R script uses package \u201cpbdMPI\u201d. library ( pbdMPI ) ns <- comm.size () # Tell all R sessions to return a message identifying themselves id <- comm.rank () ns <- comm.size () host <- system ( \"hostname\" , intern = TRUE ) comm.cat ( \"I am\" , id , \"on\" , host , \"of\" , ns , \"\\n\" , all.rank = TRUE ) # Test computations x <- 5 x <- rnorm ( x ) comm.print ( length ( x )) comm.print ( x , all.rank = TRUE ) finalize () Send the script to the batch system: sbatch <batch script>","title":"R: RMPI"},{"location":"advanced/distributed_parallelism/#matlab-parfor-and-parfeval","text":"Challenge 1. Create and run a parallel code We have the following code in MATLAB that generates an array of 10000 random numbers and then the sum of all elements is stored in a variable called s : r = rand ( 1 , 10000 ); s = sum ( r ); We want now to repeat these steps (generating the numbers and taking the sum) 6 times so that the steps are run at the same time. Use parfor to parallelize these steps. Once your code is parallelized enclose it in a parpool section and send the job to the queue. Solution % Nr. of workers nworkers = 6 ; % Use parallel pool with 'parfor' parpool ( 'name-of-your-cluster' , nworkers ); % Start parallel pool with nworkers workers myarray = []; % Optional in this exercise to store partial results parfor i = 1 : nworkers r = rand ( 1 , 10000 ); s = sum ( r ); myarray = [ myarray , s ]; end myarray % print out the results from the workers % Clean up the parallel pool delete ( gcp ( 'nocreate' )); Challenge 2. Run a parallel code with batch MATLAB function The following function uses parfeval to do some computation (specifically it takes the average per-column of a matrix with a size nsize equal to 1000): function results = parfeval_mean ( nsize ) results = parfeval (@ mean , 1 , rand ( nsize )) end Place this function in a file called parfeval_mean.m and submit this function with the MATLAB batch command. Solution c = parcluster ( 'name-of-your-cluster' ); j = c . batch (@ parfeval_mean , 1 ,{ 1000 }, 'pool' , 1 ); j . wait ; % wait for the results t = j . fetchOutputs {:}; % fetch the results fprintf ( 'Name of host: %.5f \\n' , t ); % Print out the results More info HPC2N Julia documentation . White paper on Julia parallel computing . HPC2N R documentation . Wikipedias\u2019 article on Parallel Computing .","title":"MATLAB: parfor and parfeval"},{"location":"advanced/parallel_computing/","text":"Parallel computation \u00b6 Learning outcomes I can name and describe 3 types of parallel computation I can explain at least 1 advantage of parallel computation I can explain at least 2 disadvantages of parallel computation I can explain how to use my computational resources effectively For teachers Teaching goals are: Learners have scheduled and run a job that needs more cores, with a calculation in their favorite language Learners understand when it is possible/impossible and/or useful/useless to run a job with multiple cores Lesson plan: 10 mins: Prior 5 mins: Present 15 mins: Challenge 15 mins: Feedback Prior: In an HPC context, what is a core? In an HPC context, what is a node? In an HPC context, what are computational resources? What is parallel computing? How does parallel computing work? When to use parallel computing? Feedback: When to use parallel computing? When not to use parallel computing? The main section is called \u2018The ideal effectiveness of parallelism\u2019. What does \u2018ideal\u2019 mean in this context? What could make parallelism less ideal? Prefer this session as a video? The watch the YouTube video R-Julia-MATLAB course, advanced day: Parallel computation Figure 1: CoRA, a robotic platform in which all computers (e.g. the one connected to the camera, another connected to the gripper, etc) sent messages to one receiving computer: an example of distributed parallelism Why parallel computing is important \u00b6 Most HPC clusters use 7-10 days as a maximum duration for a job. Your calculation may take longer than that. One technique that may work is to use parallel computing, where one uses multiple CPU cores to work together on a same calculation HPC cluster architecture \u00b6 Here is a simplified picture of HPC cluster architecture: flowchart TD subgraph hpc_cluster[HPC cluster] subgraph node_1[Node] subgraph cpu_1_1[CPU] core_1_1_1[Core] end end end Term What it loosely is Amount Core Something that does a calculation One or more per CPU CPU A collection of cores that share the same memory One or more per node Node A collection of CPUs that share the same memory One or more per HPC cluster HPC cluster A collection of nodes One or more per universe The universe [1] A collection of HPC clusters One [1] this is a pun to distributed parallelism Simplifications used 1 core is 1 worker is 1 thread is 1 procedure is 1 task Why do we use this simplification? Because we start from the assumption that we are reasonably smart, i.e. we will run as much threads as we have cores. There is an exercise where this assumption is broken. Types of \u2018doing more things at the same time\u2019 \u00b6 There are many types of \u2018doing more things at the same time\u2019. One way to distinguish these, is to separate these on the extent of the parallelism: Extent Parallelism Core Single-threaded (you already do this) Node Thread parallelism (this session) HPC cluster Distributed parallelism (next session) The universe Distributed parallelism Today, we will extend your toolkit from a single-threaded calculation (you already do this) to thread parallelism. The ideal effectiveness of parallelism \u00b6 Before going into details, we will look at the effectiveness of parallelism in the most optimal case, with the goal that you can determine if it is worth it. By now, you can probably guess that parallel computing spreads a calculation over multiple things that can calculate. Imagine a calculation that takes 16 time units, represented as this: Figure 2: a calculation of 16 time units run on 1 core, following the legend below: Square A unit of calculation time that \u2026 cannot be run in parallel can be run in parallel is spent doing nothing This calculation time is expressed in a time unit such as seconds and hence, can be split up into smaller blocks. Using 2 calculation units, this results in: Figure 2: a calculation of 16 time units run on 2 cores, where square is a time unit of calculation. This takes the calculation down from 16 to 10 time units. The so-called \u2018speedup\u2019 of using two workers is 16 / 10 = 1.6. How did you calculate the speedup exactly? Following [Hennessy and Patterson, 2011] (section 1.8), the speedup n (as in the sentence \u2018X is n times faster than Y\u2019) equals: n = t_y / t_x where: t_x is the time the new/enhanced process takes t_y is the time the regular/unenhanced process takes In this context, the \u2018new/enhanced process\u2019 is the calculation performed by multiple cores. Do not be confused by another version of Amdahl\u2019s Law, that is expressed and the calculation units used (and where the numerator and denominator are swapped): n = performance_x / performance_y where: performance_x is the performance (e.g. tasks completed per time unit) the new/enhanced process does performance_y is the performance of the regular process Isn\u2019t that Gustafson\u2019s Law? Not directly. We do use the same term \u2018speedup\u2019 as is calculated in Gustafson\u2019s Law, yet we apply it to compare between a single-core and a multi-core process. Gustafson\u2019s Law predict the maximum speedup, which is S = s + (p * N) = N - ((N - 1) * s) = 1 + ((N - 1) * p) S is the speedup s is fraction of the calculation that cannot be parallelized. The \u2018s\u2019 stands \u2018serial\u2019 p is fraction of the calculation that can be parallelized N is the number of workers, in our case: cores However, 4 (out of 20) calculations units are spent waiting. This means that 16 / 20 = 80% of the calculation time is spent efficiently. How did you calculate the efficiency exactly? The efficiency, f , equals: f = t_used_effectively / t_total` where: t_used_effectively is the time spend on a calculation, summed up over all cores t_total is the total time all spent by all cores These two can be calculated as such: t_used_effectively = (p + s) + (p * (N - 1)) t_total = time * N where: s is fraction of the calculation that cannot be parallelized. The \u2018s\u2019 stands \u2018serial\u2019 p is fraction of the calculation that can be parallelized N is the number of workers, in our case: cores Here one can see this calculation for more cores: Program runtime Number of cores Time Speedup Efficiency 1 16 16 / 16 = 100% 16 / 16 = 100% 2 10 16 / 10 = 160% (10 + 6) / (10 * 2) = 16 / 20 = 80% 3 8 16 / 8 = 200% (10 + 4 + 4) / (10 * 3) = 60% 4 7 16 / 7 = 229% (10 + 3 + 3 + 3) / (10 * 4) = 48% 6 6 16 / 6 = 267% (10 + (2^5)) / (10 * 6) = 70% . 12 5 16 / 5 = .% (10 + (1^11)) / (10 * 12) = 10% . 24 4.5 16 / 4.5 = .% (10 + (0.5^23)) / (10 * 24) = 34 / 50 = 68% . 48 4.25 16 / 4.25 = .% (10 + (0.25^47)) / (10 * 48) = 34 / 50 = 68% The best possible speed gain (as shown here) is called Amdahl\u2019s Law (inspired by [Amdahl, 1967] ) and, in a general form, is plotted like this: Exercises \u00b6 Question -1: change your Zoom name \u00b6 Change your Zoom name, to include your HPC cluster and favorite programming language, e.g. make Sven into Sven [Alvis, R] . You can do so by right-click on the video of yourself, then click \u2018Rename\u2019 to change your Zoom name. Question 0: remember your Zoom room \u00b6 HPC cluster name Main breakout room Alvis Room 1 Bianca Room 2 COSMOS Room 3 Dardel Room 4 Kebnekaise Room 5 LUMI Room 6 Pelle Room 7 Rackham Room 8 Tetralith Room 9 Question 1 \u00b6 Which of the lines in the graph of Amdahl\u2019s Law corresponds with the worked-out example of 16 time units? Answer The red dotted line. Using a different unit (i.e. \u2018relative speed\u2019, instead of \u2018speedup\u2019) was done on purpose. It is easy to convert between the two: just take the inverse (i.e. divide 1 by the value you have) Question 2 \u00b6 In the example of 16 time units, what is the shortest amount of time that can be spent on the calculation, given infinite resources? Answer The length of the calculation that cannot be run in parallel, which is 4 time units. Question 3 \u00b6 In the example of 16 time units, what is the maximal possible speedup? Answer 400%, as the calculation takes 16 units of work on a single core, and 4 time units on an infinite amount of cores. In mathematical form, the speedup, S , equals: S = t_regular / t_enhanced S = 16 / 4 S = 4 S = 400% where: t_enhanced is the time the enhanced process takes t_regular is the time the regular/unenhanced process takes Question 4 \u00b6 For your research project, you need to run a lot of calculations. Each calculation takes 10 hours. How do you make optimal use of your computational resources? Answer Run the calculation on a single core for 100% efficiency Question 5 \u00b6 For your research project, you also have a calculation that takes 11 days. Your HPC cluster allows a calculation of at most 10 days. Assume your HPC center will not extend your job (they will probably do so when asked: we are there to help!). How do you make optimal use of your computational resources? Answer If your calculation already has parallelism built-in, then run the calculation on two cores: this only involves changing your Slurm script, with a low loss of computational resources. If you are a really tight on computational resources, you can implement a \u2018save state\u2019 in your calculation, so that you can schedule two runs of nine days in succession, each with 100% efficiency. Alternatively, you can added thread parallelism to allow running with multiple cores. Question 6 \u00b6 Your colleague runs many jobs with a lot of cores. \u2018It is way faster!\u2019, he/she states. That same colleague, however, also complains about long waiting times before his/her jobs start. How would you explain this situation? Answer The colleague used up (or: \u2018wasted\u2019) all his/her computational resources (commonly 10,000 core hours per month on UPPMAX). Due to this, his/her jobs are only run when the HPC cluster has a low workload and activates the so-called \u2018bonus queue\u2019 (UPPMAX) or generally have to wait for their priority to go up again. Question 7 \u00b6 Your colleague has also done the exercises in this session and decided to measure the same features of her code. Below you can see the plots produced by this benchmark. What seemed to be the percentage of his/her code that could be run in parallel? Total core seconds Efficiency Speedup Benchmark: the total core seconds per number of workers Benchmark: Efficiency per number of workers Benchmark: Speedup per number of workers Answer It is unknown what portion of the code can be run in parallel. Instead, this benchmark shows how it looks if code is run in single-thread mode. In this case, the colleague forgot that the number of threads used must be specified when calling Julia. Note that these plots are less clear than you may have expected: The total core seconds is expected to increase linearily The efficiency is expected to follow the dashed line The speedup is expected to be at 100% It may be unexpected that also in computer science, results can be messier than you would expect :-) References \u00b6 [Amdahl, 1967] Amdahl, Gene M. \u201cValidity of the single processor approach to achieving large scale computing capabilities.\u201d Proceedings of the April 18-20, 1967, spring joint computer conference. 1967. Fun facts: it contains a single hand-drawn plot and no equations [Hennessy and Patterson, 2011] Hennessy, John L., and David A. Patterson. Computer architecture: a quantitative approach. Elsevier, 2011.","title":"Parallel computing"},{"location":"advanced/parallel_computing/#parallel-computation","text":"Learning outcomes I can name and describe 3 types of parallel computation I can explain at least 1 advantage of parallel computation I can explain at least 2 disadvantages of parallel computation I can explain how to use my computational resources effectively For teachers Teaching goals are: Learners have scheduled and run a job that needs more cores, with a calculation in their favorite language Learners understand when it is possible/impossible and/or useful/useless to run a job with multiple cores Lesson plan: 10 mins: Prior 5 mins: Present 15 mins: Challenge 15 mins: Feedback Prior: In an HPC context, what is a core? In an HPC context, what is a node? In an HPC context, what are computational resources? What is parallel computing? How does parallel computing work? When to use parallel computing? Feedback: When to use parallel computing? When not to use parallel computing? The main section is called \u2018The ideal effectiveness of parallelism\u2019. What does \u2018ideal\u2019 mean in this context? What could make parallelism less ideal? Prefer this session as a video? The watch the YouTube video R-Julia-MATLAB course, advanced day: Parallel computation Figure 1: CoRA, a robotic platform in which all computers (e.g. the one connected to the camera, another connected to the gripper, etc) sent messages to one receiving computer: an example of distributed parallelism","title":"Parallel computation"},{"location":"advanced/parallel_computing/#why-parallel-computing-is-important","text":"Most HPC clusters use 7-10 days as a maximum duration for a job. Your calculation may take longer than that. One technique that may work is to use parallel computing, where one uses multiple CPU cores to work together on a same calculation","title":"Why parallel computing is important"},{"location":"advanced/parallel_computing/#hpc-cluster-architecture","text":"Here is a simplified picture of HPC cluster architecture: flowchart TD subgraph hpc_cluster[HPC cluster] subgraph node_1[Node] subgraph cpu_1_1[CPU] core_1_1_1[Core] end end end Term What it loosely is Amount Core Something that does a calculation One or more per CPU CPU A collection of cores that share the same memory One or more per node Node A collection of CPUs that share the same memory One or more per HPC cluster HPC cluster A collection of nodes One or more per universe The universe [1] A collection of HPC clusters One [1] this is a pun to distributed parallelism Simplifications used 1 core is 1 worker is 1 thread is 1 procedure is 1 task Why do we use this simplification? Because we start from the assumption that we are reasonably smart, i.e. we will run as much threads as we have cores. There is an exercise where this assumption is broken.","title":"HPC cluster architecture"},{"location":"advanced/parallel_computing/#types-of-doing-more-things-at-the-same-time","text":"There are many types of \u2018doing more things at the same time\u2019. One way to distinguish these, is to separate these on the extent of the parallelism: Extent Parallelism Core Single-threaded (you already do this) Node Thread parallelism (this session) HPC cluster Distributed parallelism (next session) The universe Distributed parallelism Today, we will extend your toolkit from a single-threaded calculation (you already do this) to thread parallelism.","title":"Types of &lsquo;doing more things at the same time&rsquo;"},{"location":"advanced/parallel_computing/#the-ideal-effectiveness-of-parallelism","text":"Before going into details, we will look at the effectiveness of parallelism in the most optimal case, with the goal that you can determine if it is worth it. By now, you can probably guess that parallel computing spreads a calculation over multiple things that can calculate. Imagine a calculation that takes 16 time units, represented as this: Figure 2: a calculation of 16 time units run on 1 core, following the legend below: Square A unit of calculation time that \u2026 cannot be run in parallel can be run in parallel is spent doing nothing This calculation time is expressed in a time unit such as seconds and hence, can be split up into smaller blocks. Using 2 calculation units, this results in: Figure 2: a calculation of 16 time units run on 2 cores, where square is a time unit of calculation. This takes the calculation down from 16 to 10 time units. The so-called \u2018speedup\u2019 of using two workers is 16 / 10 = 1.6. How did you calculate the speedup exactly? Following [Hennessy and Patterson, 2011] (section 1.8), the speedup n (as in the sentence \u2018X is n times faster than Y\u2019) equals: n = t_y / t_x where: t_x is the time the new/enhanced process takes t_y is the time the regular/unenhanced process takes In this context, the \u2018new/enhanced process\u2019 is the calculation performed by multiple cores. Do not be confused by another version of Amdahl\u2019s Law, that is expressed and the calculation units used (and where the numerator and denominator are swapped): n = performance_x / performance_y where: performance_x is the performance (e.g. tasks completed per time unit) the new/enhanced process does performance_y is the performance of the regular process Isn\u2019t that Gustafson\u2019s Law? Not directly. We do use the same term \u2018speedup\u2019 as is calculated in Gustafson\u2019s Law, yet we apply it to compare between a single-core and a multi-core process. Gustafson\u2019s Law predict the maximum speedup, which is S = s + (p * N) = N - ((N - 1) * s) = 1 + ((N - 1) * p) S is the speedup s is fraction of the calculation that cannot be parallelized. The \u2018s\u2019 stands \u2018serial\u2019 p is fraction of the calculation that can be parallelized N is the number of workers, in our case: cores However, 4 (out of 20) calculations units are spent waiting. This means that 16 / 20 = 80% of the calculation time is spent efficiently. How did you calculate the efficiency exactly? The efficiency, f , equals: f = t_used_effectively / t_total` where: t_used_effectively is the time spend on a calculation, summed up over all cores t_total is the total time all spent by all cores These two can be calculated as such: t_used_effectively = (p + s) + (p * (N - 1)) t_total = time * N where: s is fraction of the calculation that cannot be parallelized. The \u2018s\u2019 stands \u2018serial\u2019 p is fraction of the calculation that can be parallelized N is the number of workers, in our case: cores Here one can see this calculation for more cores: Program runtime Number of cores Time Speedup Efficiency 1 16 16 / 16 = 100% 16 / 16 = 100% 2 10 16 / 10 = 160% (10 + 6) / (10 * 2) = 16 / 20 = 80% 3 8 16 / 8 = 200% (10 + 4 + 4) / (10 * 3) = 60% 4 7 16 / 7 = 229% (10 + 3 + 3 + 3) / (10 * 4) = 48% 6 6 16 / 6 = 267% (10 + (2^5)) / (10 * 6) = 70% . 12 5 16 / 5 = .% (10 + (1^11)) / (10 * 12) = 10% . 24 4.5 16 / 4.5 = .% (10 + (0.5^23)) / (10 * 24) = 34 / 50 = 68% . 48 4.25 16 / 4.25 = .% (10 + (0.25^47)) / (10 * 48) = 34 / 50 = 68% The best possible speed gain (as shown here) is called Amdahl\u2019s Law (inspired by [Amdahl, 1967] ) and, in a general form, is plotted like this:","title":"The ideal effectiveness of parallelism"},{"location":"advanced/parallel_computing/#exercises","text":"","title":"Exercises"},{"location":"advanced/parallel_computing/#question-1-change-your-zoom-name","text":"Change your Zoom name, to include your HPC cluster and favorite programming language, e.g. make Sven into Sven [Alvis, R] . You can do so by right-click on the video of yourself, then click \u2018Rename\u2019 to change your Zoom name.","title":"Question -1: change your Zoom name"},{"location":"advanced/parallel_computing/#question-0-remember-your-zoom-room","text":"HPC cluster name Main breakout room Alvis Room 1 Bianca Room 2 COSMOS Room 3 Dardel Room 4 Kebnekaise Room 5 LUMI Room 6 Pelle Room 7 Rackham Room 8 Tetralith Room 9","title":"Question 0: remember your Zoom room"},{"location":"advanced/parallel_computing/#question-1","text":"Which of the lines in the graph of Amdahl\u2019s Law corresponds with the worked-out example of 16 time units? Answer The red dotted line. Using a different unit (i.e. \u2018relative speed\u2019, instead of \u2018speedup\u2019) was done on purpose. It is easy to convert between the two: just take the inverse (i.e. divide 1 by the value you have)","title":"Question 1"},{"location":"advanced/parallel_computing/#question-2","text":"In the example of 16 time units, what is the shortest amount of time that can be spent on the calculation, given infinite resources? Answer The length of the calculation that cannot be run in parallel, which is 4 time units.","title":"Question 2"},{"location":"advanced/parallel_computing/#question-3","text":"In the example of 16 time units, what is the maximal possible speedup? Answer 400%, as the calculation takes 16 units of work on a single core, and 4 time units on an infinite amount of cores. In mathematical form, the speedup, S , equals: S = t_regular / t_enhanced S = 16 / 4 S = 4 S = 400% where: t_enhanced is the time the enhanced process takes t_regular is the time the regular/unenhanced process takes","title":"Question 3"},{"location":"advanced/parallel_computing/#question-4","text":"For your research project, you need to run a lot of calculations. Each calculation takes 10 hours. How do you make optimal use of your computational resources? Answer Run the calculation on a single core for 100% efficiency","title":"Question 4"},{"location":"advanced/parallel_computing/#question-5","text":"For your research project, you also have a calculation that takes 11 days. Your HPC cluster allows a calculation of at most 10 days. Assume your HPC center will not extend your job (they will probably do so when asked: we are there to help!). How do you make optimal use of your computational resources? Answer If your calculation already has parallelism built-in, then run the calculation on two cores: this only involves changing your Slurm script, with a low loss of computational resources. If you are a really tight on computational resources, you can implement a \u2018save state\u2019 in your calculation, so that you can schedule two runs of nine days in succession, each with 100% efficiency. Alternatively, you can added thread parallelism to allow running with multiple cores.","title":"Question 5"},{"location":"advanced/parallel_computing/#question-6","text":"Your colleague runs many jobs with a lot of cores. \u2018It is way faster!\u2019, he/she states. That same colleague, however, also complains about long waiting times before his/her jobs start. How would you explain this situation? Answer The colleague used up (or: \u2018wasted\u2019) all his/her computational resources (commonly 10,000 core hours per month on UPPMAX). Due to this, his/her jobs are only run when the HPC cluster has a low workload and activates the so-called \u2018bonus queue\u2019 (UPPMAX) or generally have to wait for their priority to go up again.","title":"Question 6"},{"location":"advanced/parallel_computing/#question-7","text":"Your colleague has also done the exercises in this session and decided to measure the same features of her code. Below you can see the plots produced by this benchmark. What seemed to be the percentage of his/her code that could be run in parallel? Total core seconds Efficiency Speedup Benchmark: the total core seconds per number of workers Benchmark: Efficiency per number of workers Benchmark: Speedup per number of workers Answer It is unknown what portion of the code can be run in parallel. Instead, this benchmark shows how it looks if code is run in single-thread mode. In this case, the colleague forgot that the number of threads used must be specified when calling Julia. Note that these plots are less clear than you may have expected: The total core seconds is expected to increase linearily The efficiency is expected to follow the dashed line The speedup is expected to be at 100% It may be unexpected that also in computer science, results can be messier than you would expect :-)","title":"Question 7"},{"location":"advanced/parallel_computing/#references","text":"[Amdahl, 1967] Amdahl, Gene M. \u201cValidity of the single processor approach to achieving large scale computing capabilities.\u201d Proceedings of the April 18-20, 1967, spring joint computer conference. 1967. Fun facts: it contains a single hand-drawn plot and no equations [Hennessy and Patterson, 2011] Hennessy, John L., and David A. Patterson. Computer architecture: a quantitative approach. Elsevier, 2011.","title":"References"},{"location":"advanced/thread_parallelism/","text":"Thread parallelism \u00b6 Learning outcomes I can schedule jobs with thread parallelism I can explain how jobs with thread parallelism are scheduled I can explain how Julia/MATLAB/R code makes use of thread parallelism I can explain the results of a correct benchmark I can explain the results of an incorrect benchmark I can argue why I should stick to my programming language, even if it is not the fastest For teachers Teaching goals are: Schedule and run a job that needs more cores, with a calculation in their favorite language Learners have scheduled and run a job that needs more cores, with a calculation in their favorite language Learners understand when it is possible/impossible and/or useful/useless to run a job with multiple cores Learners can argue why they should stick to their programming languages, even if it is not the fastest Prior: What is thread parallelism? Why use thread parallelism? Are there other ways to make your code run faster? What is a benchmark? Why do a benchmark? Feedback: When to use parallel computing? When not to use parallel computing? Status overview HPC cluster Julia MATLAB R Other comments Alvis Unknown Unknown Unknown . Bianca Unknown Unknown Unknown . COSMOS Yes No Yes . Dardel Yes No Yes . Kebnekaise Yes No Yes . LUMI Unknown Unknown Unknown . Rackham Yes No Yes . Pelle Yes No Yes . Tetralith Yes No Yes Seems to eat up jobs Prefer this session as a video? The watch the YouTube video R-Julia-MATLAB course, advanced day: Thread parallelism Why thread parallelism is important \u00b6 Because it is one way to speedup (pun intended) the calculation. Goal \u00b6 In this session, we are going to benchmark thread parallelism, as we should not make claims about performance without measurements [CppCore Per.6] . flowchart TD user[User] benchmark_script[Benchmark script] slurm_script[Slurm script] r_script[R script] julia_script[Julia script] matlab_script[MATLAB script] user --> |Account, language| benchmark_script benchmark_script --> |Account, language, number of cores| slurm_script slurm_script --> julia_script slurm_script --> matlab_script slurm_script --> r_script Benchmark script \u00b6 benchmark_2d_integration.sh is the script that starts a benchmark, by submitting multiple jobs to the Slurm queue, using the Slurm script below. The goal of the benchmark script is to do a fixed unit of work with increasingly more cores. As the script itself only does light calculations, you can run it directly. Here is how to call the script: bash benchmark_2d_integration.sh [ account ] [ language ] Why not call the script with ./benchmark_2d_integration.sh ? Because that would require one extra step: to make the script executable. For example: bash benchmark_2d_integration.sh staff r If you use the incorrect spelling, the script will help you. Slurm script \u00b6 This is the script that schedules a job with thread parallelism. The goal of the script is to submit a calculation that uses thread parallelism, with a custom amount of cores. This Slurm script is called by the benchmark script, i.e. not directly by a user. If the Slurm script is absent, the benchmark script will (try to) download it for you. How do I run it anyways? You do not, instead you will run the benchmark script below. However, you can run it as such: sbatch -A [ account ] -n [ number_of_cores ] do_ [ language ] _2d_integration.sh For example: sbatch -A staff -n 1 do_r_2d_integration.sh # On Dardel sbatch -A staff -n 1 -p main do_r_2d_integration.sh There are 3 Slurm scripts, 1 per language: Language Script with calculation Julia do_julia_2d_integration.sh MATLAB do_matlab_2d_integration.sh R do_r_2d_integration.sh Each of these Slurm scripts are called by the benchmark script, where the benchmark script supplies the desired number of cores. Language script \u00b6 This is the code (in your favorite language) that performs a job with thread parallelism. The goal of the language script is to have a fixed unit of work that can be done by a custom amount of cores. This language script is called by the Slurm script, i.e. not directly by a user. If the language script is absent, the benchmark script will (try to) download it for you. How do I run it anyways? Check the Slurm script for your favorite language. In general, you can run it as such: [ interpreter ] [ script_name ] [ number_of_cores ] [ grid_size ] On a login node, use 1 core and a grid size of 1 to start the lightest calculation possible: julia integration2d.jl 1 1 Rscript integration2d.R 1 1 Language Script with calculation Julia do_2d_integration.jl MATLAB do_2d_integration.m R do_2d_integration.R Exercises \u00b6 Exercise 1: start the benchmark on your HPC cluster \u00b6 The goal of this exercise is to start the benchmark script on your HPC cluster, as well as some troubleshooting. On your HPC cluster: Download the benchmark script How to do that? There are many ways to do so. One way is to download it directly from this course\u2019s repository : wget https://raw.githubusercontent.com/UPPMAX/R-matlab-julia-HPC/refs/heads/main/docs/advanced/thread_parallelism/benchmark_2d_integration.sh Run the benchmark script. Tip: see the \u2018Benchmark script\u2019 section . How to do that? The \u2018Benchmark script\u2019 section shows how: bash benchmark_2d_integration.sh staff r You can use our projects overview page to find the course NAISS project for your HPC cluster. Check the Slurm output files for problems. If there are problems: fix these, then run the benchmark script again How to do that? There are many ways to do so. One way is to show all files with the .out extension: cat *.out Exercise 2: read the benchmark script \u00b6 Now that the benchmark script is running, we have the time to figure out what it is doing. What is the most important single line in this script, i.e. the line it is all about? Tip: start looking from the bottom of the script Answer For all HPC clusters except Dardel: sbatch -A \" ${ slurm_job_account } \" -N \" ${ n_nodes } \" -n \" ${ n_cores } \" \" ${ slurm_script_name } \" For the Dardel HPC cluster: sbatch -A \" ${ slurm_job_account } \" -N \" ${ n_nodes } \" --ntasks \" ${ n_cores } \" -p shared \" ${ slurm_script_name } \" In English, describe what the line does in general terms Answer Schedule to run \u2026 on some account with some amount of nodes with some amount of cores (on Dardel) on the main partition a script with some name This line of code is part of a for loop. In English, what does the for loop achieve? Answer The for loop achieves that the same calculation is scheduled to be done with 1 core, then with 2 cores, then with 3 cores, etc., to 64 cores. Exercise 3: read the Slurm script \u00b6 The benchmark script submits a Slurm script of your favorite language multiple times to the queue: once with 1 cores, once with 2 cores, etc. What is the most important single line in this script, i.e. the line it is all about? Tip: start looking from the bottom of the script Answer The last line. Language Most important line Julia julia --threads \"${SLURM_NPROCS}\" do_2d_integration.jl \"${SLURM_NPROCS}\" MATLAB matlab -nodisplay -nosplash -nojvm -batch \"run(\\\"${matlab_target_filename}\\\"); exit;\" R Rscript --no-save --no-restore do_2d_integration.R \"${SLURM_NPROCS}\" In English, describe what the line does in general terms. Tip: this is the same answer for all programming languages. Tip 2: assume \u2018procedure\u2019 is synonym for \u2018core\u2019. Tip 3: the Julia line is closest to English. Answer Run a Julia/MATLAB/R script for the booked number of cores, without doing anything else (e.g. showing a splash screen or restoring a computational environment). Exercise 4: read the language script \u00b6 The Slurm script runs a script of your favorite language for a specified number of cores. Locate the lines of code that make the calculation perform in parallel. Answer Julia MATLAB R Threads . @threads for worker_index = 1 : n_workers results [ worker_index ] = integration2d ( grid_size , n_workers , worker_index ) end parfor worker_index = 1 : n_workers partial_results ( worker_index ) = integration2d ( grid_size , n_workers , worker_index ); end results_of_workers <- foreach ( worker_index = 1 : n_workers , .combine = c ) %dopar% { integration2d ( grid_size , n_workers , worker_index ) } In English, describe what these lines does in general terms. Answer For each available worker: per worker, do part of a calculation and combine the results Optional: what is grid_size ? What does it do? What would be a better variable name? Answer grid_size determines the accuracy of the calculation: the bigger grid_size , the smaller intervals will be integrated. A better variable name could be accuracy . However, with such a variable name, there is no natural understanding that the range of its value goes from 1 to infinity. For the name grid_size , this range is easier to feel right, as sizes are non-zero positive values by nature Locate the keyword that make the calculation perform in parallel. Or: locate the word that, when removed, would \u2018downgrade\u2019 the calculation to be single-threaded. Answer The last line. Language Keyword to indicate a parallel calculation Julia Threads.@threads MATLAB parfor R %dopar% The function that is run in parallel (i.e. integration2d ) is made suitable to be run in parallel. In English, describe which changes are made to make it suitable. Answer The function has three (instead of one) arguments: Language Function signature Julia function integration2d(grid_size::Int, n_workers::Int, worker_index::Int) MATLAB function integration2d(grid_size, n_workers, worker_index) R integration2d <- function(grid_size, n_workers, worker_index) The extra arguments are n_workers and worker_index . These allow the worker_index -th worker to know which share of the calculation it must do. What do these changes achieve? Answer These changes allow each thread to know what it needs to know to run its part of the calculation. Bonus: do you spot the bug in integrate2d ? Is this a problem? Why would someone keep it in anyways? Answer The calculation of begin_index and end_index : Julia MATLAB R workload = fld ( grid_size , n_workers ) begin_index = workload * ( worker_index - 1 ) + 1 end_index = workload * worker_index grid_cells_per_worker = floor ( grid_size / n_workers ); begin_index = grid_cells_per_worker * ( worker_index - 1 ) + 1 ; end_index = grid_cells_per_worker * worker_index ; grid_cells_per_worker <- floor ( grid_size / n_workers ) begin_index <- grid_cells_per_worker * ( worker_index - 1 ) + 1 end_index <- grid_cells_per_worker * worker_index To most clearly demonstrate this, imagine a grid_size of 3 for an n_workers of 2: Variable name Value grid_size 3 n_workers 2 grid_cells_per_worker 3 / 2 (rounded down) = 1 begin_index for first worker 1 * (1 - 1) + 1 = 1 end_index for first worker 1 * 1 = 1 begin_index for second worker 1 * (2 - 1) + 1 = 2 end_index for second worker 1 * 2 = 2 This means that, although the calculation is split in 3 parts, only 2 of these are performed. For bigger grid sizes, however, this problem gets less. One would keep such a bug in for readability: this session is about thread parallelism, not about the extensive calculation of these indices. Exercise 5: share the results \u00b6 By now, some of the calculations in exercise 1 will be finished. If not: no worries, just continue! In the terminal on your favorite HPC cluster, run the following command to collect all results: grep -EoRh \"^[jmlr].*,.*\" --include = *.out | sort | uniq Copy-paste these results to our shared HackMD document . Exercise 6: analyse the results \u00b6 Take it easy Doing this analysis yourself is useful, if you are fluent in analysing data. If not, you are encouraged to use: This spreadsheet This R script Copy-paste the results of the previous exercise into a comma-separated file called my_results.csv . These are the descriptions of the variables: Parameter Value language Your programming language hpc_cluster Your HPC cluster grid_size Accuracy n_workers Number of cores used core_secs Core seconds used, i.e. the time used by all cores together Load the comma-separated file ( my_results.csv ) in a spreadsheet or read it in your favorite programming language Add a column called wall_clock_sec , which equals core_secs divided by n_workers . wall_clock_sec is the time it took the calculation to complete Add a column called speedup , which equals the wall clock time for 1 core divided by the wall clock time of that amount of cores Plot the speedup (on the y axis) per number of workers (on the x axis). Compare your speedup with the Amdahl\u2019s Law figure of the previous session, above \u2018Exercises\u2019 ). What do you estimate is the maximum speedup? What do you estimate is the percentage of code that can be parallelized (i.e. the \u2018parallel portion\u2019 in the figure of Amdahl\u2019s Law)? Exercise 7: compare to others \u00b6 Compare your results to others, that ran the same benchmark, yet for other HPC clusters and other programming languages: Total core seconds Efficiency Speedup Benchmark: the total core seconds per number of workers Benchmark: Efficiency per number of workers Benchmark: Speedup per number of workers What do you notice? Answer This question is vague on purpose: there are many things going on here and many answers are correct: Measurements are messier than you may have thought Julia is 30x faster than R The MATLAB code is single-threaded R works best at being efficient in parallel HPC clusters differ The points on Kebnekaise differ more than other HPC clusters: this is because Kebnekaise is the most heterogeneous (i.e. has the highest number of different hardware) Exercise X1: job scheduling problem? \u00b6 You have just submitted some multithreaded jobs to the queue. What went wrong here? Why is this a problem? [richel@pelle1 thread_parallelism]$ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 54197 pelle do_r_2d_ richel R 0:14 1 p66 54200 pelle do_r_2d_ richel R 0:14 4 p[64-67] 54216 pelle do_r_2d_ richel R 0:14 3 p[104-106] 54217 pelle do_r_2d_ richel R 0:14 6 p[106-111] 54169 pelle do_r_2d_ richel R 0:15 1 p70 Answer The multithreaded jobs are split over multiple nodes. This means that the cores booked do not have shared memory. This will make the calculation slower, as the results have to be shared by sending data between nodes. Exercise X2: learn a faster programming language? \u00b6 As can be seen in the benchmark, some programming languages are faster than others. This warrants the question: should you learn to program in the programming language that does calculations fastest? The theoretically fastest programming languages allow you to write machine code. Assembler lets you do so directly. Some other languages (most notably C, C++ and Rust) allow you to insert machine code. Hence, these are the theoretically fastest languages. To write fast code, should one learn those languages instead? Below is a figure from [Prechelt, 2000] . It shows the distribution of runtime speeds of a certain problem (called z1000 ), for different programming languages. Take a close look at the figure. The paper has an advice to yes/no learn a \u2018faster\u2019 programming language. What do you think the advice is? Answer The variance within a programming language is bigger than variance between languages (adapted fig 2, from [Prechelt, 2000] ). Instead of learning a faster language, learn how to be fast in your language. Still, the Julia code is 30x faster than the R code. Why would this advice still hold? Answer Because none of the code is optimized for speed. Both Julia and R can call C code, where C is the fastest higher-level language. It would be interesting to see how this benchmark would look like for optimized code. Are there other factors that decide which programming language to use? If yes, name some. Answer There are many reasons why to use a \u2018slower\u2019 programming language: you already know the \u2018slower\u2019 programming language you need access to specific libraries/packages you have colleagues that are willing to teach you you need to to work from code that has been written someone else Where to go next? \u00b6 If you want to scale up, distributed parallelism allows you to do a calculation on many computers. Troubleshooting \u00b6 T1. Invalid account or account/partition combination specified \u00b6 sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified You\u2019ve specified the wrong account. Run: projinfo T2. There is no package called \u2018doParallel\u2019 \u00b6 This is an R error. You can find it by checking the log files: cat *.out When you see, for example, the text below, it is clearly stated that there is no package called doParallel . HPC cluster: tetralith Slurm job account used: naiss2025-22-934 Number of cores booked in Slurm: 32 Error in library(doParallel, quietly = TRUE) : there is no package called \u2018doParallel\u2019 Execution halted To fix this: load the correct module install that package from the terminal. To load the correct module, load the R module(s) as loaded by the do_r_2d_integration.sh script, for example: module load R/4.4.0-hpc1-gcc-11.3.0-bare Could you expand on that? Open the do_r_2d_integration.sh script. Search for the part where modules are loaded, which is at the bottom. Find the lines where the modules are loaded for your favorite HPC cluster, e.g. if [ ${ hpc_cluster } == \"rackham\" ] then module load R_packages/4.1.1 >/dev/null 2 > & 1 fi Copy the part that loads the modules, excluding the > and after, and run these in a terminal on your favorite HPC cluster: module load R_packages/4.1.1 You have now loaded the packages needed for the calculation. To install that package from the terminal, check this course\u2019s material on how to do so . T3. \u2018namespace \u2018rlang\u2019 0.4.12 is already loaded, but >= 1.1.0 is required\u2019 \u00b6 Error in loadNamespace ( i, c ( lib.loc, .libPaths ()) , versionCheck = vI [[ i ]]) : namespace \u2018rlang\u2019 0 .4.12 is already loaded, but > = 1 .1.0 is required Calls: <Anonymous> ... waldo_compare -> loadNamespace -> namespaceImport -> loadNamespace Execution halted This only happens on Rackham, since 2025-09-25. MATLAB error \u00b6 Warning : Executing startup failed in matlabrc . This indicates a potentially serious problem in your MATLAB setup, which should be resolved as soon as possible. Error detected was: MATLAB : undefinedVarOrClass Unable to resolve the name 'java.net.InetAddress.getLocalHost.getHostAddress'. Error using run RUN cannot execute the file 'do_2d_integration.m 48'. RUN requires a valid MATLAB script References \u00b6 [CppCore Per.6] C++ Core Guidelines: Per.6: Don\u2019t make claims about performance without measurements [Prechelt, 2000] Prechelt, Lutz. \u201cAn empirical comparison of C, C++, Java, Perl, Python, REXX and TCL.\u201d IEEE Computer 33.10 (2000): 23-29.","title":"Thread parallelism"},{"location":"advanced/thread_parallelism/#thread-parallelism","text":"Learning outcomes I can schedule jobs with thread parallelism I can explain how jobs with thread parallelism are scheduled I can explain how Julia/MATLAB/R code makes use of thread parallelism I can explain the results of a correct benchmark I can explain the results of an incorrect benchmark I can argue why I should stick to my programming language, even if it is not the fastest For teachers Teaching goals are: Schedule and run a job that needs more cores, with a calculation in their favorite language Learners have scheduled and run a job that needs more cores, with a calculation in their favorite language Learners understand when it is possible/impossible and/or useful/useless to run a job with multiple cores Learners can argue why they should stick to their programming languages, even if it is not the fastest Prior: What is thread parallelism? Why use thread parallelism? Are there other ways to make your code run faster? What is a benchmark? Why do a benchmark? Feedback: When to use parallel computing? When not to use parallel computing? Status overview HPC cluster Julia MATLAB R Other comments Alvis Unknown Unknown Unknown . Bianca Unknown Unknown Unknown . COSMOS Yes No Yes . Dardel Yes No Yes . Kebnekaise Yes No Yes . LUMI Unknown Unknown Unknown . Rackham Yes No Yes . Pelle Yes No Yes . Tetralith Yes No Yes Seems to eat up jobs Prefer this session as a video? The watch the YouTube video R-Julia-MATLAB course, advanced day: Thread parallelism","title":"Thread parallelism"},{"location":"advanced/thread_parallelism/#why-thread-parallelism-is-important","text":"Because it is one way to speedup (pun intended) the calculation.","title":"Why thread parallelism is important"},{"location":"advanced/thread_parallelism/#goal","text":"In this session, we are going to benchmark thread parallelism, as we should not make claims about performance without measurements [CppCore Per.6] . flowchart TD user[User] benchmark_script[Benchmark script] slurm_script[Slurm script] r_script[R script] julia_script[Julia script] matlab_script[MATLAB script] user --> |Account, language| benchmark_script benchmark_script --> |Account, language, number of cores| slurm_script slurm_script --> julia_script slurm_script --> matlab_script slurm_script --> r_script","title":"Goal"},{"location":"advanced/thread_parallelism/#benchmark-script","text":"benchmark_2d_integration.sh is the script that starts a benchmark, by submitting multiple jobs to the Slurm queue, using the Slurm script below. The goal of the benchmark script is to do a fixed unit of work with increasingly more cores. As the script itself only does light calculations, you can run it directly. Here is how to call the script: bash benchmark_2d_integration.sh [ account ] [ language ] Why not call the script with ./benchmark_2d_integration.sh ? Because that would require one extra step: to make the script executable. For example: bash benchmark_2d_integration.sh staff r If you use the incorrect spelling, the script will help you.","title":"Benchmark script"},{"location":"advanced/thread_parallelism/#slurm-script","text":"This is the script that schedules a job with thread parallelism. The goal of the script is to submit a calculation that uses thread parallelism, with a custom amount of cores. This Slurm script is called by the benchmark script, i.e. not directly by a user. If the Slurm script is absent, the benchmark script will (try to) download it for you. How do I run it anyways? You do not, instead you will run the benchmark script below. However, you can run it as such: sbatch -A [ account ] -n [ number_of_cores ] do_ [ language ] _2d_integration.sh For example: sbatch -A staff -n 1 do_r_2d_integration.sh # On Dardel sbatch -A staff -n 1 -p main do_r_2d_integration.sh There are 3 Slurm scripts, 1 per language: Language Script with calculation Julia do_julia_2d_integration.sh MATLAB do_matlab_2d_integration.sh R do_r_2d_integration.sh Each of these Slurm scripts are called by the benchmark script, where the benchmark script supplies the desired number of cores.","title":"Slurm script"},{"location":"advanced/thread_parallelism/#language-script","text":"This is the code (in your favorite language) that performs a job with thread parallelism. The goal of the language script is to have a fixed unit of work that can be done by a custom amount of cores. This language script is called by the Slurm script, i.e. not directly by a user. If the language script is absent, the benchmark script will (try to) download it for you. How do I run it anyways? Check the Slurm script for your favorite language. In general, you can run it as such: [ interpreter ] [ script_name ] [ number_of_cores ] [ grid_size ] On a login node, use 1 core and a grid size of 1 to start the lightest calculation possible: julia integration2d.jl 1 1 Rscript integration2d.R 1 1 Language Script with calculation Julia do_2d_integration.jl MATLAB do_2d_integration.m R do_2d_integration.R","title":"Language script"},{"location":"advanced/thread_parallelism/#exercises","text":"","title":"Exercises"},{"location":"advanced/thread_parallelism/#exercise-1-start-the-benchmark-on-your-hpc-cluster","text":"The goal of this exercise is to start the benchmark script on your HPC cluster, as well as some troubleshooting. On your HPC cluster: Download the benchmark script How to do that? There are many ways to do so. One way is to download it directly from this course\u2019s repository : wget https://raw.githubusercontent.com/UPPMAX/R-matlab-julia-HPC/refs/heads/main/docs/advanced/thread_parallelism/benchmark_2d_integration.sh Run the benchmark script. Tip: see the \u2018Benchmark script\u2019 section . How to do that? The \u2018Benchmark script\u2019 section shows how: bash benchmark_2d_integration.sh staff r You can use our projects overview page to find the course NAISS project for your HPC cluster. Check the Slurm output files for problems. If there are problems: fix these, then run the benchmark script again How to do that? There are many ways to do so. One way is to show all files with the .out extension: cat *.out","title":"Exercise 1: start the benchmark on your HPC cluster"},{"location":"advanced/thread_parallelism/#exercise-2-read-the-benchmark-script","text":"Now that the benchmark script is running, we have the time to figure out what it is doing. What is the most important single line in this script, i.e. the line it is all about? Tip: start looking from the bottom of the script Answer For all HPC clusters except Dardel: sbatch -A \" ${ slurm_job_account } \" -N \" ${ n_nodes } \" -n \" ${ n_cores } \" \" ${ slurm_script_name } \" For the Dardel HPC cluster: sbatch -A \" ${ slurm_job_account } \" -N \" ${ n_nodes } \" --ntasks \" ${ n_cores } \" -p shared \" ${ slurm_script_name } \" In English, describe what the line does in general terms Answer Schedule to run \u2026 on some account with some amount of nodes with some amount of cores (on Dardel) on the main partition a script with some name This line of code is part of a for loop. In English, what does the for loop achieve? Answer The for loop achieves that the same calculation is scheduled to be done with 1 core, then with 2 cores, then with 3 cores, etc., to 64 cores.","title":"Exercise 2: read the benchmark script"},{"location":"advanced/thread_parallelism/#exercise-3-read-the-slurm-script","text":"The benchmark script submits a Slurm script of your favorite language multiple times to the queue: once with 1 cores, once with 2 cores, etc. What is the most important single line in this script, i.e. the line it is all about? Tip: start looking from the bottom of the script Answer The last line. Language Most important line Julia julia --threads \"${SLURM_NPROCS}\" do_2d_integration.jl \"${SLURM_NPROCS}\" MATLAB matlab -nodisplay -nosplash -nojvm -batch \"run(\\\"${matlab_target_filename}\\\"); exit;\" R Rscript --no-save --no-restore do_2d_integration.R \"${SLURM_NPROCS}\" In English, describe what the line does in general terms. Tip: this is the same answer for all programming languages. Tip 2: assume \u2018procedure\u2019 is synonym for \u2018core\u2019. Tip 3: the Julia line is closest to English. Answer Run a Julia/MATLAB/R script for the booked number of cores, without doing anything else (e.g. showing a splash screen or restoring a computational environment).","title":"Exercise 3: read the Slurm script"},{"location":"advanced/thread_parallelism/#exercise-4-read-the-language-script","text":"The Slurm script runs a script of your favorite language for a specified number of cores. Locate the lines of code that make the calculation perform in parallel. Answer Julia MATLAB R Threads . @threads for worker_index = 1 : n_workers results [ worker_index ] = integration2d ( grid_size , n_workers , worker_index ) end parfor worker_index = 1 : n_workers partial_results ( worker_index ) = integration2d ( grid_size , n_workers , worker_index ); end results_of_workers <- foreach ( worker_index = 1 : n_workers , .combine = c ) %dopar% { integration2d ( grid_size , n_workers , worker_index ) } In English, describe what these lines does in general terms. Answer For each available worker: per worker, do part of a calculation and combine the results Optional: what is grid_size ? What does it do? What would be a better variable name? Answer grid_size determines the accuracy of the calculation: the bigger grid_size , the smaller intervals will be integrated. A better variable name could be accuracy . However, with such a variable name, there is no natural understanding that the range of its value goes from 1 to infinity. For the name grid_size , this range is easier to feel right, as sizes are non-zero positive values by nature Locate the keyword that make the calculation perform in parallel. Or: locate the word that, when removed, would \u2018downgrade\u2019 the calculation to be single-threaded. Answer The last line. Language Keyword to indicate a parallel calculation Julia Threads.@threads MATLAB parfor R %dopar% The function that is run in parallel (i.e. integration2d ) is made suitable to be run in parallel. In English, describe which changes are made to make it suitable. Answer The function has three (instead of one) arguments: Language Function signature Julia function integration2d(grid_size::Int, n_workers::Int, worker_index::Int) MATLAB function integration2d(grid_size, n_workers, worker_index) R integration2d <- function(grid_size, n_workers, worker_index) The extra arguments are n_workers and worker_index . These allow the worker_index -th worker to know which share of the calculation it must do. What do these changes achieve? Answer These changes allow each thread to know what it needs to know to run its part of the calculation. Bonus: do you spot the bug in integrate2d ? Is this a problem? Why would someone keep it in anyways? Answer The calculation of begin_index and end_index : Julia MATLAB R workload = fld ( grid_size , n_workers ) begin_index = workload * ( worker_index - 1 ) + 1 end_index = workload * worker_index grid_cells_per_worker = floor ( grid_size / n_workers ); begin_index = grid_cells_per_worker * ( worker_index - 1 ) + 1 ; end_index = grid_cells_per_worker * worker_index ; grid_cells_per_worker <- floor ( grid_size / n_workers ) begin_index <- grid_cells_per_worker * ( worker_index - 1 ) + 1 end_index <- grid_cells_per_worker * worker_index To most clearly demonstrate this, imagine a grid_size of 3 for an n_workers of 2: Variable name Value grid_size 3 n_workers 2 grid_cells_per_worker 3 / 2 (rounded down) = 1 begin_index for first worker 1 * (1 - 1) + 1 = 1 end_index for first worker 1 * 1 = 1 begin_index for second worker 1 * (2 - 1) + 1 = 2 end_index for second worker 1 * 2 = 2 This means that, although the calculation is split in 3 parts, only 2 of these are performed. For bigger grid sizes, however, this problem gets less. One would keep such a bug in for readability: this session is about thread parallelism, not about the extensive calculation of these indices.","title":"Exercise 4: read the language script"},{"location":"advanced/thread_parallelism/#exercise-5-share-the-results","text":"By now, some of the calculations in exercise 1 will be finished. If not: no worries, just continue! In the terminal on your favorite HPC cluster, run the following command to collect all results: grep -EoRh \"^[jmlr].*,.*\" --include = *.out | sort | uniq Copy-paste these results to our shared HackMD document .","title":"Exercise 5: share the results"},{"location":"advanced/thread_parallelism/#exercise-6-analyse-the-results","text":"Take it easy Doing this analysis yourself is useful, if you are fluent in analysing data. If not, you are encouraged to use: This spreadsheet This R script Copy-paste the results of the previous exercise into a comma-separated file called my_results.csv . These are the descriptions of the variables: Parameter Value language Your programming language hpc_cluster Your HPC cluster grid_size Accuracy n_workers Number of cores used core_secs Core seconds used, i.e. the time used by all cores together Load the comma-separated file ( my_results.csv ) in a spreadsheet or read it in your favorite programming language Add a column called wall_clock_sec , which equals core_secs divided by n_workers . wall_clock_sec is the time it took the calculation to complete Add a column called speedup , which equals the wall clock time for 1 core divided by the wall clock time of that amount of cores Plot the speedup (on the y axis) per number of workers (on the x axis). Compare your speedup with the Amdahl\u2019s Law figure of the previous session, above \u2018Exercises\u2019 ). What do you estimate is the maximum speedup? What do you estimate is the percentage of code that can be parallelized (i.e. the \u2018parallel portion\u2019 in the figure of Amdahl\u2019s Law)?","title":"Exercise 6: analyse the results"},{"location":"advanced/thread_parallelism/#exercise-7-compare-to-others","text":"Compare your results to others, that ran the same benchmark, yet for other HPC clusters and other programming languages: Total core seconds Efficiency Speedup Benchmark: the total core seconds per number of workers Benchmark: Efficiency per number of workers Benchmark: Speedup per number of workers What do you notice? Answer This question is vague on purpose: there are many things going on here and many answers are correct: Measurements are messier than you may have thought Julia is 30x faster than R The MATLAB code is single-threaded R works best at being efficient in parallel HPC clusters differ The points on Kebnekaise differ more than other HPC clusters: this is because Kebnekaise is the most heterogeneous (i.e. has the highest number of different hardware)","title":"Exercise 7: compare to others"},{"location":"advanced/thread_parallelism/#exercise-x1-job-scheduling-problem","text":"You have just submitted some multithreaded jobs to the queue. What went wrong here? Why is this a problem? [richel@pelle1 thread_parallelism]$ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 54197 pelle do_r_2d_ richel R 0:14 1 p66 54200 pelle do_r_2d_ richel R 0:14 4 p[64-67] 54216 pelle do_r_2d_ richel R 0:14 3 p[104-106] 54217 pelle do_r_2d_ richel R 0:14 6 p[106-111] 54169 pelle do_r_2d_ richel R 0:15 1 p70 Answer The multithreaded jobs are split over multiple nodes. This means that the cores booked do not have shared memory. This will make the calculation slower, as the results have to be shared by sending data between nodes.","title":"Exercise X1: job scheduling problem?"},{"location":"advanced/thread_parallelism/#exercise-x2-learn-a-faster-programming-language","text":"As can be seen in the benchmark, some programming languages are faster than others. This warrants the question: should you learn to program in the programming language that does calculations fastest? The theoretically fastest programming languages allow you to write machine code. Assembler lets you do so directly. Some other languages (most notably C, C++ and Rust) allow you to insert machine code. Hence, these are the theoretically fastest languages. To write fast code, should one learn those languages instead? Below is a figure from [Prechelt, 2000] . It shows the distribution of runtime speeds of a certain problem (called z1000 ), for different programming languages. Take a close look at the figure. The paper has an advice to yes/no learn a \u2018faster\u2019 programming language. What do you think the advice is? Answer The variance within a programming language is bigger than variance between languages (adapted fig 2, from [Prechelt, 2000] ). Instead of learning a faster language, learn how to be fast in your language. Still, the Julia code is 30x faster than the R code. Why would this advice still hold? Answer Because none of the code is optimized for speed. Both Julia and R can call C code, where C is the fastest higher-level language. It would be interesting to see how this benchmark would look like for optimized code. Are there other factors that decide which programming language to use? If yes, name some. Answer There are many reasons why to use a \u2018slower\u2019 programming language: you already know the \u2018slower\u2019 programming language you need access to specific libraries/packages you have colleagues that are willing to teach you you need to to work from code that has been written someone else","title":"Exercise X2: learn a faster programming language?"},{"location":"advanced/thread_parallelism/#where-to-go-next","text":"If you want to scale up, distributed parallelism allows you to do a calculation on many computers.","title":"Where to go next?"},{"location":"advanced/thread_parallelism/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"advanced/thread_parallelism/#t1-invalid-account-or-accountpartition-combination-specified","text":"sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified You\u2019ve specified the wrong account. Run: projinfo","title":"T1. Invalid account or account/partition combination specified"},{"location":"advanced/thread_parallelism/#t2-there-is-no-package-called-doparallel","text":"This is an R error. You can find it by checking the log files: cat *.out When you see, for example, the text below, it is clearly stated that there is no package called doParallel . HPC cluster: tetralith Slurm job account used: naiss2025-22-934 Number of cores booked in Slurm: 32 Error in library(doParallel, quietly = TRUE) : there is no package called \u2018doParallel\u2019 Execution halted To fix this: load the correct module install that package from the terminal. To load the correct module, load the R module(s) as loaded by the do_r_2d_integration.sh script, for example: module load R/4.4.0-hpc1-gcc-11.3.0-bare Could you expand on that? Open the do_r_2d_integration.sh script. Search for the part where modules are loaded, which is at the bottom. Find the lines where the modules are loaded for your favorite HPC cluster, e.g. if [ ${ hpc_cluster } == \"rackham\" ] then module load R_packages/4.1.1 >/dev/null 2 > & 1 fi Copy the part that loads the modules, excluding the > and after, and run these in a terminal on your favorite HPC cluster: module load R_packages/4.1.1 You have now loaded the packages needed for the calculation. To install that package from the terminal, check this course\u2019s material on how to do so .","title":"T2. There is no package called \u2018doParallel\u2019"},{"location":"advanced/thread_parallelism/#t3-namespace-rlang-0412-is-already-loaded-but-110-is-required","text":"Error in loadNamespace ( i, c ( lib.loc, .libPaths ()) , versionCheck = vI [[ i ]]) : namespace \u2018rlang\u2019 0 .4.12 is already loaded, but > = 1 .1.0 is required Calls: <Anonymous> ... waldo_compare -> loadNamespace -> namespaceImport -> loadNamespace Execution halted This only happens on Rackham, since 2025-09-25.","title":"T3. &lsquo;namespace \u2018rlang\u2019 0.4.12 is already loaded, but &gt;= 1.1.0 is required&rsquo;"},{"location":"advanced/thread_parallelism/#matlab-error","text":"Warning : Executing startup failed in matlabrc . This indicates a potentially serious problem in your MATLAB setup, which should be resolved as soon as possible. Error detected was: MATLAB : undefinedVarOrClass Unable to resolve the name 'java.net.InetAddress.getLocalHost.getHostAddress'. Error using run RUN cannot execute the file 'do_2d_integration.m 48'. RUN requires a valid MATLAB script","title":"MATLAB error"},{"location":"advanced/thread_parallelism/#references","text":"[CppCore Per.6] C++ Core Guidelines: Per.6: Don\u2019t make claims about performance without measurements [Prechelt, 2000] Prechelt, Lutz. \u201cAn empirical comparison of C, C++, Java, Perl, Python, REXX and TCL.\u201d IEEE Computer 33.10 (2000): 23-29.","title":"References"},{"location":"common/hpc_clusters/","text":"HPC clusters \u00b6 The HPC centers UPPMAX, HPC2N, LUNARC, NSC and PDC \u00b6 Five HPC centers There are many similarities: Login vs. calculation/compute nodes Environmental module system with software hidden until loaded with module load Slurm batch job and scheduling system \u2026 many small differences: commands to load R, Matlab and Julia and packages/libraries sometimes different versions of R, Matlab and Julia, etc. slightly different flags to Slurm \u2026 and some bigger differences: UPPMAX has three different clusters Rackham for general purpose computing on CPUs only Snowy available for local projects and suits long jobs (< 1 month) and has GPUs Bianca for sensitive data and has GPUs HPC2N has Kebnekaise with GPUs LUNARC has Cosmos with GPUs (and Cosmos-SENS) NSC has several clusters BerzeLiUs (AI/ML, NAISS) Tetralith (NAISS) Sigma (LiU local) Freja (R&D, located at SMHI) Nebula (MET Norway R&D) Stratus (weather forecasts, located at NSC) Cirrus (weather forecasts, located at SMHI) We will be using Tetralith, which also has GPUs PDC has Dardel with AMD GPUs Terminology: modules We call the applications available via the module system modules . HPC2N LUNARC NSC PDC UPPMAX Briefly about the cluster hardware and system at UPPMAX, HPC2N, LUNARC, NSC and PDC \u00b6 What is a cluster? Login nodes and calculations/computation nodes A network of computers, each computer working as a node . Each node contains several processor cores and RAM and a local disk called scratch. The user logs in to login nodes via Internet through ssh or Thinlinc. Here the file management and lighter data analysis can be performed. The calculation nodes have to be used for intense computing. Common features \u00b6 Linux kernel Bash shell x86-64 CPUs, some clusters with Intel processors and some with AMD. NVidia GPUs (HPC2N/LUNARC, also AMD) except for Dardel with AMD. HPC Cluster Kebnekaise Rackham Snowy Bianca COSMOS Tetralith Dardel Cores/compute node 28 (72 for largemem, 128/256 for AMD Zen3/Zen4) 20 16 16 48 32 128 Memory/compute node 128-3072 GB 128-1024 GB 128-4096 GB 128-512 GB 256-512 GB 96-384 GB 256-2048 GB GPU NVidia V100, A100, A6000, L40s, H100, A40, AMD MI100 None NVidia T4 NVidia A100 NVidia A100 NVidia T4 four AMD Instinct\u2122 MI250X \u00e1 2 GCDs Overview of the UPPMAX systems \u00b6 graph TB Node1 -- interactive --> SubGraph2Flow Node1 -- sbatch --> SubGraph2Flow subgraph \"Snowy\" SubGraph2Flow(calculation nodes) end ThinLinc -- usr-sensXXX + 2FA + VPN ----> SubGraph1Flow Terminal/ThinLinc -- usr --> Node1 Terminal -- usr-sensXXX + 2FA + VPN ----> SubGraph1Flow Node1 -- usr-sensXXX + 2FA + no VPN ----> SubGraph1Flow subgraph \"Bianca\" SubGraph1Flow(Bianca login) -- usr+passwd --> private(private cluster) private -- interactive --> calcB(calculation nodes) private -- sbatch --> calcB end subgraph \"Rackham\" Node1[Login] -- interactive --> Node2[calculation nodes] Node1 -- sbatch --> Node2 end Overview of the HPC2N system \u00b6 graph TB Terminal/ThinLinc -- usr --> Node1 subgraph \"Kebnekaise\" Node1[Login] -- interactive --> Node2[compute nodes] Node1 -- sbatch --> Node2 end Overview of the LUNARC system \u00b6 Overview of the NSC systems \u00b6","title":"HPC clusters"},{"location":"common/hpc_clusters/#hpc-clusters","text":"","title":"HPC clusters"},{"location":"common/hpc_clusters/#the-hpc-centers-uppmax-hpc2n-lunarc-nsc-and-pdc","text":"Five HPC centers There are many similarities: Login vs. calculation/compute nodes Environmental module system with software hidden until loaded with module load Slurm batch job and scheduling system \u2026 many small differences: commands to load R, Matlab and Julia and packages/libraries sometimes different versions of R, Matlab and Julia, etc. slightly different flags to Slurm \u2026 and some bigger differences: UPPMAX has three different clusters Rackham for general purpose computing on CPUs only Snowy available for local projects and suits long jobs (< 1 month) and has GPUs Bianca for sensitive data and has GPUs HPC2N has Kebnekaise with GPUs LUNARC has Cosmos with GPUs (and Cosmos-SENS) NSC has several clusters BerzeLiUs (AI/ML, NAISS) Tetralith (NAISS) Sigma (LiU local) Freja (R&D, located at SMHI) Nebula (MET Norway R&D) Stratus (weather forecasts, located at NSC) Cirrus (weather forecasts, located at SMHI) We will be using Tetralith, which also has GPUs PDC has Dardel with AMD GPUs Terminology: modules We call the applications available via the module system modules . HPC2N LUNARC NSC PDC UPPMAX","title":"The HPC centers UPPMAX, HPC2N, LUNARC, NSC and PDC"},{"location":"common/hpc_clusters/#briefly-about-the-cluster-hardware-and-system-at-uppmax-hpc2n-lunarc-nsc-and-pdc","text":"What is a cluster? Login nodes and calculations/computation nodes A network of computers, each computer working as a node . Each node contains several processor cores and RAM and a local disk called scratch. The user logs in to login nodes via Internet through ssh or Thinlinc. Here the file management and lighter data analysis can be performed. The calculation nodes have to be used for intense computing.","title":"Briefly about the cluster hardware and system at UPPMAX, HPC2N, LUNARC, NSC and PDC"},{"location":"common/hpc_clusters/#common-features","text":"Linux kernel Bash shell x86-64 CPUs, some clusters with Intel processors and some with AMD. NVidia GPUs (HPC2N/LUNARC, also AMD) except for Dardel with AMD. HPC Cluster Kebnekaise Rackham Snowy Bianca COSMOS Tetralith Dardel Cores/compute node 28 (72 for largemem, 128/256 for AMD Zen3/Zen4) 20 16 16 48 32 128 Memory/compute node 128-3072 GB 128-1024 GB 128-4096 GB 128-512 GB 256-512 GB 96-384 GB 256-2048 GB GPU NVidia V100, A100, A6000, L40s, H100, A40, AMD MI100 None NVidia T4 NVidia A100 NVidia A100 NVidia T4 four AMD Instinct\u2122 MI250X \u00e1 2 GCDs","title":"Common features"},{"location":"common/hpc_clusters/#overview-of-the-uppmax-systems","text":"graph TB Node1 -- interactive --> SubGraph2Flow Node1 -- sbatch --> SubGraph2Flow subgraph \"Snowy\" SubGraph2Flow(calculation nodes) end ThinLinc -- usr-sensXXX + 2FA + VPN ----> SubGraph1Flow Terminal/ThinLinc -- usr --> Node1 Terminal -- usr-sensXXX + 2FA + VPN ----> SubGraph1Flow Node1 -- usr-sensXXX + 2FA + no VPN ----> SubGraph1Flow subgraph \"Bianca\" SubGraph1Flow(Bianca login) -- usr+passwd --> private(private cluster) private -- interactive --> calcB(calculation nodes) private -- sbatch --> calcB end subgraph \"Rackham\" Node1[Login] -- interactive --> Node2[calculation nodes] Node1 -- sbatch --> Node2 end","title":"Overview of the UPPMAX systems"},{"location":"common/hpc_clusters/#overview-of-the-hpc2n-system","text":"graph TB Terminal/ThinLinc -- usr --> Node1 subgraph \"Kebnekaise\" Node1[Login] -- interactive --> Node2[compute nodes] Node1 -- sbatch --> Node2 end","title":"Overview of the HPC2N system"},{"location":"common/hpc_clusters/#overview-of-the-lunarc-system","text":"","title":"Overview of the LUNARC system"},{"location":"common/hpc_clusters/#overview-of-the-nsc-systems","text":"","title":"Overview of the NSC systems"},{"location":"common/login/","text":"Log in and other preparations \u00b6 Learning outcomes This is an optional session. If you have done all three steps, see you at 10:00 sharp! Be able to follow this course: Step 1: You have fulfilled the prerequisites (you have already done this) Step 2a : You can log in (see below) (this session) Step 2b : You can use a text editor on your HPC cluster (this session) (optional) Step 3: Use the tarball with exercises . This step is optional, as we also do this under course time Note You were invited to be part of the course project. If you already have research projects in any of the clusters you can use them. The CPU-hours required during the course will be low! Step 1: Log in \u00b6 For beginners: use the bold login method. HPC cluster Login method Documentation YouTube video Alvis SSH Documentation YouTube video Alvis Website Documentation YouTube video Bianca SSH Documentation YouTube video Bianca Website Documentation YouTube video COSMOS Local ThinLinc client Documentation YouTube video COSMOS SSH Documentation YouTube video Dardel Local ThinLinc client Documentation YouTube video Dardel SSH Documentation YouTube video Kebnekaise Local ThinLinc client Documentation YouTube video Kebnekaise SSH Documentation YouTube video Kebnekaise Website Documentation YouTube video LUMI SSH Documentation YouTube video Pelle SSH Documentation YouTube video Pelle Local ThinLinc client Documentation YouTube video Rackham Local ThinLinc client Documentation YouTube video Rackham SSH Documentation YouTube video Rackham Website Documentation YouTube video Tetralith Local ThinLinc client Documentation YouTube video Tetralith SSH Documentation YouTube video What are the differences between these login methods? These are the ways to access your HPC cluster and some of their features: How it looks like How to access your HPC cluster Features Remote desktop via a website Familiar remote desktop, clumsy, clunky, no need to install software, not available at all centers Remote desktop via a local ThinLinc client Familiar remote desktop, clumsy, need to install ThinLinc Console environment using an SSH client A console environment, powerful, need to install an SSH client Step 2: Make a work directory \u00b6 Directory names OK? NSC PDC C3SE UPPMAX HPC2N LUNARC Create a working directory where you can code along. Example. If your username is jlpicard and you are at NSC, then we recommend you create this folder: mkdir /proj/courses-fall-2025/users/jlpicard Create a working directory where you can code along. Example. If your username is sevenof9 and you are at PDC, then we recommend you create this folder: mkdir /cfs/klemming/projects/supr/courses-fall-2025/sevenof9/ Create a working directory where you can code along. Example. If your username is sevenof9 and you are at Alvis at C3SE, then we recommend you create this folder: mkdir /mimer/NOBACKUP/groups/courses-fall-2025/sevenof9/ Create a working directory where you can code along. Example. If your username is \u201cmrspock\u201d and you are at UPPMAX, then we recommend you create this folder: ``` console mkdir /proj/r-matlab-julia-pelle/mrspock/ ``` Create a working directory where you can code along. Example. If your username is bbrydsoe and you are at HPC2N, then we recommend you create this folder: mkdir /proj/nobackup/fall-courses/bbrydsoe/ Create a working directory in your home space where you can code along. Example. Create this folder: mkdir $HOME/r-matlab-julia Test an editor \u00b6 Learn how to use an text editor at Use a text editor . Download and extract the tarball with exercises \u00b6 Learn how to download and extract the tarball with exercises at Use the tarball with exercises .","title":"Login"},{"location":"common/login/#log-in-and-other-preparations","text":"Learning outcomes This is an optional session. If you have done all three steps, see you at 10:00 sharp! Be able to follow this course: Step 1: You have fulfilled the prerequisites (you have already done this) Step 2a : You can log in (see below) (this session) Step 2b : You can use a text editor on your HPC cluster (this session) (optional) Step 3: Use the tarball with exercises . This step is optional, as we also do this under course time Note You were invited to be part of the course project. If you already have research projects in any of the clusters you can use them. The CPU-hours required during the course will be low!","title":"Log in and other preparations"},{"location":"common/login/#step-1-log-in","text":"For beginners: use the bold login method. HPC cluster Login method Documentation YouTube video Alvis SSH Documentation YouTube video Alvis Website Documentation YouTube video Bianca SSH Documentation YouTube video Bianca Website Documentation YouTube video COSMOS Local ThinLinc client Documentation YouTube video COSMOS SSH Documentation YouTube video Dardel Local ThinLinc client Documentation YouTube video Dardel SSH Documentation YouTube video Kebnekaise Local ThinLinc client Documentation YouTube video Kebnekaise SSH Documentation YouTube video Kebnekaise Website Documentation YouTube video LUMI SSH Documentation YouTube video Pelle SSH Documentation YouTube video Pelle Local ThinLinc client Documentation YouTube video Rackham Local ThinLinc client Documentation YouTube video Rackham SSH Documentation YouTube video Rackham Website Documentation YouTube video Tetralith Local ThinLinc client Documentation YouTube video Tetralith SSH Documentation YouTube video What are the differences between these login methods? These are the ways to access your HPC cluster and some of their features: How it looks like How to access your HPC cluster Features Remote desktop via a website Familiar remote desktop, clumsy, clunky, no need to install software, not available at all centers Remote desktop via a local ThinLinc client Familiar remote desktop, clumsy, need to install ThinLinc Console environment using an SSH client A console environment, powerful, need to install an SSH client","title":"Step 1: Log in"},{"location":"common/login/#step-2-make-a-work-directory","text":"Directory names OK? NSC PDC C3SE UPPMAX HPC2N LUNARC Create a working directory where you can code along. Example. If your username is jlpicard and you are at NSC, then we recommend you create this folder: mkdir /proj/courses-fall-2025/users/jlpicard Create a working directory where you can code along. Example. If your username is sevenof9 and you are at PDC, then we recommend you create this folder: mkdir /cfs/klemming/projects/supr/courses-fall-2025/sevenof9/ Create a working directory where you can code along. Example. If your username is sevenof9 and you are at Alvis at C3SE, then we recommend you create this folder: mkdir /mimer/NOBACKUP/groups/courses-fall-2025/sevenof9/ Create a working directory where you can code along. Example. If your username is \u201cmrspock\u201d and you are at UPPMAX, then we recommend you create this folder: ``` console mkdir /proj/r-matlab-julia-pelle/mrspock/ ``` Create a working directory where you can code along. Example. If your username is bbrydsoe and you are at HPC2N, then we recommend you create this folder: mkdir /proj/nobackup/fall-courses/bbrydsoe/ Create a working directory in your home space where you can code along. Example. Create this folder: mkdir $HOME/r-matlab-julia","title":"Step 2: Make a work directory"},{"location":"common/login/#test-an-editor","text":"Learn how to use an text editor at Use a text editor .","title":"Test an editor"},{"location":"common/login/#download-and-extract-the-tarball-with-exercises","text":"Learn how to download and extract the tarball with exercises at Use the tarball with exercises .","title":"Download and extract the tarball with exercises"},{"location":"common/on-demand/","text":"On Demand Applications \u00b6 Learning Objectives What are On-Demand applications and when to use it Which interface to use on each resource and how to start them How to set the job parameters for your application What is Desktop On Demand? Is it right for my job? \u00b6 On Cosmos (LUNARC), Kebnekaise (HPC2N), and Dardel (PDC), some applications are available through one of a couple of On Demand services. On Demand applications provide an interactive environment to schedule jobs on compute nodes using a graphic user interface (GUI) instead of the typical batch submission script. How you reach this interface is dependent on the system you use and their choice of On Demand client. Cosmos and Dardel use the On-Demand Desktop developed at LUNARC, which is accessible via Thinlinc. Kebnekaise uses Open OnDemand 1 via a dedicated web portal at https://portal.hpc2n.umu.se . Desktop On-Demand is most appropriate for interactive work requiring small-to-medium amounts of computing resources. Non-interactive jobs and jobs that take more than a day or so should generally be submitted as batch jobs. If you have a longer job that requires an interactive interface to submit, make sure you keep track of the wall time limits for your facility. On-Demand applications are not accessible via SSH; you must use either Thinlinc (Cosmos and Dardel) or the dedicated web portal (Kebnekaise). On-Demand App Availability for this Course RStudio and MATLAB are both available as On-Demand applications at all 3 facilities covered on this page. On Cosmos, there are also interactive On-Demand command lines (for CPUs and GPUs) under Applications - General that you may want to use with Julia, or that may still let you use RStudio or MATLAB if for some reason those apps fail to start with the more direct methods described below. Tip Batch jobs submitted from within these interactive sessions are not bound by the same job parameters as the On-Demand interface. E.g., if you are in the MATLAB GUI and submit a job with the batch command, the batch job will not be interrupted if the GUI is closed or times out. Thinlinc Access Limited on Dardel (PDC) Here we focus on Cosmos and Kebnekaise because access to Dardel via Thinlinc is severely restricted. Only 30 users total may have an active ThinLinc session at a time, and queue times are very long. Other PDC resources not tested in this course may be more flexible, but if you must run a program on Dardel interactively, it is better to use SSH with X-forwarding (that is, log in with ssh -X <user>@dardel.pdc.kth.se ) and then follow the workflow described in this link . Keep in mind that if you do not need a full node, you can also select a number of cores on Dardel\u2019s shared partition, which may help reduce your time in the queue. See here for information on Dardel partitions . Starting the On-Demand Interface \u00b6 COSMOS (and Dardel) Kebnekaise For most programs, the start-up process is roughly the same: Log into COSMOS (or Dardel) via your usual Thinlinc client or browser interface to start an HPC Desktop session. Click Applications in the top left corner, hover over the items prefixed with Applications - until you find your desired application (on Dardel, On-Demand applications are prefixed with PDC- ), and click it. The top-level Applications menu on Cosmos looks like this: Warning If you start a terminal session or another application from Favorites , System Tools , or other menu headings not prefixed with Applications - or PDC- , and launch an interactive program from that, it will run on a login node. Do not run intensive programs this way! To start an Open OnDemand session on Kebnekaise, Open https://portal.hpc2n.umu.se in your browser. The page looks like this: Click the blue button labeled \u201cLogin to HPC2N OnDemand\u201d. A login window should open with boxes for your login credentials. Enter your HPC2N username and password, then click \u201cSign In\u201d. You will now be on the HPC2N Open OnDemand dashboard. The top of it looks like this: Find the Interactive Apps tab in the menu bar along the top and click it to open a drop-down menu of available apps. The menu currently looks like this: Warning Unlike on Cosmos and Dardel, On-Demand applications on Kebnekaise are not reachable through Thinlinc, regardless of whether you use the desktop client or a browser! If you find similar-looking applications in the Thinlinc interface, be aware that they all run on login nodes! Setting Job Parameters \u00b6 COSMOS (and Dardel) Kebnekaise Upon clicking your chosen application, a pop-up interface called the GfxLauncher will appear and let you set the following options: Wall time - how long your interactive session will remain open. When it ends, the whole window closes immediately and any unsaved work is lost. You can select the time from a drop-down menu, or type in the time manually. On Cosmos, CPU-only applications (indicated with \u201c(CPU)\u201d in the name) can run for up to 168 hours (7 days), but the rest are limited to 48 hours. Default is 30 minutes. Requirements - how many tasks per node you need. The default is usually 1 or 4 tasks per node. There is also a gear icon to the right of this box that can pull up a second menu (see figure below) where you can set the name of your job, the number of tasks per node, the amount of memory per CPU core, and/or whether or not to use a full node. Resource - which kind of node you want in terms of the architecture (AMD or Intel) and number of cores in the CPU (or GPU). Options and defaults vary by program. Project - choose from a drop-down menu the project with which your work is associated. This is mainly to keep your usage in line with your licenses and permissions, and to send any applicable invoices to the correct PI. Licensed software will only work for projects whose group members are covered by the license. The GfxLauncher GUI (here used to launch MATLAB). The box on the left is the basic menu and the box on the right is what pops up when the gear icon next to Requirements is clicked. When you\u2019re happy with your settings, click \u201cStart\u201d. The GfxLauncher menu will stay open in the background so that you can monitor your wall time usage with the Usage bar. Leave this window open\u2014your application depends on it! Warning Closing the GfxLauncher popup after your application starts will kill the application immediately! If you want, you can also look at the associated SLURM scripts by clicking the \u201cMore\u201d button at the bottom of the GfxLauncher menu and clicking the \u201cScript\u201d tab (example below), or view the logs under the \u201cLogg\u201d tab. If an app fails to start, the first step of troubleshooting will always be to check the \u201cLogg\u201d tab. Terminals on Compute nodes If you don\u2019t see the program you want to run interactively listed under any other Applications sub-menus, or if the usual menu item fails to launch the application, you may still be able to launch it via one of the terminals under Applications - General , or the GPU Accelerated Terminal under Applications - Visualization . The CPU terminal allows for a wall time of up to 168 hours (7 days), while the two GPU terminals can only run for 48 hours (2 days) at most. For more on the specifications of the different nodes these terminals can run on, see LUNARC\u2019s webpage on COSMOS . If you finish before your wall time is up and close the app, the app should stop in the GfxLauncher window within a couple of minutes, but you can always force it to stop by clicking the \u201cStop\u201d button. This may be necessary for Jupyter Lab. If you select any of the options under \u201cInteractive apps\u201d, a page will open that looks like this (using RStudio as an example): Most of the options you have to set will be the same whether you choose RStudio Server, MATLAB Proxy, Jupyter Notebook, or even the Kebnekaise desktop. The parameters required for all apps include: Compute Project - Dropdown menu where you can choose (one of) your compute projects to launch with. Number of Hours - Wall time. The maximum is 12 hours, but you should avoid using more than you need to conserve your allocation and minimize queuing time. Node type - Choose from options described below the dropdown menu. If you pick \u201cany GPU\u201d, leave \u201cNumber of Cores\u201d empty. Number of Cores - Choose any number up to 28. Each core has 4GB of memory. This is only a valid field if you pick \u201cany\u201d or \u201cLarge memory\u201d for the \u201cNode type\u201d selection. Working directory - Default is $HOME. You can either type a full path manually or click \u201cSelect Path\u201d to open a file browser if you are unsure of the full path. \u201cI would like to receive an email when my job starts\u201d - Check box if you agree. For some apps, like RStudio and Jupyter Notebook, you will also see an option to choose a Runtime environment. Choose from \u201cSystem provided\u201d, \u201cProject provided\u201d, or \u201cUser provided\u201d. If you or your project do not have a custom environment (with specific Python or R packages, for instance), then use \u201cSystem provided\u201d. Once you enter your desired parameters, click Launch . If the parameters are all valid, the page will reload and looks something like this for as long as your job is in the queue (using RStudio as an example): When the job starts, the title bar of the box containing your job will turn from blue to green, the status message will change from \u201cQueued\u201d to \u201cRunning\u201d, and the number of nodes and cores with appear in the title bar. You can have more than one OnDemand job running or queued. Running jobs will look like these: For all apps, the equivalent of a start button will be a bright blue rectangle near the bottom of the job box, though the words on the button may vary. When you click this button, your app should launch in a new window. Important Closing the GUI window for your app before time runs out (e.g. the MATLAB GUI or the browser for Jupyter Notebook) does not stop your job or release the resources associated with it! If you want to stop your job and avoid spending any more of your resource budget on it, you must click the red \u201cDelete\u201d button near the top right of your interactive job listing. Otherwise, you can reopen any closed app as long as time remains in the job allocated for it. Open OnDemand is a web service that allows HPC users to schedule jobs, run notebooks and work interactively on a remote cluster from any device that supports a modern browser. The Open OnDemand project was funded by NSF and is currently maintained by the Ohio SuperComputing Centre. Read more about OpenOndemand.org . \u21a9","title":"Desktop on demand"},{"location":"common/on-demand/#on-demand-applications","text":"Learning Objectives What are On-Demand applications and when to use it Which interface to use on each resource and how to start them How to set the job parameters for your application","title":"On Demand Applications"},{"location":"common/on-demand/#what-is-desktop-on-demand-is-it-right-for-my-job","text":"On Cosmos (LUNARC), Kebnekaise (HPC2N), and Dardel (PDC), some applications are available through one of a couple of On Demand services. On Demand applications provide an interactive environment to schedule jobs on compute nodes using a graphic user interface (GUI) instead of the typical batch submission script. How you reach this interface is dependent on the system you use and their choice of On Demand client. Cosmos and Dardel use the On-Demand Desktop developed at LUNARC, which is accessible via Thinlinc. Kebnekaise uses Open OnDemand 1 via a dedicated web portal at https://portal.hpc2n.umu.se . Desktop On-Demand is most appropriate for interactive work requiring small-to-medium amounts of computing resources. Non-interactive jobs and jobs that take more than a day or so should generally be submitted as batch jobs. If you have a longer job that requires an interactive interface to submit, make sure you keep track of the wall time limits for your facility. On-Demand applications are not accessible via SSH; you must use either Thinlinc (Cosmos and Dardel) or the dedicated web portal (Kebnekaise). On-Demand App Availability for this Course RStudio and MATLAB are both available as On-Demand applications at all 3 facilities covered on this page. On Cosmos, there are also interactive On-Demand command lines (for CPUs and GPUs) under Applications - General that you may want to use with Julia, or that may still let you use RStudio or MATLAB if for some reason those apps fail to start with the more direct methods described below. Tip Batch jobs submitted from within these interactive sessions are not bound by the same job parameters as the On-Demand interface. E.g., if you are in the MATLAB GUI and submit a job with the batch command, the batch job will not be interrupted if the GUI is closed or times out. Thinlinc Access Limited on Dardel (PDC) Here we focus on Cosmos and Kebnekaise because access to Dardel via Thinlinc is severely restricted. Only 30 users total may have an active ThinLinc session at a time, and queue times are very long. Other PDC resources not tested in this course may be more flexible, but if you must run a program on Dardel interactively, it is better to use SSH with X-forwarding (that is, log in with ssh -X <user>@dardel.pdc.kth.se ) and then follow the workflow described in this link . Keep in mind that if you do not need a full node, you can also select a number of cores on Dardel\u2019s shared partition, which may help reduce your time in the queue. See here for information on Dardel partitions .","title":"What is Desktop On Demand? Is it right for my job?"},{"location":"common/on-demand/#starting-the-on-demand-interface","text":"COSMOS (and Dardel) Kebnekaise For most programs, the start-up process is roughly the same: Log into COSMOS (or Dardel) via your usual Thinlinc client or browser interface to start an HPC Desktop session. Click Applications in the top left corner, hover over the items prefixed with Applications - until you find your desired application (on Dardel, On-Demand applications are prefixed with PDC- ), and click it. The top-level Applications menu on Cosmos looks like this: Warning If you start a terminal session or another application from Favorites , System Tools , or other menu headings not prefixed with Applications - or PDC- , and launch an interactive program from that, it will run on a login node. Do not run intensive programs this way! To start an Open OnDemand session on Kebnekaise, Open https://portal.hpc2n.umu.se in your browser. The page looks like this: Click the blue button labeled \u201cLogin to HPC2N OnDemand\u201d. A login window should open with boxes for your login credentials. Enter your HPC2N username and password, then click \u201cSign In\u201d. You will now be on the HPC2N Open OnDemand dashboard. The top of it looks like this: Find the Interactive Apps tab in the menu bar along the top and click it to open a drop-down menu of available apps. The menu currently looks like this: Warning Unlike on Cosmos and Dardel, On-Demand applications on Kebnekaise are not reachable through Thinlinc, regardless of whether you use the desktop client or a browser! If you find similar-looking applications in the Thinlinc interface, be aware that they all run on login nodes!","title":"Starting the On-Demand Interface"},{"location":"common/on-demand/#setting-job-parameters","text":"COSMOS (and Dardel) Kebnekaise Upon clicking your chosen application, a pop-up interface called the GfxLauncher will appear and let you set the following options: Wall time - how long your interactive session will remain open. When it ends, the whole window closes immediately and any unsaved work is lost. You can select the time from a drop-down menu, or type in the time manually. On Cosmos, CPU-only applications (indicated with \u201c(CPU)\u201d in the name) can run for up to 168 hours (7 days), but the rest are limited to 48 hours. Default is 30 minutes. Requirements - how many tasks per node you need. The default is usually 1 or 4 tasks per node. There is also a gear icon to the right of this box that can pull up a second menu (see figure below) where you can set the name of your job, the number of tasks per node, the amount of memory per CPU core, and/or whether or not to use a full node. Resource - which kind of node you want in terms of the architecture (AMD or Intel) and number of cores in the CPU (or GPU). Options and defaults vary by program. Project - choose from a drop-down menu the project with which your work is associated. This is mainly to keep your usage in line with your licenses and permissions, and to send any applicable invoices to the correct PI. Licensed software will only work for projects whose group members are covered by the license. The GfxLauncher GUI (here used to launch MATLAB). The box on the left is the basic menu and the box on the right is what pops up when the gear icon next to Requirements is clicked. When you\u2019re happy with your settings, click \u201cStart\u201d. The GfxLauncher menu will stay open in the background so that you can monitor your wall time usage with the Usage bar. Leave this window open\u2014your application depends on it! Warning Closing the GfxLauncher popup after your application starts will kill the application immediately! If you want, you can also look at the associated SLURM scripts by clicking the \u201cMore\u201d button at the bottom of the GfxLauncher menu and clicking the \u201cScript\u201d tab (example below), or view the logs under the \u201cLogg\u201d tab. If an app fails to start, the first step of troubleshooting will always be to check the \u201cLogg\u201d tab. Terminals on Compute nodes If you don\u2019t see the program you want to run interactively listed under any other Applications sub-menus, or if the usual menu item fails to launch the application, you may still be able to launch it via one of the terminals under Applications - General , or the GPU Accelerated Terminal under Applications - Visualization . The CPU terminal allows for a wall time of up to 168 hours (7 days), while the two GPU terminals can only run for 48 hours (2 days) at most. For more on the specifications of the different nodes these terminals can run on, see LUNARC\u2019s webpage on COSMOS . If you finish before your wall time is up and close the app, the app should stop in the GfxLauncher window within a couple of minutes, but you can always force it to stop by clicking the \u201cStop\u201d button. This may be necessary for Jupyter Lab. If you select any of the options under \u201cInteractive apps\u201d, a page will open that looks like this (using RStudio as an example): Most of the options you have to set will be the same whether you choose RStudio Server, MATLAB Proxy, Jupyter Notebook, or even the Kebnekaise desktop. The parameters required for all apps include: Compute Project - Dropdown menu where you can choose (one of) your compute projects to launch with. Number of Hours - Wall time. The maximum is 12 hours, but you should avoid using more than you need to conserve your allocation and minimize queuing time. Node type - Choose from options described below the dropdown menu. If you pick \u201cany GPU\u201d, leave \u201cNumber of Cores\u201d empty. Number of Cores - Choose any number up to 28. Each core has 4GB of memory. This is only a valid field if you pick \u201cany\u201d or \u201cLarge memory\u201d for the \u201cNode type\u201d selection. Working directory - Default is $HOME. You can either type a full path manually or click \u201cSelect Path\u201d to open a file browser if you are unsure of the full path. \u201cI would like to receive an email when my job starts\u201d - Check box if you agree. For some apps, like RStudio and Jupyter Notebook, you will also see an option to choose a Runtime environment. Choose from \u201cSystem provided\u201d, \u201cProject provided\u201d, or \u201cUser provided\u201d. If you or your project do not have a custom environment (with specific Python or R packages, for instance), then use \u201cSystem provided\u201d. Once you enter your desired parameters, click Launch . If the parameters are all valid, the page will reload and looks something like this for as long as your job is in the queue (using RStudio as an example): When the job starts, the title bar of the box containing your job will turn from blue to green, the status message will change from \u201cQueued\u201d to \u201cRunning\u201d, and the number of nodes and cores with appear in the title bar. You can have more than one OnDemand job running or queued. Running jobs will look like these: For all apps, the equivalent of a start button will be a bright blue rectangle near the bottom of the job box, though the words on the button may vary. When you click this button, your app should launch in a new window. Important Closing the GUI window for your app before time runs out (e.g. the MATLAB GUI or the browser for Jupyter Notebook) does not stop your job or release the resources associated with it! If you want to stop your job and avoid spending any more of your resource budget on it, you must click the red \u201cDelete\u201d button near the top right of your interactive job listing. Otherwise, you can reopen any closed app as long as time remains in the job allocated for it. Open OnDemand is a web service that allows HPC users to schedule jobs, run notebooks and work interactively on a remote cluster from any device that supports a modern browser. The Open OnDemand project was funded by NSF and is currently maintained by the Ohio SuperComputing Centre. Read more about OpenOndemand.org . \u21a9","title":"Setting Job Parameters"},{"location":"common/other_courses/","text":"Other courses \u00b6 All courses by NAISS centers can be found at the SCoRe overview of courses . Below, we mention some courses and course material specifically. NAISS courses \u00b6 NAISS Introduction to Linux NAISS file transfer course Linux material by the NAISS HPC centers \u00b6 UPPMAX Linux documentation UPPMAX basic Linux commands documentation Introduction to UPPMAX, which has a day on Linux HPC2N intro course material (including link to recordings) HPC2N YouTube channel video on Linux HPC2N Linux cheat sheet LUNARC Intro to Linux shells and commands LUNARC Useful Linux commands PDC intro course material Material for improving your programming skills \u00b6 First level \u00b6 The Carpentries teaches basic lab skills for research computing: Software carpentry courses/material Second level \u00b6 Code Refinery develops and maintains training material on software best practices for researchers that already write code. Their material addresses all academic disciplines and tries to be as programming language-independent as possible. Code Refinery lessons Third level \u00b6 ENCCS (EuroCC National Competence Centre Sweden) is a national centre that supports industry, public administration and academia accessing and using European supercomputers. They give higher-level training of programming and specific software. ENCCS training material","title":"Other courses"},{"location":"common/other_courses/#other-courses","text":"All courses by NAISS centers can be found at the SCoRe overview of courses . Below, we mention some courses and course material specifically.","title":"Other courses"},{"location":"common/other_courses/#naiss-courses","text":"NAISS Introduction to Linux NAISS file transfer course","title":"NAISS courses"},{"location":"common/other_courses/#linux-material-by-the-naiss-hpc-centers","text":"UPPMAX Linux documentation UPPMAX basic Linux commands documentation Introduction to UPPMAX, which has a day on Linux HPC2N intro course material (including link to recordings) HPC2N YouTube channel video on Linux HPC2N Linux cheat sheet LUNARC Intro to Linux shells and commands LUNARC Useful Linux commands PDC intro course material","title":"Linux material by the NAISS HPC centers"},{"location":"common/other_courses/#material-for-improving-your-programming-skills","text":"","title":"Material for improving your programming skills"},{"location":"common/other_courses/#first-level","text":"The Carpentries teaches basic lab skills for research computing: Software carpentry courses/material","title":"First level"},{"location":"common/other_courses/#second-level","text":"Code Refinery develops and maintains training material on software best practices for researchers that already write code. Their material addresses all academic disciplines and tries to be as programming language-independent as possible. Code Refinery lessons","title":"Second level"},{"location":"common/other_courses/#third-level","text":"ENCCS (EuroCC National Competence Centre Sweden) is a national centre that supports industry, public administration and academia accessing and using European supercomputers. They give higher-level training of programming and specific software. ENCCS training material","title":"Third level"},{"location":"common/use_tarball/","text":"Use the tarball with exercises \u00b6 Goal You can run the example files needed for the exercises A tarball is a file that contains multiple files, similar to a zip file. To use the files it contains, it needs to be untarred/unzipped/uncompressed first. Procedure \u00b6 Prefer a video? This part of this YouTube video shows you this procedure. The procedure has these steps: Get the tarball Uncompress the tarball Step 1: get the tarball \u00b6 In a terminal, cd to a good directory to keep the exercises (for instance in your just created folder in the project directory) You may create a new folder ( mkdir ), called exercises or similar). Use the following command to download the file to your current folder: R MATLAB (wait until that day) Julia (wait until that day) wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesR.tar.gz wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesMatlab.tar.gz wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesJulia.tar.gz How does that look like? Your output will look somewhat like this: [ sven@rackham3 ~ ] $ wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesR.tar.gz --2024-10-23 11 :49:30-- https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesR.tar.gz Resolving github.com ( github.com ) ... 4 .225.11.194 Connecting to github.com ( github.com ) | 4 .225.11.194 | :443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/UPPMAX/R-matlab-julia-HPC/refs/heads/main/exercises/exercisesR.tar.gz [ following ] --2024-10-23 11 :49:30-- [ URL ] Resolving raw.githubusercontent.com ( raw.githubusercontent.com ) ... 185 .199.111.133, 185 .199.108.133, 185 .199.109.133, ... Connecting to raw.githubusercontent.com ( raw.githubusercontent.com ) | 185 .199.111.133 | :443... connected. HTTP request sent, awaiting response... 200 OK Length: 56007 ( 55K ) [ application/octet-stream ] Saving to: \u2018exercisesR.tar.gz\u2019 100 % [====================================== > ] 56 ,007 --.-K/s in 0 .002s 2024 -10-23 11 :49:30 ( 31 .4 MB/s ) - \u2018exercisesR.tar.gz\u2019 saved [ 56007 /56007 ] Step 2: Uncompress the tarball \u00b6 In a terminal, use the following command to uncompress the file: R MATLAB (wait until that day) Julia (wait until that day) tar -xvzf exercisesR.tar.gz tar -xvzf exercisesMatlab.tar.gz tar -xvzf exercisesJulia.tar.gz How does that look like? Your output will look similar to this: R MATLAB Julia [ sven@rackham3 ~ ] $ tar -xvzf exercisesR.tar.gz r/iris_ml-rackham.sh r/Rscript_ML-kebnekaise.sh r/hello.R r/script-df.R r/add2-cosmos.sh r/Rscript_ML-cosmos.sh r/script-df-rackham.sh r/serial-rackham.sh r/iris.csv r/Rmpi-cosmos.sh r/validation-cosmos.sh r/validation-rackham.sh r/parallel_foreach.R r/serial_sum.R r/iris_ml.R r/serial.R r/serial-cosmos.sh r/parallel_foreach-cosmos.sh r/clusterapply.R r/parallel_foreach-kebnekaise.sh r/add2-kebnekaise.sh r/validation-kebnekaise.sh r/README.md r/Rmpi.R r/Rmpi-kebnekaise.sh r/serial-kebnekaise.sh r/Rscript_ML-rackham.sh r/parallel_foreach-rackham.sh r/script-df-kebnekaise.sh r/add2.R r/Rscript.R r/add2-rackham.sh r/sleep.R r/script-df-fixme.R r/Rmpi-rackham.sh r/iris_ml-kebnekaise.sh r/iris_ml-cosmos.sh r/validation.R r/script-df-cosmos.sh [ sven@rackham3 ~ ] $ tar -xvzf exercisesMatlab.tar.gz matlab/ matlab/parallel_example.m matlab/example-parallel-matlab.sh matlab/serial-monte-rackham.sh matlab/parallel_example-rackham.sh matlab/serial-monte-kebnekaise.sh matlab/parallel_example-kebnekaise.sh matlab/mmult.m matlab/parfeval_mean.m matlab/monte_carlo_pi.m matlab/parallel_example-cosmos.sh matlab/parfor-greet.m matlab/MorePractice.rst matlab/add2.m matlab/serial-monte-cosmos.sh matlab/dice_stats_par.m [ sven@rackham3 ~ ] $ tar -xvzf exercisesJulia.tar.gz julia/ julia/script-df-rackham.sh julia/parallelJulia/ julia/parallelJulia/solution/ julia/parallelJulia/solution/script-df-sol.jl julia/parallelJulia/script-df.jl julia/parallelJulia/runHPC2N.sh julia/parallelJulia/runUPPMAX.sh julia/parallelJulia/1.md julia/script-df.jl julia/batchJulia/ julia/batchJulia/3.md julia/batchJulia/2.md julia/batchJulia/3.uppmax-batch-script.sh julia/batchJulia/serial-sum.jl julia/batchJulia/3.kebnekaise-batch-script.sh julia/batchJulia/script-gpu.jl julia/batchJulia/Solutions/ julia/batchJulia/Solutions/2/ julia/batchJulia/Solutions/2/Kebnekaise.md julia/batchJulia/Solutions/2/Rackham.sh julia/batchJulia/Solutions/3/ julia/batchJulia/Solutions/3/3.kebnekaise-batch-script.sh julia/batchJulia/Solutions/3/Solution.md julia/batchJulia/Solutions/1/ julia/batchJulia/Solutions/1/Rackham.md julia/batchJulia/Solutions/1/Kebnekaise.md julia/batchJulia/1.md julia/README.md julia/sleep-threads.jl julia/script-df-kebnekaise.sh julia/script-df-fixme.jl julia/isolatedJulia/ julia/isolatedJulia/2.md julia/isolatedJulia/Solutions/ julia/isolatedJulia/Solutions/2/ julia/isolatedJulia/Solutions/2/Solution.md julia/isolatedJulia/Solutions/1/ julia/isolatedJulia/Solutions/1/Solution.md julia/isolatedJulia/1.md julia/loadRun/ julia/loadRun/2.md julia/loadRun/serial-sum.jl julia/loadRun/Solutions/ julia/loadRun/Solutions/2/ julia/loadRun/Solutions/2/Rackham.md julia/loadRun/Solutions/2/Kebnekaise.md julia/loadRun/Solutions/1/ julia/loadRun/Solutions/1/Solution.md julia/loadRun/1.md After decompressing, there is a folder called r , or matlab or julia that contains the exercises. Do you want the whole repo? If you are happy with just the exercises, the tarballs of the language specific ones are enough. By cloning the whole repo, you get all the materials, planning documents, and exercises. If you think this makes sense type this in the command line in the directory you want it: git clone https://github.com/UPPMAX/R-matlab-julia-HPC.git Note however, that if you during exercise work modify files, they will be overwritten if you make git pull (like if the teacher needs to modify something). If this is the case, then make a copy somewhere else with your answers!","title":"Use the tarball"},{"location":"common/use_tarball/#use-the-tarball-with-exercises","text":"Goal You can run the example files needed for the exercises A tarball is a file that contains multiple files, similar to a zip file. To use the files it contains, it needs to be untarred/unzipped/uncompressed first.","title":"Use the tarball with exercises"},{"location":"common/use_tarball/#procedure","text":"Prefer a video? This part of this YouTube video shows you this procedure. The procedure has these steps: Get the tarball Uncompress the tarball","title":"Procedure"},{"location":"common/use_tarball/#step-1-get-the-tarball","text":"In a terminal, cd to a good directory to keep the exercises (for instance in your just created folder in the project directory) You may create a new folder ( mkdir ), called exercises or similar). Use the following command to download the file to your current folder: R MATLAB (wait until that day) Julia (wait until that day) wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesR.tar.gz wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesMatlab.tar.gz wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesJulia.tar.gz How does that look like? Your output will look somewhat like this: [ sven@rackham3 ~ ] $ wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesR.tar.gz --2024-10-23 11 :49:30-- https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/exercises/exercisesR.tar.gz Resolving github.com ( github.com ) ... 4 .225.11.194 Connecting to github.com ( github.com ) | 4 .225.11.194 | :443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/UPPMAX/R-matlab-julia-HPC/refs/heads/main/exercises/exercisesR.tar.gz [ following ] --2024-10-23 11 :49:30-- [ URL ] Resolving raw.githubusercontent.com ( raw.githubusercontent.com ) ... 185 .199.111.133, 185 .199.108.133, 185 .199.109.133, ... Connecting to raw.githubusercontent.com ( raw.githubusercontent.com ) | 185 .199.111.133 | :443... connected. HTTP request sent, awaiting response... 200 OK Length: 56007 ( 55K ) [ application/octet-stream ] Saving to: \u2018exercisesR.tar.gz\u2019 100 % [====================================== > ] 56 ,007 --.-K/s in 0 .002s 2024 -10-23 11 :49:30 ( 31 .4 MB/s ) - \u2018exercisesR.tar.gz\u2019 saved [ 56007 /56007 ]","title":"Step 1: get the tarball"},{"location":"common/use_tarball/#step-2-uncompress-the-tarball","text":"In a terminal, use the following command to uncompress the file: R MATLAB (wait until that day) Julia (wait until that day) tar -xvzf exercisesR.tar.gz tar -xvzf exercisesMatlab.tar.gz tar -xvzf exercisesJulia.tar.gz How does that look like? Your output will look similar to this: R MATLAB Julia [ sven@rackham3 ~ ] $ tar -xvzf exercisesR.tar.gz r/iris_ml-rackham.sh r/Rscript_ML-kebnekaise.sh r/hello.R r/script-df.R r/add2-cosmos.sh r/Rscript_ML-cosmos.sh r/script-df-rackham.sh r/serial-rackham.sh r/iris.csv r/Rmpi-cosmos.sh r/validation-cosmos.sh r/validation-rackham.sh r/parallel_foreach.R r/serial_sum.R r/iris_ml.R r/serial.R r/serial-cosmos.sh r/parallel_foreach-cosmos.sh r/clusterapply.R r/parallel_foreach-kebnekaise.sh r/add2-kebnekaise.sh r/validation-kebnekaise.sh r/README.md r/Rmpi.R r/Rmpi-kebnekaise.sh r/serial-kebnekaise.sh r/Rscript_ML-rackham.sh r/parallel_foreach-rackham.sh r/script-df-kebnekaise.sh r/add2.R r/Rscript.R r/add2-rackham.sh r/sleep.R r/script-df-fixme.R r/Rmpi-rackham.sh r/iris_ml-kebnekaise.sh r/iris_ml-cosmos.sh r/validation.R r/script-df-cosmos.sh [ sven@rackham3 ~ ] $ tar -xvzf exercisesMatlab.tar.gz matlab/ matlab/parallel_example.m matlab/example-parallel-matlab.sh matlab/serial-monte-rackham.sh matlab/parallel_example-rackham.sh matlab/serial-monte-kebnekaise.sh matlab/parallel_example-kebnekaise.sh matlab/mmult.m matlab/parfeval_mean.m matlab/monte_carlo_pi.m matlab/parallel_example-cosmos.sh matlab/parfor-greet.m matlab/MorePractice.rst matlab/add2.m matlab/serial-monte-cosmos.sh matlab/dice_stats_par.m [ sven@rackham3 ~ ] $ tar -xvzf exercisesJulia.tar.gz julia/ julia/script-df-rackham.sh julia/parallelJulia/ julia/parallelJulia/solution/ julia/parallelJulia/solution/script-df-sol.jl julia/parallelJulia/script-df.jl julia/parallelJulia/runHPC2N.sh julia/parallelJulia/runUPPMAX.sh julia/parallelJulia/1.md julia/script-df.jl julia/batchJulia/ julia/batchJulia/3.md julia/batchJulia/2.md julia/batchJulia/3.uppmax-batch-script.sh julia/batchJulia/serial-sum.jl julia/batchJulia/3.kebnekaise-batch-script.sh julia/batchJulia/script-gpu.jl julia/batchJulia/Solutions/ julia/batchJulia/Solutions/2/ julia/batchJulia/Solutions/2/Kebnekaise.md julia/batchJulia/Solutions/2/Rackham.sh julia/batchJulia/Solutions/3/ julia/batchJulia/Solutions/3/3.kebnekaise-batch-script.sh julia/batchJulia/Solutions/3/Solution.md julia/batchJulia/Solutions/1/ julia/batchJulia/Solutions/1/Rackham.md julia/batchJulia/Solutions/1/Kebnekaise.md julia/batchJulia/1.md julia/README.md julia/sleep-threads.jl julia/script-df-kebnekaise.sh julia/script-df-fixme.jl julia/isolatedJulia/ julia/isolatedJulia/2.md julia/isolatedJulia/Solutions/ julia/isolatedJulia/Solutions/2/ julia/isolatedJulia/Solutions/2/Solution.md julia/isolatedJulia/Solutions/1/ julia/isolatedJulia/Solutions/1/Solution.md julia/isolatedJulia/1.md julia/loadRun/ julia/loadRun/2.md julia/loadRun/serial-sum.jl julia/loadRun/Solutions/ julia/loadRun/Solutions/2/ julia/loadRun/Solutions/2/Rackham.md julia/loadRun/Solutions/2/Kebnekaise.md julia/loadRun/Solutions/1/ julia/loadRun/Solutions/1/Solution.md julia/loadRun/1.md After decompressing, there is a folder called r , or matlab or julia that contains the exercises. Do you want the whole repo? If you are happy with just the exercises, the tarballs of the language specific ones are enough. By cloning the whole repo, you get all the materials, planning documents, and exercises. If you think this makes sense type this in the command line in the directory you want it: git clone https://github.com/UPPMAX/R-matlab-julia-HPC.git Note however, that if you during exercise work modify files, they will be overwritten if you make git pull (like if the teacher needs to modify something). If this is the case, then make a copy somewhere else with your answers!","title":"Step 2: Uncompress the tarball"},{"location":"common/use_text_editor/","text":"Use a text editor \u00b6 nano cheat sheet After logging in to your HPC cluster, start a terminal and type nano to start the text editor called \u2018nano\u2019. CTRL-O : save CTRL-X : quit The clusters provide these text editors on the command line: nano vi, vim emacs We recommend nano unless you are used to another editor: Text editors at HPC2N Text editors at UPPMAX Any of the above links would be helpful for you. Challenge Let\u2019s make a script with the name example.py : nano example.py Insert the following text: # This program prints Hello, world! print ( 'Hello, world!' ) Save and exit. In nano: <ctrl>+O , <ctrl>+X You can run the Python script in the shell like this: python example.py or python2 example.py","title":"Use a text editor"},{"location":"common/use_text_editor/#use-a-text-editor","text":"nano cheat sheet After logging in to your HPC cluster, start a terminal and type nano to start the text editor called \u2018nano\u2019. CTRL-O : save CTRL-X : quit The clusters provide these text editors on the command line: nano vi, vim emacs We recommend nano unless you are used to another editor: Text editors at HPC2N Text editors at UPPMAX Any of the above links would be helpful for you. Challenge Let\u2019s make a script with the name example.py : nano example.py Insert the following text: # This program prints Hello, world! print ( 'Hello, world!' ) Save and exit. In nano: <ctrl>+O , <ctrl>+X You can run the Python script in the shell like this: python example.py or python2 example.py","title":"Use a text editor"},{"location":"evaluations/","text":"Evaluations \u00b6 Iteration Date Language Evaluations 1 2023-02 All Evaluation 2 2023-10-17 Python Evaluation . 2023-10-18 Julia Evaluation . 2023-10-19 R Evaluation . 2023-10 All Evaluation 3 2024-03-12 Python Evaluation . 2024-03-13 Julia Evaluation . 2024-03-14 R Evaluation 4 2024-10-22 Python Evaluation . 2024-10-23 Julia Evaluation . 2024-10-24 R Evaluation . 2024-10-25 MATLAB Evaluation 5 2025-03-24 R Evaluation . 2025-03-25 MATLAB Evaluation . 2025-03-26 Julia Evaluation . 2025-03 All Evaluation 6 2025-10-06 R Evaluation . 2025-10-07 MATLAB Evaluation . 2025-10-08 Julia Evaluation . 2025-10-10 Advanced Evaluation","title":"Evaluations"},{"location":"evaluations/#evaluations","text":"Iteration Date Language Evaluations 1 2023-02 All Evaluation 2 2023-10-17 Python Evaluation . 2023-10-18 Julia Evaluation . 2023-10-19 R Evaluation . 2023-10 All Evaluation 3 2024-03-12 Python Evaluation . 2024-03-13 Julia Evaluation . 2024-03-14 R Evaluation 4 2024-10-22 Python Evaluation . 2024-10-23 Julia Evaluation . 2024-10-24 R Evaluation . 2024-10-25 MATLAB Evaluation 5 2025-03-24 R Evaluation . 2025-03-25 MATLAB Evaluation . 2025-03-26 Julia Evaluation . 2025-03 All Evaluation 6 2025-10-06 R Evaluation . 2025-10-07 MATLAB Evaluation . 2025-10-08 Julia Evaluation . 2025-10-10 Advanced Evaluation","title":"Evaluations"},{"location":"evaluations/202302_courses/","text":"Evaluations for the Feb 2023 version of this course \u00b6","title":"Evaluations for the Feb 2023 version of this course"},{"location":"evaluations/202302_courses/#evaluations-for-the-feb-2023-version-of-this-course","text":"","title":"Evaluations for the Feb 2023 version of this course"},{"location":"evaluations/20231017_python/","text":"Python evaluation 2023-10-17 \u00b6 Reflections: Richel Evaluation results in its original form too. Overall, how would you rate today\u2019s training event? \u00b6 Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? \u00b6 Formatted ReadTheDocs materials I liked how easy it was to ask questions, how friendly the lecturers / organizers were and that there was time for trying it yourself. The course material is really nice to read and helpful. Course materials are really nicely put together The instructions and code given in the course material on GitHub That materials were extensive and provided (both in advance and during the day). The break-out session was good to let me ask the stupid questions to fellows that could help out or make me catch up. I liked the \u2018code along\u2019 parts, when it worked and was an appropriate pace one could actually follow. Materials Very good material, but a bit chaotic Good material structure Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? \u00b6 Maybe have two streams, since most Python programmers already know about venv, etc. a little bit too much information too fast during the zoom call. It\u2019s not possible to fit in all your superb infos from the material in the course itself For some tasks, there was too much time allocated, for example for the pip installs at the start. I would suggest checking in with students more frequently about if they are done or not, and then continuing. Or students could be instructed to join the main room again when they are done. This way, if someone takes longer, you could see who is left in the breakout rooms and check in with them. Lectures should be given more time (or the contents reduced). Unfortunately it was at times a hard to follow. Perhaps, also, make it a bit more clear what is expected from the exercises. It looked like everything was rushed up and it was not clear what we were supposed to do in the breakout rooms Create one single entrance point for all the material where links to the rest are to be found. I got lost with five or more different links, folders and the information overflow as it got started. I also missed the big picture of the super computer (no previous experience), how the different systems are related, the main difference of Rackham and hp2cn/kebnekaise and so on. I felt I have too little information to make informed choices. For example, it would be easier if everybody used the same server, and only one\u2026 Much more time spent on code along and instructions, this would improve the course a lot I think! I understand and respect that for you this is very clear and it flows really easily. But for us who is watching over zoom: scrolling up and down fast while screen sharing makes it impossible to follow; as it is over zoom the instructions for the exercise part needs to be much clearer in my view, also do this - BEFORE - entering breakout rooms; additional thought about exercises is what really are the exercises, the ones stated at the end of each lecture part or the text/instructions embedded in the \u2018slides\u2019 in the instruction pages; the \u2018messages\u2019 that can be sent out to the breakout rooms are hard to notice sometimes while working on exercise, perhaps paste this in chat and/or document so one can re-read; please specify clearly how long a task \u2018should\u2019 take approx. (is the whole time in breakout rooms dedicated to the assigned task, or is the time also there for break); specify in start of each part/\u2018lecture\u2019 what the plan is maybe?; \u2018slides\u2019 can be a bit clearer I would say. Thank you, though! I understand that this is no easy feat to lecture and teach HPC over zoom. You didi a great job, but these are things I wish for at least. It will be good if you give a small introduction about different uppmax servers and what are the differences at the beginning of the course. Maybe actually follow the material that is there. It\u2019s hard for us to do exercises and go back and check later when one of the presenters just had his own material. Also, maybe make actual parallel sessions instead of going break rooms. Too much time in break rooms in general, and then other sections got rushed. The first should be how to run a simple script. \u201cbatch mode\u201d should come before \u201cvirtual environments\u201d. Also, avoid \u201cpip\u201d at the start, take that later since modules are enough for standard tasks materials Length of teaching today was \u00b6 Adequate: 75% Too short: 17% Too long: 8% Depth of content was \u00b6 Adequate: 83% Superficial: 17% The pace of teaching was \u00b6 Too fast: 50% Too slow: 25% Adequate: 25% Teaching aids used (e.g. slides) were well prepared \u00b6 Agree (completely): 83 % Disagree: 8% Hands-on exercises and demonstrations were \u00b6 OK: 50% Few: 50% Hands-on exercises and demonstrations were well prepared \u00b6 Agree (completely): 75 % Disagree: 8% How would you rate the instructors overall teaching performances? \u00b6 7.08 Do you feel you achieved your desired learning outcome? \u00b6 Yes: 67% Not sure: 33 Did today\u2019s course meet your expectation? \u00b6 Yes: 67% Not sure: 33 Do you have any additional comments? \u00b6 Overall nicely put together course No generally, the teaching speed was a bit high and then lots of waiting time in-between, but if you get lost, it\u2019s hard to catch up later when all is presented in a serial way. If the entire chain is provided (and I as participant know the path we are following), I could potentially use the breaks to catch up later and jump back in. Thanks! Drop the break out rooms and having people go there for exercises. Or just make a silent room for those that don\u2019t want to discuss and rest can stay in main room. Wastes time. Confusing with different setups for Ume\u00e5 and Uppsala. NA","title":"Python evaluation 2023-10-17"},{"location":"evaluations/20231017_python/#python-evaluation-2023-10-17","text":"Reflections: Richel Evaluation results in its original form too.","title":"Python evaluation 2023-10-17"},{"location":"evaluations/20231017_python/#overall-how-would-you-rate-todays-training-event","text":"","title":"Overall, how would you rate today&rsquo;s training event?"},{"location":"evaluations/20231017_python/#todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-what-did-you-like-best","text":"Formatted ReadTheDocs materials I liked how easy it was to ask questions, how friendly the lecturers / organizers were and that there was time for trying it yourself. The course material is really nice to read and helpful. Course materials are really nicely put together The instructions and code given in the course material on GitHub That materials were extensive and provided (both in advance and during the day). The break-out session was good to let me ask the stupid questions to fellows that could help out or make me catch up. I liked the \u2018code along\u2019 parts, when it worked and was an appropriate pace one could actually follow. Materials Very good material, but a bit chaotic Good material structure","title":"Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best?"},{"location":"evaluations/20231017_python/#todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-where-should-we-improve","text":"Maybe have two streams, since most Python programmers already know about venv, etc. a little bit too much information too fast during the zoom call. It\u2019s not possible to fit in all your superb infos from the material in the course itself For some tasks, there was too much time allocated, for example for the pip installs at the start. I would suggest checking in with students more frequently about if they are done or not, and then continuing. Or students could be instructed to join the main room again when they are done. This way, if someone takes longer, you could see who is left in the breakout rooms and check in with them. Lectures should be given more time (or the contents reduced). Unfortunately it was at times a hard to follow. Perhaps, also, make it a bit more clear what is expected from the exercises. It looked like everything was rushed up and it was not clear what we were supposed to do in the breakout rooms Create one single entrance point for all the material where links to the rest are to be found. I got lost with five or more different links, folders and the information overflow as it got started. I also missed the big picture of the super computer (no previous experience), how the different systems are related, the main difference of Rackham and hp2cn/kebnekaise and so on. I felt I have too little information to make informed choices. For example, it would be easier if everybody used the same server, and only one\u2026 Much more time spent on code along and instructions, this would improve the course a lot I think! I understand and respect that for you this is very clear and it flows really easily. But for us who is watching over zoom: scrolling up and down fast while screen sharing makes it impossible to follow; as it is over zoom the instructions for the exercise part needs to be much clearer in my view, also do this - BEFORE - entering breakout rooms; additional thought about exercises is what really are the exercises, the ones stated at the end of each lecture part or the text/instructions embedded in the \u2018slides\u2019 in the instruction pages; the \u2018messages\u2019 that can be sent out to the breakout rooms are hard to notice sometimes while working on exercise, perhaps paste this in chat and/or document so one can re-read; please specify clearly how long a task \u2018should\u2019 take approx. (is the whole time in breakout rooms dedicated to the assigned task, or is the time also there for break); specify in start of each part/\u2018lecture\u2019 what the plan is maybe?; \u2018slides\u2019 can be a bit clearer I would say. Thank you, though! I understand that this is no easy feat to lecture and teach HPC over zoom. You didi a great job, but these are things I wish for at least. It will be good if you give a small introduction about different uppmax servers and what are the differences at the beginning of the course. Maybe actually follow the material that is there. It\u2019s hard for us to do exercises and go back and check later when one of the presenters just had his own material. Also, maybe make actual parallel sessions instead of going break rooms. Too much time in break rooms in general, and then other sections got rushed. The first should be how to run a simple script. \u201cbatch mode\u201d should come before \u201cvirtual environments\u201d. Also, avoid \u201cpip\u201d at the start, take that later since modules are enough for standard tasks materials","title":"Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve?"},{"location":"evaluations/20231017_python/#length-of-teaching-today-was","text":"Adequate: 75% Too short: 17% Too long: 8%","title":"Length of teaching today was"},{"location":"evaluations/20231017_python/#depth-of-content-was","text":"Adequate: 83% Superficial: 17%","title":"Depth of content was"},{"location":"evaluations/20231017_python/#the-pace-of-teaching-was","text":"Too fast: 50% Too slow: 25% Adequate: 25%","title":"The pace of teaching was"},{"location":"evaluations/20231017_python/#teaching-aids-used-eg-slides-were-well-prepared","text":"Agree (completely): 83 % Disagree: 8%","title":"Teaching aids used (e.g. slides) were well prepared"},{"location":"evaluations/20231017_python/#hands-on-exercises-and-demonstrations-were","text":"OK: 50% Few: 50%","title":"Hands-on exercises and demonstrations were"},{"location":"evaluations/20231017_python/#hands-on-exercises-and-demonstrations-were-well-prepared","text":"Agree (completely): 75 % Disagree: 8%","title":"Hands-on exercises and demonstrations were well prepared"},{"location":"evaluations/20231017_python/#how-would-you-rate-the-instructors-overall-teaching-performances","text":"7.08","title":"How would you rate the instructors overall teaching performances?"},{"location":"evaluations/20231017_python/#do-you-feel-you-achieved-your-desired-learning-outcome","text":"Yes: 67% Not sure: 33","title":"Do you feel you achieved your desired learning outcome?"},{"location":"evaluations/20231017_python/#did-todays-course-meet-your-expectation","text":"Yes: 67% Not sure: 33","title":"Did today&rsquo;s course meet your expectation?"},{"location":"evaluations/20231017_python/#do-you-have-any-additional-comments","text":"Overall nicely put together course No generally, the teaching speed was a bit high and then lots of waiting time in-between, but if you get lost, it\u2019s hard to catch up later when all is presented in a serial way. If the entire chain is provided (and I as participant know the path we are following), I could potentially use the breaks to catch up later and jump back in. Thanks! Drop the break out rooms and having people go there for exercises. Or just make a silent room for those that don\u2019t want to discuss and rest can stay in main room. Wastes time. Confusing with different setups for Ume\u00e5 and Uppsala. NA","title":"Do you have any additional comments?"},{"location":"evaluations/20231018_julia/","text":"Julia evaluation 2023-10-18 \u00b6 Number of learners: 2 Feedback: Today was faster than yesterday. For example, teach fewer things more in-depth Remove the standard things, such as package management, etc, as it feels like a waste of time. Instead focus more on how to leverage the power of Julia on HPC, by, e.g. showing an analysis using machine learning or AI, in combination to make graphs. Have a red thread/\u2019learning line\u2019 in the course and follow that one. Pick a clear course goal","title":"Julia evaluation 2023-10-18"},{"location":"evaluations/20231018_julia/#julia-evaluation-2023-10-18","text":"Number of learners: 2 Feedback: Today was faster than yesterday. For example, teach fewer things more in-depth Remove the standard things, such as package management, etc, as it feels like a waste of time. Instead focus more on how to leverage the power of Julia on HPC, by, e.g. showing an analysis using machine learning or AI, in combination to make graphs. Have a red thread/\u2019learning line\u2019 in the course and follow that one. Pick a clear course goal","title":"Julia evaluation 2023-10-18"},{"location":"evaluations/20231019_r/","text":"R evaluation 2023-10-19 \u00b6 Reflections: Richel Spoken evaluation \u00b6 Number of learners: 12 Feedback: 2x The course is well organized! Teachers do not distinguish enough between the different UPPMAX clusters on the first day, however, this was fixed on day 3 :-) Day 3 was slightly better than Day 1 (both were good!), with the advice: on Day 1, take more for ThinLinc and interactive Everything was quite good, especially SLURM, better than some other UPPMAX course. This was mostly due to, today, doing exercises step by step and having enough time to follow ThinLinc was of correct length today, on Day 1 indeed a little bit too short All the instructors should use the same R version, because else you need to load a different R module","title":"R evaluation 2023-10-19"},{"location":"evaluations/20231019_r/#r-evaluation-2023-10-19","text":"Reflections: Richel","title":"R evaluation 2023-10-19"},{"location":"evaluations/20231019_r/#spoken-evaluation","text":"Number of learners: 12 Feedback: 2x The course is well organized! Teachers do not distinguish enough between the different UPPMAX clusters on the first day, however, this was fixed on day 3 :-) Day 3 was slightly better than Day 1 (both were good!), with the advice: on Day 1, take more for ThinLinc and interactive Everything was quite good, especially SLURM, better than some other UPPMAX course. This was mostly due to, today, doing exercises step by step and having enough time to follow ThinLinc was of correct length today, on Day 1 indeed a little bit too short All the instructors should use the same R version, because else you need to load a different R module","title":"Spoken evaluation"},{"location":"evaluations/202310_courses/","text":"2023-10 courses \u00b6 20231017_python 20231018_julia 20231019_r","title":"2023-10 courses"},{"location":"evaluations/202310_courses/#2023-10-courses","text":"20231017_python 20231018_julia 20231019_r","title":"2023-10 courses"},{"location":"evaluations/20240312_python/","text":"Evaluation \u00b6 20240312_python.xlsx","title":"Evaluation"},{"location":"evaluations/20240312_python/#evaluation","text":"20240312_python.xlsx","title":"Evaluation"},{"location":"evaluations/20240313_julia/","text":"Evaluation \u00b6 20240313_julia.xlsx","title":"Evaluation"},{"location":"evaluations/20240313_julia/#evaluation","text":"20240313_julia.xlsx","title":"Evaluation"},{"location":"evaluations/20240314_r/","text":"Evaluation \u00b6 20240314_r.xlsx","title":"Evaluation"},{"location":"evaluations/20240314_r/#evaluation","text":"20240314_r.xlsx","title":"Evaluation"},{"location":"evaluations/20241022_python/","text":"Evaluation 2024-10-22: Python \u00b6 Reflections: Richel","title":"Evaluation 2024-10-22: Python"},{"location":"evaluations/20241022_python/#evaluation-2024-10-22-python","text":"Reflections: Richel","title":"Evaluation 2024-10-22: Python"},{"location":"evaluations/20241023_julia/","text":"Evaluation 2024-10-23: Julia \u00b6","title":"Evaluation 2024-10-23: Julia"},{"location":"evaluations/20241023_julia/#evaluation-2024-10-23-julia","text":"","title":"Evaluation 2024-10-23: Julia"},{"location":"evaluations/20241024_r/","text":"Evaluation 2024-10-24: R \u00b6 Reflections: Richel 5 responses 1. Overall, how would you rate today\u2019s training event? \u00b6 7.4. Average Number 2. Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? \u00b6 I liked being able to run through the examples myself. Pedro\u2019s sessions 3. Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? \u00b6 I think sometimes there could have been slightly less kinds of examples, and one main example per module with extra focus and more time to complete. 4. Training event organisation (e.g. announcement, registration, \u2026): \u2013 What did you like best? \u2013 Where should we improve? \u00b6 I liked the general organization, felt like any questions one might have where answered in the introductory email. 5. Length of teaching today was \u00b6 Adequate 4 Too short 0 Too long 1 6. Depth of content was \u00b6 Adequate 4 Too superficial 1 Too profound 0 7. The pace of teaching was \u00b6 Adequate 3 Too slow 0 Too fast 2 8. Teaching aids used (e.g. slides) were well prepared \u00b6 Agree completely 2 Agree 3 No strong feelings 0 Disagree 0 Disagree completely 0 9. Hands-on exercises and demonstrations were \u00b6 Adequate 3 Too few 2 Too many 0 10. Hands-on exercises and demonstrations were well prepared \u00b6 Agree completely 2 Agree 3 No strong feelings 0 Disagree 0 Disagree completely 0 11. How would you rate the separate sessions? \u00b6 12. Give your confidence levels of the following statements \u00b6 13.Did today\u2019s course meet your expectation? \u00b6 Yes 3 No 0 Not sure 2 14. Which future training topics would you like to be provided by the training host(s)? \u00b6 2 responses 15. Do you have any additional comments? \u00b6 1 response","title":"Evaluation 2024-10-24: R"},{"location":"evaluations/20241024_r/#evaluation-2024-10-24-r","text":"Reflections: Richel 5 responses","title":"Evaluation 2024-10-24: R"},{"location":"evaluations/20241024_r/#1-overall-how-would-you-rate-todays-training-event","text":"7.4. Average Number","title":"1. Overall, how would you rate today&rsquo;s training event?"},{"location":"evaluations/20241024_r/#2-todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-what-did-you-like-best","text":"I liked being able to run through the examples myself. Pedro\u2019s sessions","title":"2. Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best?"},{"location":"evaluations/20241024_r/#3-todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-where-should-we-improve","text":"I think sometimes there could have been slightly less kinds of examples, and one main example per module with extra focus and more time to complete.","title":"3. Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve?"},{"location":"evaluations/20241024_r/#4-training-event-organisation-eg-announcement-registration-what-did-you-like-best-where-should-we-improve","text":"I liked the general organization, felt like any questions one might have where answered in the introductory email.","title":"4. Training event organisation (e.g. announcement, registration, &hellip;): \u2013 What did you like best? \u2013 Where should we improve?"},{"location":"evaluations/20241024_r/#5-length-of-teaching-today-was","text":"Adequate 4 Too short 0 Too long 1","title":"5. Length of teaching today was"},{"location":"evaluations/20241024_r/#6-depth-of-content-was","text":"Adequate 4 Too superficial 1 Too profound 0","title":"6. Depth of content was"},{"location":"evaluations/20241024_r/#7-the-pace-of-teaching-was","text":"Adequate 3 Too slow 0 Too fast 2","title":"7. The pace of teaching was"},{"location":"evaluations/20241024_r/#8-teaching-aids-used-eg-slides-were-well-prepared","text":"Agree completely 2 Agree 3 No strong feelings 0 Disagree 0 Disagree completely 0","title":"8. Teaching aids used (e.g. slides) were well prepared"},{"location":"evaluations/20241024_r/#9-hands-on-exercises-and-demonstrations-were","text":"Adequate 3 Too few 2 Too many 0","title":"9. Hands-on exercises and demonstrations were"},{"location":"evaluations/20241024_r/#10-hands-on-exercises-and-demonstrations-were-well-prepared","text":"Agree completely 2 Agree 3 No strong feelings 0 Disagree 0 Disagree completely 0","title":"10. Hands-on exercises and demonstrations were well prepared"},{"location":"evaluations/20241024_r/#11-how-would-you-rate-the-separate-sessions","text":"","title":"11. How would you rate the separate sessions?"},{"location":"evaluations/20241024_r/#12-give-your-confidence-levels-of-the-following-statements","text":"","title":"12. Give your confidence levels of the following statements"},{"location":"evaluations/20241024_r/#13did-todays-course-meet-your-expectation","text":"Yes 3 No 0 Not sure 2","title":"13.Did today&rsquo;s course meet your expectation?"},{"location":"evaluations/20241024_r/#14-which-future-training-topics-would-you-like-to-be-provided-by-the-training-hosts","text":"2 responses","title":"14. Which future training topics would you like to be provided by the training host(s)?"},{"location":"evaluations/20241024_r/#15-do-you-have-any-additional-comments","text":"1 response","title":"15. Do you have any additional comments?"},{"location":"evaluations/20241025_matlab/","text":"Evaluation 2024-10-25: MATLAB \u00b6","title":"Evaluation 2024-10-25: MATLAB"},{"location":"evaluations/20241025_matlab/#evaluation-2024-10-25-matlab","text":"","title":"Evaluation 2024-10-25: MATLAB"},{"location":"evaluations/20250324_r/","text":"Evaluation \u00b6 Date: 2025-03-24 language: R Registrations: 33 Participants: 12 (36% of registrations shows up) Number of evaluations filled in: 8 (67% response rate) Average course satisfaction: 8.25/10.0 Success score: 77% Reflections: Richel Question 1: Overall, how would you rate today\u2019s training event? \u00b6 8.25 Question 2: What do you think about the pace of teaching overall? \u00b6 The pace was different for the different for the different modules Extremely helpful day, and the exercises were generally very well prepared and pedagogic A good general overview of how to run R in HPC. By the end it was a bit long but it clarified a lot of concepts for me It was good! I am happy with the pace Good For me, it was a little too fast The pace is very appropriate, although I couldn\u2019t really follow the ML part as it was out of my area of expertise Question 3: Confidences \u00b6 As table: average_confidence_per_question.csv Coding scheme: Column text Description NA I did not attend that session 0 I have no confidence I can do this 1 I have low confidence I can do this 2 I have some confidence I can do this 3 I have good confidence I can do this 4 I can absolutely do this! Question 4: Would you recommend this course to someone else? \u00b6 Answer Amount Yes 8 No 0 Not sure 0 Question 5: Which future training topics would you like to be provided by the training host(s)? \u00b6 More on parallelizing in R It would be nice to perhaps have a little module on transferring files to and from the server: while I feel pretty confident about using R, I\u2019m not completely sure how to get files to and from he server. And secondly, I\u2019m 99% sure it should be reasonably straightforward, but it might be nice for there to be a little extra note on best practices for installing and using STAN for Markov Chain Monte Carlo (MCMC). R, Julia and MATLAB can work with STAN, and because MCMC is such a slow process, I can imagine that this could be helpful. Question 6: Do you have any additional comments? \u00b6 Suggestions/ideas: For some portions of this course it was a bit unclear when/what to do hands-on. The course documentation is very good and will help me the most in the future. I really liked the materials and the web page. It was extremely helpful, and I have bookmarked the course page because it is easier to use than all the official documentation. The exercises were well-prepared, although the parallel processing section was less good on both the web page, and the exercise code needed a bit of editing to get it to work. The organization was great - however the initial email maybe didn\u2019t make it quite clear enough that setting up a login account could take many days. I happened to already have one, but sitting the day before to try to set up, I could easily have missed that. I really appreciate the amount of hands-on demonstrations: they were great. Only a very, very small comment is that it was not always clear which .sh file was appropriate for the relevant exercises, for instance serial and parallel at the start. The exercises might be named with the same names, which would make it easier to find them. But this is such a small complaint. The whole material for the day was excellent, and I\u2019m feeling extremely confident about moving forward with getting started. I had some issues with my account not being set up correctly, which set me behind for the whole course and meant there were some things I couldn\u2019t test myself as they were being explained. This was not the fault of the course facilitators, but did make the course less useful for me. I liked the exercises best, makes it easy to understand how you can directly apply. I would have appreciated more time for them though. The length of the course is good, the machine learning part was difficult for me to follow.","title":"Evaluation"},{"location":"evaluations/20250324_r/#evaluation","text":"Date: 2025-03-24 language: R Registrations: 33 Participants: 12 (36% of registrations shows up) Number of evaluations filled in: 8 (67% response rate) Average course satisfaction: 8.25/10.0 Success score: 77% Reflections: Richel","title":"Evaluation"},{"location":"evaluations/20250324_r/#question-1-overall-how-would-you-rate-todays-training-event","text":"8.25","title":"Question 1: Overall, how would you rate today&rsquo;s training event?"},{"location":"evaluations/20250324_r/#question-2-what-do-you-think-about-the-pace-of-teaching-overall","text":"The pace was different for the different for the different modules Extremely helpful day, and the exercises were generally very well prepared and pedagogic A good general overview of how to run R in HPC. By the end it was a bit long but it clarified a lot of concepts for me It was good! I am happy with the pace Good For me, it was a little too fast The pace is very appropriate, although I couldn\u2019t really follow the ML part as it was out of my area of expertise","title":"Question 2: What do you think about the pace of teaching overall?"},{"location":"evaluations/20250324_r/#question-3-confidences","text":"As table: average_confidence_per_question.csv Coding scheme: Column text Description NA I did not attend that session 0 I have no confidence I can do this 1 I have low confidence I can do this 2 I have some confidence I can do this 3 I have good confidence I can do this 4 I can absolutely do this!","title":"Question 3: Confidences"},{"location":"evaluations/20250324_r/#question-4-would-you-recommend-this-course-to-someone-else","text":"Answer Amount Yes 8 No 0 Not sure 0","title":"Question 4: Would you recommend this course to someone else?"},{"location":"evaluations/20250324_r/#question-5-which-future-training-topics-would-you-like-to-be-provided-by-the-training-hosts","text":"More on parallelizing in R It would be nice to perhaps have a little module on transferring files to and from the server: while I feel pretty confident about using R, I\u2019m not completely sure how to get files to and from he server. And secondly, I\u2019m 99% sure it should be reasonably straightforward, but it might be nice for there to be a little extra note on best practices for installing and using STAN for Markov Chain Monte Carlo (MCMC). R, Julia and MATLAB can work with STAN, and because MCMC is such a slow process, I can imagine that this could be helpful.","title":"Question 5: Which future training topics would you like to be provided by the training host(s)?"},{"location":"evaluations/20250324_r/#question-6-do-you-have-any-additional-comments","text":"Suggestions/ideas: For some portions of this course it was a bit unclear when/what to do hands-on. The course documentation is very good and will help me the most in the future. I really liked the materials and the web page. It was extremely helpful, and I have bookmarked the course page because it is easier to use than all the official documentation. The exercises were well-prepared, although the parallel processing section was less good on both the web page, and the exercise code needed a bit of editing to get it to work. The organization was great - however the initial email maybe didn\u2019t make it quite clear enough that setting up a login account could take many days. I happened to already have one, but sitting the day before to try to set up, I could easily have missed that. I really appreciate the amount of hands-on demonstrations: they were great. Only a very, very small comment is that it was not always clear which .sh file was appropriate for the relevant exercises, for instance serial and parallel at the start. The exercises might be named with the same names, which would make it easier to find them. But this is such a small complaint. The whole material for the day was excellent, and I\u2019m feeling extremely confident about moving forward with getting started. I had some issues with my account not being set up correctly, which set me behind for the whole course and meant there were some things I couldn\u2019t test myself as they were being explained. This was not the fault of the course facilitators, but did make the course less useful for me. I liked the exercises best, makes it easy to understand how you can directly apply. I would have appreciated more time for them though. The length of the course is good, the machine learning part was difficult for me to follow.","title":"Question 6: Do you have any additional comments?"},{"location":"evaluations/20250325_matlab/","text":"Evaluation \u00b6 Date: 2025-03-25 language: MATLAB Registrations: ? Participants: 4 Number of evaluations filled in: 3 (75% response rate) Average course satisfaction: 7.25/10.0 success_score.txt : 77% Analysis script: analyse.R Average confidence per question as a table: average_confidences.csv comments.txt \u00b6 The course material is excellent. However, it is not necessary that someone reads it in real time in a zoom meeting, and the level of interaction in the zoom meeting has been low. I could just as well have gone through the course material in my own pace at a time that suited me and learned almost as much (my background is that I know some Linux, Matlab, and cluster batch job submission from before). future_topics.txt \u00b6 using matlab with thirdparty tools","title":"Evaluation"},{"location":"evaluations/20250325_matlab/#evaluation","text":"Date: 2025-03-25 language: MATLAB Registrations: ? Participants: 4 Number of evaluations filled in: 3 (75% response rate) Average course satisfaction: 7.25/10.0 success_score.txt : 77% Analysis script: analyse.R Average confidence per question as a table: average_confidences.csv","title":"Evaluation"},{"location":"evaluations/20250325_matlab/#commentstxt","text":"The course material is excellent. However, it is not necessary that someone reads it in real time in a zoom meeting, and the level of interaction in the zoom meeting has been low. I could just as well have gone through the course material in my own pace at a time that suited me and learned almost as much (my background is that I know some Linux, Matlab, and cluster batch job submission from before).","title":"comments.txt"},{"location":"evaluations/20250325_matlab/#future_topicstxt","text":"using matlab with thirdparty tools","title":"future_topics.txt"},{"location":"evaluations/20250326_julia/","text":"Evaluation \u00b6 Date: 2025-03-26 language: Julia Registrations: ? Participants: 4 Number of evaluations filled in: 3 (75% response rate) Average course satisfaction: 8.3/10.0 success_score.txt : 82% Analysis script: analyse.R Average confidence per question as a table: average_confidences.csv Question 2: What do you think about the pace of teaching overall? \u00b6 teaching was excellent It was good. However, some parts were redundant if you had attended another day. ok Question 5: Which future training topics would you like to be provided by the training host(s)? \u00b6 machine learning/deep learning/ai-related training A more detailed overview of how to setup everything to work with the gpus. Question 6: Do you have any additional comments? \u00b6 Suggestions/ideas: What did you like best? (materials, exercises, structure) Where should we improve? (materials, exercises, structure) Training organization (announcement, registration, participant info, \u2026): what was good and what should we improve? Length/depth of the course: too long/short, too superficial/profound, adequate. Teaching aids used (presentations, slides, videos, \u2026): well prepared or not? Specific sessions that were better/worse in this? Hands-ons, exercises, demonstrations: were they well prepared or not? Specific sessions that were better/worse in this? Hands-ons and demonstrations: too few, too many, adequate amount? Comments about specific sessions? Good/bad? [None]","title":"Evaluation"},{"location":"evaluations/20250326_julia/#evaluation","text":"Date: 2025-03-26 language: Julia Registrations: ? Participants: 4 Number of evaluations filled in: 3 (75% response rate) Average course satisfaction: 8.3/10.0 success_score.txt : 82% Analysis script: analyse.R Average confidence per question as a table: average_confidences.csv","title":"Evaluation"},{"location":"evaluations/20250326_julia/#question-2-what-do-you-think-about-the-pace-of-teaching-overall","text":"teaching was excellent It was good. However, some parts were redundant if you had attended another day. ok","title":"Question 2: What do you think about the pace of teaching overall?"},{"location":"evaluations/20250326_julia/#question-5-which-future-training-topics-would-you-like-to-be-provided-by-the-training-hosts","text":"machine learning/deep learning/ai-related training A more detailed overview of how to setup everything to work with the gpus.","title":"Question 5: Which future training topics would you like to be provided by the training host(s)?"},{"location":"evaluations/20250326_julia/#question-6-do-you-have-any-additional-comments","text":"Suggestions/ideas: What did you like best? (materials, exercises, structure) Where should we improve? (materials, exercises, structure) Training organization (announcement, registration, participant info, \u2026): what was good and what should we improve? Length/depth of the course: too long/short, too superficial/profound, adequate. Teaching aids used (presentations, slides, videos, \u2026): well prepared or not? Specific sessions that were better/worse in this? Hands-ons, exercises, demonstrations: were they well prepared or not? Specific sessions that were better/worse in this? Hands-ons and demonstrations: too few, too many, adequate amount? Comments about specific sessions? Good/bad? [None]","title":"Question 6: Do you have any additional comments?"},{"location":"evaluations/202503_courses/","text":"2025-03 courses \u00b6 This is the course-wide evaluation. The course lasted from 2024-03-12 to and including 2024-03-14. analyse.R","title":"2025-03 courses"},{"location":"evaluations/202503_courses/#2025-03-courses","text":"This is the course-wide evaluation. The course lasted from 2024-03-12 to and including 2024-03-14. analyse.R","title":"2025-03 courses"},{"location":"evaluations/20251006_r/","text":"Evaluation \u00b6 Date: 2025-10-06 Day: R Registrations: 14 Active participants: 6 Inactive participants: 1 OS: 3x macOS, 3x Windows","title":"Evaluation"},{"location":"evaluations/20251006_r/#evaluation","text":"Date: 2025-10-06 Day: R Registrations: 14 Active participants: 6 Inactive participants: 1 OS: 3x macOS, 3x Windows","title":"Evaluation"},{"location":"evaluations/20251007_matlab/","text":"Evaluation \u00b6 Date: 2025-10-07 Day: MATLAB Registrations: 6 Active participants: 1 Inactive participants: 1 OS: 1x macOS","title":"Evaluation"},{"location":"evaluations/20251007_matlab/#evaluation","text":"Date: 2025-10-07 Day: MATLAB Registrations: 6 Active participants: 1 Inactive participants: 1 OS: 1x macOS","title":"Evaluation"},{"location":"evaluations/20251008_julia/","text":"Evaluation \u00b6 Date: 2025-10-08 Day: Julia Registrations: 5 Active participants: 1 Inactive participants: 1 OS: 1x macOS and Linux (Ubuntu)","title":"Evaluation"},{"location":"evaluations/20251008_julia/#evaluation","text":"Date: 2025-10-08 Day: Julia Registrations: 5 Active participants: 1 Inactive participants: 1 OS: 1x macOS and Linux (Ubuntu)","title":"Evaluation"},{"location":"evaluations/20251010_advanced/","text":"Evaluation \u00b6 Date: 2025-10-10 Day: Advanced Registrations: 14 (for whole course) Active participants: 1 Inactive participants: 1 OS: 1x macOS and Linux (Ubuntu)","title":"Evaluation"},{"location":"evaluations/20251010_advanced/#evaluation","text":"Date: 2025-10-10 Day: Advanced Registrations: 14 (for whole course) Active participants: 1 Inactive participants: 1 OS: 1x macOS and Linux (Ubuntu)","title":"Evaluation"},{"location":"julia/batch/","text":"Batch system \u00b6 Learning outcomes for today Short introduction to SLURM scheduler Show structure of a batch script Run the provided batch scrip examples Your expectations? What is a batch job? How to make a batch job? How can I run a Julia simulation in batch mode? Instructor note Intro 5 min Lecture and 10 min Compute allocations in this workshop Pelle/Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934 Storage space for this workshop Pelle/Rackham: /proj/r-matlab-julia-pelle Kebnekaise: /proj/nobackup/fall-courses Tetralith: /proj/courses-fall-2025/users/ Dardel: /cfs/klemming/projects/snic/courses-fall-2025 Alvis: /mimer/NOBACKUP/groups/courses-fall-2025/ Warning Any longer, resource-intensive, or parallel jobs must be run through a batch script . The batch system used at HPC clusters in Sweden is called SLURM. SLURM is an Open Source job scheduler, which provides three key functions Keeps track of available system resources Enforces local system resource usage and job scheduling policies Manages a job queue, distributing work across resources according to policies In order to run a batch job, you need to create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script). Guides and documentation at: HPC2N , UPPMAX , LUNARC , and PDC . Workflow Write a batch script Inside the batch script you need to load the modules you need, for instance Julia Possibly activate an isolated/virtual environment to access own-installed packages Ask for resources depending on if it is a parallel job or a serial job, if you need GPUs or not, etc. Give the command(s) to your Julia script Submit batch script with sbatch <my-julia-script.sh> Common file extensions for batch scripts are .sh or .batch , but they are not necessary. You can choose any name that makes sense to you. Useful commands \u00b6 Submit job: sbatch <jobscript.sh> Get list of your jobs: squeue -u <username> Check on a specific job: scontrol show job <job-id> Delete a specific job: scancel <job-id> Useful info about a job: sacct -l -j <job-id> | less -S Url to a page with info about the job (Kebnekaise only): job-usage <job-id> Examples of batch scripts \u00b6 Serial code \u00b6 Hello World Short serial example for running on different clusters. UPPMAX (Bianca) UPPMAX (Pelle) HPC2N LUNARC PDC NSC script.jl #!/bin/bash -l # -l cleans the environment in the batch job, recommended at UPPMAX #SBATCH -A sens202t-uv-wxyz # your project_ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.8.5 # Julia module julia script.jl # run the serial script #!/bin/bash -l # -l cleans the environment in the batch job, recommended at UPPMAX #SBATCH -A uppmax202t-uv-wxyz # your project_ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml Julia/1.10.9-LTS-linux-x86_64 # Julia module julia script.jl # run the serial script #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module julia script.jl # run the serial script #!/bin/bash #SBATCH -A lu202w-x-yz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module julia script.jl # run the serial script #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 julia script.jl # run the serial script #!/bin/bash #SBATCH -A naiss202t-uv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load any modules you need, here for Julia ml julia/1.9.4-bdist julia script.jl # run the serial script Julia example code. y = \"Hello World\" println ( y ) Send the script to the batch: $ sbatch <batch script> Serial code + self-installed package in virt. env. \u00b6 Virtual environment Short serial example for running on Julia with a virtual environment. Create an environment my-third-env and install the package DFTK . Here, there are batch scripts for using this environment (it is assumed that the batch scripts are in the my-third-env folder): UPPMAX (Bianca) UPPMAX (Pelle) HPC2N LUNARC PDC NSC serial-env.jl #!/bin/bash -l # -l cleans the environment in the batch job, recommended at UPPMAX #SBATCH -A sens202t-uv-wxyz # Change to your own after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.8.5 # Julia module # Move to the directory where the \".toml\" files for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash -l # -l cleans the environment in the batch job, recommended at UPPMAX #SBATCH -A uppmax202t-uv-wxyz # Change to your own after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml Julia/1.10.9-LTS-linux-x86_64 # Julia module # Move to the directory where the \".toml\" files for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module # Move to the directory where the \".toml\" files # for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash #SBATCH -A lu202w-x-yz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module # Move to the directory where the \".toml\" files # for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # Move to the directory where the \".toml\" files # for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash #SBATCH -A naiss202t-uv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load any modules you need, here for Julia ml julia/1.9.4-bdist # Move to the directory where the \".toml\" files # for the environment are located julia --project = . serial-env.jl # run the script Julia example code where an environment is used. using Pkg Pkg . status () You should see the installed packages in the output file. In the present case because I installed the DFTK package only in my-third-env environment, I can see the following output: Status `/path-to-project-storage/my-third-env/Project.toml` [ acf6eb54 ] DFTK v0 .6.2 Exercises \u00b6 Exercise 1. Run a serial script \u00b6 Run the serial script serial-sum.jl : x = parse ( Int32 , ARGS [ 1 ] ) y = parse ( Int32 , ARGS [ 2 ] ) summ = x + y println ( \"The sum of the two numbers is \" , summ ) This scripts accepts two integers as command line arguments. Answer HPC2N UPPMAX (Bianca/Rackham) UPPMAX (Pelle) LUNARC PDC NSC This batch script is for Kebnekaise. #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for Bianca/Rackham. #!/bin/bash -l #SBATCH -A naiss202t-uv-wxyz # Change to your own after the course #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:05:00 # Asking for 5 minutes #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file module load julia/1.8.5 julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for Pelle. #!/bin/bash -l #SBATCH -A uppmax202t-uv-wxyz # Change to your own after the course #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:05:00 # Asking for 5 minutes #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file module load Julia/1.10.9-LTS-linux-x86_64 julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for LUNARC. #!/bin/bash #SBATCH -A lu202w-x-yz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for PDC. #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for NSC. #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:04:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.9.4-bdist julia serial-sum.jl Arg1 Arg2 # run the serial script Summary The SLURM scheduler handles allocations to the calculation nodes Batch jobs runs without interaction with user A batch script consists of a part with SLURM parameters describing the allocation and a second part describing the actual work within the job, for instance one or several Julia scripts.","title":"Batch jobs"},{"location":"julia/batch/#batch-system","text":"Learning outcomes for today Short introduction to SLURM scheduler Show structure of a batch script Run the provided batch scrip examples Your expectations? What is a batch job? How to make a batch job? How can I run a Julia simulation in batch mode? Instructor note Intro 5 min Lecture and 10 min Compute allocations in this workshop Pelle/Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934 Storage space for this workshop Pelle/Rackham: /proj/r-matlab-julia-pelle Kebnekaise: /proj/nobackup/fall-courses Tetralith: /proj/courses-fall-2025/users/ Dardel: /cfs/klemming/projects/snic/courses-fall-2025 Alvis: /mimer/NOBACKUP/groups/courses-fall-2025/ Warning Any longer, resource-intensive, or parallel jobs must be run through a batch script . The batch system used at HPC clusters in Sweden is called SLURM. SLURM is an Open Source job scheduler, which provides three key functions Keeps track of available system resources Enforces local system resource usage and job scheduling policies Manages a job queue, distributing work across resources according to policies In order to run a batch job, you need to create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script). Guides and documentation at: HPC2N , UPPMAX , LUNARC , and PDC . Workflow Write a batch script Inside the batch script you need to load the modules you need, for instance Julia Possibly activate an isolated/virtual environment to access own-installed packages Ask for resources depending on if it is a parallel job or a serial job, if you need GPUs or not, etc. Give the command(s) to your Julia script Submit batch script with sbatch <my-julia-script.sh> Common file extensions for batch scripts are .sh or .batch , but they are not necessary. You can choose any name that makes sense to you.","title":"Batch system"},{"location":"julia/batch/#useful-commands","text":"Submit job: sbatch <jobscript.sh> Get list of your jobs: squeue -u <username> Check on a specific job: scontrol show job <job-id> Delete a specific job: scancel <job-id> Useful info about a job: sacct -l -j <job-id> | less -S Url to a page with info about the job (Kebnekaise only): job-usage <job-id>","title":"Useful commands"},{"location":"julia/batch/#examples-of-batch-scripts","text":"","title":"Examples of batch scripts"},{"location":"julia/batch/#serial-code","text":"Hello World Short serial example for running on different clusters. UPPMAX (Bianca) UPPMAX (Pelle) HPC2N LUNARC PDC NSC script.jl #!/bin/bash -l # -l cleans the environment in the batch job, recommended at UPPMAX #SBATCH -A sens202t-uv-wxyz # your project_ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.8.5 # Julia module julia script.jl # run the serial script #!/bin/bash -l # -l cleans the environment in the batch job, recommended at UPPMAX #SBATCH -A uppmax202t-uv-wxyz # your project_ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml Julia/1.10.9-LTS-linux-x86_64 # Julia module julia script.jl # run the serial script #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module julia script.jl # run the serial script #!/bin/bash #SBATCH -A lu202w-x-yz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module julia script.jl # run the serial script #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 julia script.jl # run the serial script #!/bin/bash #SBATCH -A naiss202t-uv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load any modules you need, here for Julia ml julia/1.9.4-bdist julia script.jl # run the serial script Julia example code. y = \"Hello World\" println ( y ) Send the script to the batch: $ sbatch <batch script>","title":"Serial code"},{"location":"julia/batch/#serial-code-self-installed-package-in-virt-env","text":"Virtual environment Short serial example for running on Julia with a virtual environment. Create an environment my-third-env and install the package DFTK . Here, there are batch scripts for using this environment (it is assumed that the batch scripts are in the my-third-env folder): UPPMAX (Bianca) UPPMAX (Pelle) HPC2N LUNARC PDC NSC serial-env.jl #!/bin/bash -l # -l cleans the environment in the batch job, recommended at UPPMAX #SBATCH -A sens202t-uv-wxyz # Change to your own after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.8.5 # Julia module # Move to the directory where the \".toml\" files for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash -l # -l cleans the environment in the batch job, recommended at UPPMAX #SBATCH -A uppmax202t-uv-wxyz # Change to your own after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml Julia/1.10.9-LTS-linux-x86_64 # Julia module # Move to the directory where the \".toml\" files for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module # Move to the directory where the \".toml\" files # for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash #SBATCH -A lu202w-x-yz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module # Move to the directory where the \".toml\" files # for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # Move to the directory where the \".toml\" files # for the environment are located julia --project = . serial-env.jl # run the script #!/bin/bash #SBATCH -A naiss202t-uv-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n *FIXME* # nr. tasks #SBATCH --time=00:20:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load any modules you need, here for Julia ml julia/1.9.4-bdist # Move to the directory where the \".toml\" files # for the environment are located julia --project = . serial-env.jl # run the script Julia example code where an environment is used. using Pkg Pkg . status () You should see the installed packages in the output file. In the present case because I installed the DFTK package only in my-third-env environment, I can see the following output: Status `/path-to-project-storage/my-third-env/Project.toml` [ acf6eb54 ] DFTK v0 .6.2","title":"Serial code + self-installed package in virt. env."},{"location":"julia/batch/#exercises","text":"","title":"Exercises"},{"location":"julia/batch/#exercise-1-run-a-serial-script","text":"Run the serial script serial-sum.jl : x = parse ( Int32 , ARGS [ 1 ] ) y = parse ( Int32 , ARGS [ 2 ] ) summ = x + y println ( \"The sum of the two numbers is \" , summ ) This scripts accepts two integers as command line arguments. Answer HPC2N UPPMAX (Bianca/Rackham) UPPMAX (Pelle) LUNARC PDC NSC This batch script is for Kebnekaise. #!/bin/bash #SBATCH -A hpc2n202w-xyz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for Bianca/Rackham. #!/bin/bash -l #SBATCH -A naiss202t-uv-wxyz # Change to your own after the course #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:05:00 # Asking for 5 minutes #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file module load julia/1.8.5 julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for Pelle. #!/bin/bash -l #SBATCH -A uppmax202t-uv-wxyz # Change to your own after the course #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:05:00 # Asking for 5 minutes #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file module load Julia/1.10.9-LTS-linux-x86_64 julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for LUNARC. #!/bin/bash #SBATCH -A lu202w-x-yz # your project_ID #SBATCH -J job-serial # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml purge > /dev/null 2 > & 1 # recommended purge ml Julia/1.8.5-linux-x86_64 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for PDC. #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -p shared # name of the queue #SBATCH --ntasks=1 # nr. of tasks #SBATCH --cpus-per-task=1 # nr. of cores per-task #SBATCH --time=00:03:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file # Load dependencies and Julia version ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 julia serial-sum.jl Arg1 Arg2 # run the serial script This batch script is for NSC. #!/bin/bash #SBATCH -A naiss202t-uv-wxyz # your project_ID #SBATCH -J job # name of the job #SBATCH -n 1 # nr. tasks #SBATCH --time=00:04:00 # requested time #SBATCH --error=job.%J.err # error file #SBATCH --output=job.%J.out # output file ml julia/1.9.4-bdist julia serial-sum.jl Arg1 Arg2 # run the serial script Summary The SLURM scheduler handles allocations to the calculation nodes Batch jobs runs without interaction with user A batch script consists of a part with SLURM parameters describing the allocation and a second part describing the actual work within the job, for instance one or several Julia scripts.","title":"Exercise 1. Run a serial script"},{"location":"julia/environments_packages/","text":"Managing environments and packages \u00b6 Learning outcomes for this session How to work with Julia\u2019s environment and package management. How to check for and use site-installed packages, if any. Your background? Have you used Python? Have you used Matlab? Were you at yesterdays Matlab sessions? Have you used Git? Are you completely new to this, code packages and version control? Key points Environments Project Version control Nested, base, tools not part of the project, use with caution Packages Introduction \u00b6 Packages are pieces of software to be used by your scripts or interactive sessions as-is or modified, stand-alone or\u2014most often\u2014in combinations to make a full toolkit customized to your task. Such a toolkit may itself become a new package. Packages allow us to cooperate on code that can be useful in several places. Environments are sets of packages that are available for use simultaneously and, if all is right, work together. If you come from Python, you have likely seen that there are several different ways to deal with environments and package management in this language, for instance, conda and pip . The situation can be described as several ecosystems of packages. You may have come across the term \u201cdependency hell\u201d or even had a taste (or perhaps far more than you could stomach) of this yourself. Julia comes with a great system for environments and packages included. As a result there is essentially a single ecosystem of packages and \u201cdependency hell\u201d is as close to eliminated as it can be. Julia environments are defined by two TOML (Tom\u2019s Obvious Minimal Language) files, Project.toml and Manifest.toml . The former specifies which packages you\u2019ve asked for, the latter specifies all packages loaded including their exact versions. Include both of these files ( Project.toml all the time, Manifest.toml at points for which you want exact reproducibility) in your version control and you get traceable, reproducible environments with minimal effort! Getting started \u00b6 To get started we will go to the Julia documentation and make use of the official tutorial on the Julia package manager: https://docs.julialang.org/en/v1/stdlib/Pkg/#Pkg Summary of what we\u2019ve seen so far \u00b6 Get to the Pkg REPL by pressing ] in the Julia REPL In the Pkg REPL, use the following commands: ? to get help activate to activate an existing or new environment st (alias for status ) to see which packages have been added to the active environment add to add packages to the active environment, this may or may not have download and precompilation steps that take a bit of time rm (alias for remove ) to remove packages from the active environment up (alias for update ) to update packages in the active environment Project.toml stores what packages you\u2019ve asked for Manifest.toml stores how Pkg resolved this, with all dependencies and exact versions Environment stacking When you have not activated any specific environment, the active environment is your personal base environment for the Julia version you\u2019re currently running (called e.g. @v1.11). This is normally reachable in addition to your active environment. This can be a convenient way to access development tools that you want active but your project does not depend on . If you have tools available through your base environment and you need to check that your project can be reproduced properly without your base environment, you\u2019ll want to read this: Loading project environment only The full stack of reachable environments are defined by the global Julia variable LOAD_PATH : julia>LOAD_PATH 3-element Vector{String}: \"@\" \"@v#.#\" \"@stdlib\" where @ is the active environment, @v#.# is your base environment for the Julia version in use, and @stdlib is the standard library. To include just the current environment we can modify the LOAD_PATH variable from the julian prompt with the following functions: julia> empty!(LOAD_PATH) # this will clean out the path julia> push!(LOAD_PATH, \"@\") # this will add the current environment Choosing package versions \u00b6 ] add < package > @< version > Example: ] add DataFrames @ 1.1 But where are the packages installed? \u00b6 Each environment only consists of two files, the code is not here. Packages will be installed to and loaded from paths defined by the global Julia variable DEPOT_PATH . In a Julia installation on the clusters this will by default be set to: ~/.julia where ~ is the user home as appropriate on the system. If Julia is installed centrally On a local system like a personal linux computer or if Julia would have been installed at a \u201csystem level\u201d and not i a module system an architecture-specific shared system directory, e.g. /usr/local/share/julia ; an architecture-independent shared system directory, e.g. /usr/share/julia . Packages can consist of relatively many files and some clusters have a limit on the number of files you can have in your home directory. If this becomes a problem, you can set the DEPOT_PATH to a Project folder Non-interactive use of Pkg \u00b6 We\u2019ve seen how to use Pkg interactively, with its special REPL mode. All the functions of Pkg can also be used by loading the Pkg module and calling its functions just like any other module. On the clusters, this is especially useful for activating or even setting up an environment from inside a Julia script, so that you can run it in a batch job. For example: activate the environment of the current folder by calling these two lines in your .jl file: using Pkg Pkg . activate ( \".\" ) Besides the previous two options for activating an environment, you can also activate it on the Linux command line (assuming that you are located in the environment directory): julia --project=. Bianca At Bianca there is a central library with installed packages. You may control the present \u201ccentral library\u201d by typing ml help julia/<version> in the BASH shell. A possibly more up-to-date status can be found from the Julia shell: julia> using Pkg julia> Pkg . activate ( DEPOT_PATH [ 2 ] * \"/environments/v1.8\" ); #change version (1.8) accordingly if you have another main version of Julia julia> Pkg . status () julia> Pkg . activate ( DEPOT_PATH [ 1 ] * \"/environments/v1.8\" ); #to return to user library A selection of the Julia packages and libraries installed on Bianca are: - BenchmarkTools - CSV - CUDA - MPI - Distributed - IJulia - Plots - PyPlot - Gadfly - DataFrames - DistributedArrays - PlotlyJS Site-installed packages in environments At Bianca the central environment adds to the environment stack: julia> LOAD_PATH 4-element Vector{String}: \"@\" \"@v#.#\" \"@stdlib\" \"/sw/comp/julia/1.8.5/rackham/lib/glob_pkg/environments/v1.8\" Bianca Intermediate workshop Info page Course material Julia on Bianca Exercises \u00b6 We need the packages IJulia and Pluto for running the integrated development environments (IDEs) Jupyter and Pluto. We also need the MPI package on Friday. Make these exercises be the installation of the packages that we will later use. It might be advisable to install IJulia and Pluto in separate environments. Challenge 1. Install Pluto It may take 5-10 minutes or so. This you can do in an ordinary terminal NSC PDC UPPMAX (Bianca/Rackham) UPPMAX (Pelle) HPC2N & LUNARC $ ml julia/1.10.2-bdist $ julia Note: not fully tested successfully, but this step works $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 $ julia $ module load julia/1.8.5 $ module load Julia/1.10.9-LTS-linux-x86_64 $ module load GCCcore/13.2.0 Julia/1.9.3-linux-x86_64 $ julia In Julia for all clusters (output may differ for different clusters and Julia versions): shell > mkdir pluto - env shell > cd pluto - env ( @v1 .10 ) pkg > activate . Activating new project at `path-to-folder\\pluto-env` ( pluto - env ) pkg > add Pluto ( pluto - env ) pkg > status Status `path-to-folder\\pluto-env\\Project.toml` [ c3e4b0f8 ] Pluto v0 .20.19 2. Install IJulia This is done only once, but for each combination of Julia you would like to use. Also Python must be loaded It may take 5-10 minutes or so. This you can do in an ordinary terminal (book an interactive session, for safety) NSC PDC UPPMAX (Bianca/Rackham) UPPMAX (Pelle) HPC2N & LUNARC $ ml Python/3.11.5-env-hpc1-gcc-2023b-eb $ ml julia/1.10.2-bdist $ julia $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 $ ml cray-python/3.11.5 $ julia $ module load julia/1.8.5 $ module load python/3.9.5 $ julia $ module load Julia/1.10.9-LTS-linux-x86_64 $ module load JupyterLab/4.2.5-GCCcore-13.3.0 $ julia $ module load GCC/13.2.0 JupyterLab/4.2.0 $ module load Julia/1.8.5-linux-x86_64 $ julia In Julia for all clusters (output may differ for different clusters and Julia versions): shell > mkdir jupyter - env shell > cd jupyter - env ( @v1 .10 ) pkg > activate . Activating new project at `path-to-folder\\jupyter-env` ( jupyter - env ) pkg > add IJulia ( jupyter - env ) pkg > status Status `path-to-folder\\jupyter-env\\Project.toml` [ 7073 ff75 ] IJulia v1 .27.0 Challenge 3. Required package for parallel jobs In order to use MPI with Julia you will need to follow the next steps (only the first time): UPPMAX (Bianca/Rackham) UPPMAX (Pelle) HPC2N LUNARC PDC NSC # Load the tool chain which contains a MPI library $ ml gcc/11.3.0 openmpi/4.1.3 # Load Julia $ ml Julia/1.8.5 # Start Julia on the command line $ julia # Change to ``package mode`` and add the ``MPI`` package # Load the tool chain which contains a MPI library $ ml OpenMPI/5.0.3-GCC-13.3.0 # Load Julia $ ml Julia/1.10.9-LTS-linux-x86_64 # Start Julia on the command line $ julia # Change to ``package mode`` # Load the tool chain which contains a MPI library $ ml foss/2021b # Load Julia $ ml Julia/1.8.5-linux-x86_64 # Start Julia on the command line $ julia # Change to ``package mode`` # Load the tool chain which contains a MPI library $ ml foss/2021b # Load Julia $ ml Julia/1.8.5-linux-x86_64 # Start Julia on the command line $ julia # Change to ``package mode`` # Load the tool chain for Julia which already contains a MPI library (cray-mpich) $ ml PDCOLD/23.12 julia/1.10.2-cpeGNU-23.12 # Start Julia on the command line $ julia # Change to ``package mode`` # Load the tool chain which contains a MPI library $ ml buildtool-easybuild/4.8.0-hpce082752a2 foss/2023b # Load Julia $ ml julia/1.9.4-bdist # Start Julia on the command line $ julia # Change to ``package mode`` In Julia for all clusters (output may differ for different clusters and Julia versions): shell > mkdir MPI - env shell > cd MPI - env ( @v1 .10 ) pkg > activate . Activating new project at `path-to-folder\\MPI-env` ( MPI - env ) pkg > add MPI # In the ``julian`` mode run these commands: ( MPI - env ) julia > using MPI ( MPI - env ) julia > MPI . install_mpiexecjl () [ Info : Installing `mpiexecjl` to `/home/u/username/.julia/bin` ... [ Info : Done! End Julia with <CTRL>+D In terminal shell for all clusters (output may differ for different clusters and Julia versions): # Add the installed ``mpiexecjl`` wrapper to your path on the Linux command line $ export PATH = ~/.julia/bin: $PATH # Now the wrapper should be available on the command line Extra Challenge. Project environment with csv Create a project environment called new-env and activate it. Then, install the package CSV in this environment. For your knowledge, CSV is a package that offers tools for dealing with .csv files. After this, check that this package was installed. Finally, deactivate the environment. Solution for all centres ``` julia shell> mkdir new-env shell> cd new-env (@v1.8) pkg> activate . Activating new project at `path-to-folder\\new-env` (new-env) pkg> add CSV (new-env) pkg> status Status `path-to-folder\\new-env\\Project.toml` [336ed68f] CSV v0.10.9 (new-env) pkg> deactivate ``` Summary Environments in Julia created by Julia itself so third party software are not required. With a virtual environment you can tailor an environment with specific versions for Julia and packages, not interfering with other installed Julia versions and packages. Make it for each project you have for reproducibility. The environments in Julia are lightweight so it is recommended to start a new environment for each project that you are developing. Bianca Extra reading Julia environments","title":"Environments and packages"},{"location":"julia/environments_packages/#managing-environments-and-packages","text":"Learning outcomes for this session How to work with Julia\u2019s environment and package management. How to check for and use site-installed packages, if any. Your background? Have you used Python? Have you used Matlab? Were you at yesterdays Matlab sessions? Have you used Git? Are you completely new to this, code packages and version control? Key points Environments Project Version control Nested, base, tools not part of the project, use with caution Packages","title":"Managing environments and packages"},{"location":"julia/environments_packages/#introduction","text":"Packages are pieces of software to be used by your scripts or interactive sessions as-is or modified, stand-alone or\u2014most often\u2014in combinations to make a full toolkit customized to your task. Such a toolkit may itself become a new package. Packages allow us to cooperate on code that can be useful in several places. Environments are sets of packages that are available for use simultaneously and, if all is right, work together. If you come from Python, you have likely seen that there are several different ways to deal with environments and package management in this language, for instance, conda and pip . The situation can be described as several ecosystems of packages. You may have come across the term \u201cdependency hell\u201d or even had a taste (or perhaps far more than you could stomach) of this yourself. Julia comes with a great system for environments and packages included. As a result there is essentially a single ecosystem of packages and \u201cdependency hell\u201d is as close to eliminated as it can be. Julia environments are defined by two TOML (Tom\u2019s Obvious Minimal Language) files, Project.toml and Manifest.toml . The former specifies which packages you\u2019ve asked for, the latter specifies all packages loaded including their exact versions. Include both of these files ( Project.toml all the time, Manifest.toml at points for which you want exact reproducibility) in your version control and you get traceable, reproducible environments with minimal effort!","title":"Introduction"},{"location":"julia/environments_packages/#getting-started","text":"To get started we will go to the Julia documentation and make use of the official tutorial on the Julia package manager: https://docs.julialang.org/en/v1/stdlib/Pkg/#Pkg","title":"Getting started"},{"location":"julia/environments_packages/#summary-of-what-weve-seen-so-far","text":"Get to the Pkg REPL by pressing ] in the Julia REPL In the Pkg REPL, use the following commands: ? to get help activate to activate an existing or new environment st (alias for status ) to see which packages have been added to the active environment add to add packages to the active environment, this may or may not have download and precompilation steps that take a bit of time rm (alias for remove ) to remove packages from the active environment up (alias for update ) to update packages in the active environment Project.toml stores what packages you\u2019ve asked for Manifest.toml stores how Pkg resolved this, with all dependencies and exact versions Environment stacking When you have not activated any specific environment, the active environment is your personal base environment for the Julia version you\u2019re currently running (called e.g. @v1.11). This is normally reachable in addition to your active environment. This can be a convenient way to access development tools that you want active but your project does not depend on . If you have tools available through your base environment and you need to check that your project can be reproduced properly without your base environment, you\u2019ll want to read this: Loading project environment only The full stack of reachable environments are defined by the global Julia variable LOAD_PATH : julia>LOAD_PATH 3-element Vector{String}: \"@\" \"@v#.#\" \"@stdlib\" where @ is the active environment, @v#.# is your base environment for the Julia version in use, and @stdlib is the standard library. To include just the current environment we can modify the LOAD_PATH variable from the julian prompt with the following functions: julia> empty!(LOAD_PATH) # this will clean out the path julia> push!(LOAD_PATH, \"@\") # this will add the current environment","title":"Summary of what we&rsquo;ve seen so far"},{"location":"julia/environments_packages/#choosing-package-versions","text":"] add < package > @< version > Example: ] add DataFrames @ 1.1","title":"Choosing package versions"},{"location":"julia/environments_packages/#but-where-are-the-packages-installed","text":"Each environment only consists of two files, the code is not here. Packages will be installed to and loaded from paths defined by the global Julia variable DEPOT_PATH . In a Julia installation on the clusters this will by default be set to: ~/.julia where ~ is the user home as appropriate on the system. If Julia is installed centrally On a local system like a personal linux computer or if Julia would have been installed at a \u201csystem level\u201d and not i a module system an architecture-specific shared system directory, e.g. /usr/local/share/julia ; an architecture-independent shared system directory, e.g. /usr/share/julia . Packages can consist of relatively many files and some clusters have a limit on the number of files you can have in your home directory. If this becomes a problem, you can set the DEPOT_PATH to a Project folder","title":"But where are the packages installed?"},{"location":"julia/environments_packages/#non-interactive-use-of-pkg","text":"We\u2019ve seen how to use Pkg interactively, with its special REPL mode. All the functions of Pkg can also be used by loading the Pkg module and calling its functions just like any other module. On the clusters, this is especially useful for activating or even setting up an environment from inside a Julia script, so that you can run it in a batch job. For example: activate the environment of the current folder by calling these two lines in your .jl file: using Pkg Pkg . activate ( \".\" ) Besides the previous two options for activating an environment, you can also activate it on the Linux command line (assuming that you are located in the environment directory): julia --project=. Bianca At Bianca there is a central library with installed packages. You may control the present \u201ccentral library\u201d by typing ml help julia/<version> in the BASH shell. A possibly more up-to-date status can be found from the Julia shell: julia> using Pkg julia> Pkg . activate ( DEPOT_PATH [ 2 ] * \"/environments/v1.8\" ); #change version (1.8) accordingly if you have another main version of Julia julia> Pkg . status () julia> Pkg . activate ( DEPOT_PATH [ 1 ] * \"/environments/v1.8\" ); #to return to user library A selection of the Julia packages and libraries installed on Bianca are: - BenchmarkTools - CSV - CUDA - MPI - Distributed - IJulia - Plots - PyPlot - Gadfly - DataFrames - DistributedArrays - PlotlyJS Site-installed packages in environments At Bianca the central environment adds to the environment stack: julia> LOAD_PATH 4-element Vector{String}: \"@\" \"@v#.#\" \"@stdlib\" \"/sw/comp/julia/1.8.5/rackham/lib/glob_pkg/environments/v1.8\" Bianca Intermediate workshop Info page Course material Julia on Bianca","title":"Non-interactive use of Pkg"},{"location":"julia/environments_packages/#exercises","text":"We need the packages IJulia and Pluto for running the integrated development environments (IDEs) Jupyter and Pluto. We also need the MPI package on Friday. Make these exercises be the installation of the packages that we will later use. It might be advisable to install IJulia and Pluto in separate environments. Challenge 1. Install Pluto It may take 5-10 minutes or so. This you can do in an ordinary terminal NSC PDC UPPMAX (Bianca/Rackham) UPPMAX (Pelle) HPC2N & LUNARC $ ml julia/1.10.2-bdist $ julia Note: not fully tested successfully, but this step works $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 $ julia $ module load julia/1.8.5 $ module load Julia/1.10.9-LTS-linux-x86_64 $ module load GCCcore/13.2.0 Julia/1.9.3-linux-x86_64 $ julia In Julia for all clusters (output may differ for different clusters and Julia versions): shell > mkdir pluto - env shell > cd pluto - env ( @v1 .10 ) pkg > activate . Activating new project at `path-to-folder\\pluto-env` ( pluto - env ) pkg > add Pluto ( pluto - env ) pkg > status Status `path-to-folder\\pluto-env\\Project.toml` [ c3e4b0f8 ] Pluto v0 .20.19 2. Install IJulia This is done only once, but for each combination of Julia you would like to use. Also Python must be loaded It may take 5-10 minutes or so. This you can do in an ordinary terminal (book an interactive session, for safety) NSC PDC UPPMAX (Bianca/Rackham) UPPMAX (Pelle) HPC2N & LUNARC $ ml Python/3.11.5-env-hpc1-gcc-2023b-eb $ ml julia/1.10.2-bdist $ julia $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 $ ml cray-python/3.11.5 $ julia $ module load julia/1.8.5 $ module load python/3.9.5 $ julia $ module load Julia/1.10.9-LTS-linux-x86_64 $ module load JupyterLab/4.2.5-GCCcore-13.3.0 $ julia $ module load GCC/13.2.0 JupyterLab/4.2.0 $ module load Julia/1.8.5-linux-x86_64 $ julia In Julia for all clusters (output may differ for different clusters and Julia versions): shell > mkdir jupyter - env shell > cd jupyter - env ( @v1 .10 ) pkg > activate . Activating new project at `path-to-folder\\jupyter-env` ( jupyter - env ) pkg > add IJulia ( jupyter - env ) pkg > status Status `path-to-folder\\jupyter-env\\Project.toml` [ 7073 ff75 ] IJulia v1 .27.0 Challenge 3. Required package for parallel jobs In order to use MPI with Julia you will need to follow the next steps (only the first time): UPPMAX (Bianca/Rackham) UPPMAX (Pelle) HPC2N LUNARC PDC NSC # Load the tool chain which contains a MPI library $ ml gcc/11.3.0 openmpi/4.1.3 # Load Julia $ ml Julia/1.8.5 # Start Julia on the command line $ julia # Change to ``package mode`` and add the ``MPI`` package # Load the tool chain which contains a MPI library $ ml OpenMPI/5.0.3-GCC-13.3.0 # Load Julia $ ml Julia/1.10.9-LTS-linux-x86_64 # Start Julia on the command line $ julia # Change to ``package mode`` # Load the tool chain which contains a MPI library $ ml foss/2021b # Load Julia $ ml Julia/1.8.5-linux-x86_64 # Start Julia on the command line $ julia # Change to ``package mode`` # Load the tool chain which contains a MPI library $ ml foss/2021b # Load Julia $ ml Julia/1.8.5-linux-x86_64 # Start Julia on the command line $ julia # Change to ``package mode`` # Load the tool chain for Julia which already contains a MPI library (cray-mpich) $ ml PDCOLD/23.12 julia/1.10.2-cpeGNU-23.12 # Start Julia on the command line $ julia # Change to ``package mode`` # Load the tool chain which contains a MPI library $ ml buildtool-easybuild/4.8.0-hpce082752a2 foss/2023b # Load Julia $ ml julia/1.9.4-bdist # Start Julia on the command line $ julia # Change to ``package mode`` In Julia for all clusters (output may differ for different clusters and Julia versions): shell > mkdir MPI - env shell > cd MPI - env ( @v1 .10 ) pkg > activate . Activating new project at `path-to-folder\\MPI-env` ( MPI - env ) pkg > add MPI # In the ``julian`` mode run these commands: ( MPI - env ) julia > using MPI ( MPI - env ) julia > MPI . install_mpiexecjl () [ Info : Installing `mpiexecjl` to `/home/u/username/.julia/bin` ... [ Info : Done! End Julia with <CTRL>+D In terminal shell for all clusters (output may differ for different clusters and Julia versions): # Add the installed ``mpiexecjl`` wrapper to your path on the Linux command line $ export PATH = ~/.julia/bin: $PATH # Now the wrapper should be available on the command line Extra Challenge. Project environment with csv Create a project environment called new-env and activate it. Then, install the package CSV in this environment. For your knowledge, CSV is a package that offers tools for dealing with .csv files. After this, check that this package was installed. Finally, deactivate the environment. Solution for all centres ``` julia shell> mkdir new-env shell> cd new-env (@v1.8) pkg> activate . Activating new project at `path-to-folder\\new-env` (new-env) pkg> add CSV (new-env) pkg> status Status `path-to-folder\\new-env\\Project.toml` [336ed68f] CSV v0.10.9 (new-env) pkg> deactivate ``` Summary Environments in Julia created by Julia itself so third party software are not required. With a virtual environment you can tailor an environment with specific versions for Julia and packages, not interfering with other installed Julia versions and packages. Make it for each project you have for reproducibility. The environments in Julia are lightweight so it is recommended to start a new environment for each project that you are developing. Bianca Extra reading Julia environments","title":"Exercises"},{"location":"julia/evaluation-julia/","text":"Evaluation \u00b6 This is the page for evaluating the current iteration of the course. The evaluation form for the Julia part can be found here . It takes into account that one may need to leave early too. Where can I find the results of earlier evaluations? At the \u2018Evaluations\u2019 page . Any feedback during the day \u00b6 Form to submit any feedback during the day For teachers: what is in that form? Thanks for your feedback. This feedback will be published as-is at the end of the day, if and only if there are no personal details (email, address, etc.) in the feedback. Do mention the teachers, assistants, etc by name! Evaluation questions \u00b6 Evaluation questions form Why do you evaluate under lesson hours? Because we value your time: your free time should be your free time. We think the time lost teaching is worth it to improve our teaching. For teachers: what is in that form? Question 1: Overall, how would you rate today\u2019s training event? A grade from 1 to (and including) 10 Question 2: What do you think about the pace of teaching overall? [Free text] Question 3: Confidences I can use the module system to load a specific verison of Julia I can run Julia I can use the Julia interpreter I can run a Julia script I can install a Julia package I can work with Julia project environments I can write a batch script I can submit a batch script to the job scheduler I can work with Julia on Jupyter notebooks I can work with Julia on Pluto notebooks Coding scheme: Column text Description NA I did not attend that session 0 I have no confidence I can do this 1 I have low confidence I can do this 2 I have some confidence I can do this 3 I have good confidence I can do this 4 I can absolutely do this! Question 4: Would you recommend this course to someone else? Multiple choice: Yes, no, not sure Question 5: Which future training topics would you like to be provided by the training host(s)? [Free text] Question 6: Do you have any additional comments? [Free text]","title":"Evaluation"},{"location":"julia/evaluation-julia/#evaluation","text":"This is the page for evaluating the current iteration of the course. The evaluation form for the Julia part can be found here . It takes into account that one may need to leave early too. Where can I find the results of earlier evaluations? At the \u2018Evaluations\u2019 page .","title":"Evaluation"},{"location":"julia/evaluation-julia/#any-feedback-during-the-day","text":"Form to submit any feedback during the day For teachers: what is in that form? Thanks for your feedback. This feedback will be published as-is at the end of the day, if and only if there are no personal details (email, address, etc.) in the feedback. Do mention the teachers, assistants, etc by name!","title":"Any feedback during the day"},{"location":"julia/evaluation-julia/#evaluation-questions","text":"Evaluation questions form Why do you evaluate under lesson hours? Because we value your time: your free time should be your free time. We think the time lost teaching is worth it to improve our teaching. For teachers: what is in that form? Question 1: Overall, how would you rate today\u2019s training event? A grade from 1 to (and including) 10 Question 2: What do you think about the pace of teaching overall? [Free text] Question 3: Confidences I can use the module system to load a specific verison of Julia I can run Julia I can use the Julia interpreter I can run a Julia script I can install a Julia package I can work with Julia project environments I can write a batch script I can submit a batch script to the job scheduler I can work with Julia on Jupyter notebooks I can work with Julia on Pluto notebooks Coding scheme: Column text Description NA I did not attend that session 0 I have no confidence I can do this 1 I have low confidence I can do this 2 I have some confidence I can do this 3 I have good confidence I can do this 4 I can absolutely do this! Question 4: Would you recommend this course to someone else? Multiple choice: Yes, no, not sure Question 5: Which future training topics would you like to be provided by the training host(s)? [Free text] Question 6: Do you have any additional comments? [Free text]","title":"Evaluation questions"},{"location":"julia/extra/","text":"Julia - extra reading and links \u00b6 Documentation \u00b6 Documentation at the HPC centres UPPMAX HPC2N LUNARC has no specific Julia documentation, but here you can find installed versions (The user demand on Julia has been low). NSC PDC Official Julia documentation Slack channel for Julia and instructions for joining it HPC2N YouTube video on Julia in HPC Courses - Material for improving your programming skills \u00b6 First level \u00b6 The Carpentries teaches basic lab skills for research computing. Programming with Julia (alpha) Second level \u00b6 CodeRefinery develops and maintains training material on software best practices for researchers that already write code. Their material addresses all academic disciplines and tries to be as programming language-independent as possible . Not yet anything Julia specific ENCCS (EuroCC National Competence Centre Sweden) is a national centre that supports industry, public administration and academia accessing and using European supercomputers. They give higher-level training of programming and specific software. Julia for High-Performance Computing Julia for High-Performance Data Analytics Tools for coding \u00b6 A website called Modern Julia Workflows has a good page about tools for Writing your code . Notebooks \u00b6 Jupyter: Jupyter notebooks are familiar to many Python and R users. Pluto.jl: Offers a similar notebook experience to Jupyter, but understands global references between cells, and reactively re-evaluates cells affected by a code change. IDEs \u00b6 While arguably a text editor with extensions, VSCode has a Julia extension that is considered the best-supported IDE for Julia. A text editor like nano , emacs , vim , and a Julia REPL in a second window also creates a good experience. There are Julia plugins for Emacs and Vim and probably more editors. Packages \u00b6 List ecosystems","title":"More about Julia"},{"location":"julia/extra/#julia-extra-reading-and-links","text":"","title":"Julia - extra reading and links"},{"location":"julia/extra/#documentation","text":"Documentation at the HPC centres UPPMAX HPC2N LUNARC has no specific Julia documentation, but here you can find installed versions (The user demand on Julia has been low). NSC PDC Official Julia documentation Slack channel for Julia and instructions for joining it HPC2N YouTube video on Julia in HPC","title":"Documentation"},{"location":"julia/extra/#courses-material-for-improving-your-programming-skills","text":"","title":"Courses - Material for improving your programming skills"},{"location":"julia/extra/#first-level","text":"The Carpentries teaches basic lab skills for research computing. Programming with Julia (alpha)","title":"First level"},{"location":"julia/extra/#second-level","text":"CodeRefinery develops and maintains training material on software best practices for researchers that already write code. Their material addresses all academic disciplines and tries to be as programming language-independent as possible . Not yet anything Julia specific ENCCS (EuroCC National Competence Centre Sweden) is a national centre that supports industry, public administration and academia accessing and using European supercomputers. They give higher-level training of programming and specific software. Julia for High-Performance Computing Julia for High-Performance Data Analytics","title":"Second level"},{"location":"julia/extra/#tools-for-coding","text":"A website called Modern Julia Workflows has a good page about tools for Writing your code .","title":"Tools for coding"},{"location":"julia/extra/#notebooks","text":"Jupyter: Jupyter notebooks are familiar to many Python and R users. Pluto.jl: Offers a similar notebook experience to Jupyter, but understands global references between cells, and reactively re-evaluates cells affected by a code change.","title":"Notebooks"},{"location":"julia/extra/#ides","text":"While arguably a text editor with extensions, VSCode has a Julia extension that is considered the best-supported IDE for Julia. A text editor like nano , emacs , vim , and a Julia REPL in a second window also creates a good experience. There are Julia plugins for Emacs and Vim and probably more editors.","title":"IDEs"},{"location":"julia/extra/#packages","text":"List ecosystems","title":"Packages"},{"location":"julia/interactive/","text":"Julia interactively \u00b6 Learning outcomes for today Be able to start interactive sessions Use the Julia REPL in the interactive session Be able to run Julia in Jupyter notebook OR Pluto Instructor note Intro 5 min Lecture and 10 min Notes It is possible to run Julia directly on the login (including ThinLinc) nodes. But this should only be done for shorter jobs or jobs that do not use a lot of resources, as the login nodes can otherwise become slow for all users. If you want to work interactively with your code or data, you should start an interactive session. If you rather will run a script which won\u2019t use any interactive user input while running, you can instead start a batch job, see next session. There are several ways to run Julia interactively General \u00b6 In order to run interactively, you need to have compute nodes allocated to run on, and this is done through the Slurm system. Interactive sessions at NSC, PDC, HPC2N, UPPMAX and LUNARC \u00b6 Here we define an interactive session as a session with direct access to a compute node. Or alternatively: an interactive session is a session, in which there is no queue before a command is run on a compute node. This differs between HPC2N and UPPMAX : HPC2N: the user remains on a login node. All commands can be sent directly to the compute node using srun UPPMAX: the user is actually on a computer node. Whatever command is done, it is run on the compute node LUNARC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node NSC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node PDC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node Start an interactive session \u00b6 To start an interactive session, one needs to allocate resources on the cluster first. The command to request an interactive node differs per HPC cluster: Cluster interactive salloc GfxLauncher Open OnDemand HPC2N Works Recommended N/A Recommended UPPMAX Recommended Works N/A N/A LUNARC Works N/A Recommended N/A NSC Recommended N/A N/A N/A PDC N/A Recommended Possible N/A Example, HPC2N vs. UPPMAX (also valid for NSC, PDC and LUNARC): graph TD subgraph uppmax[UPPMAX] subgraph login_node[Login nodes] user_on_login_node[User on login node] end subgraph compute_node[Compute nodes] user_on_computer_node[User on compute node] job_on_compute_node[Job on compute node] end end subgraph hpc2n[HPC2N] subgraph hpc2n_login_node[Login nodes] hpc2n_user_on_login_node[User on login node] hpc2n_user_in_interactive_mode[User on login node in interactive session] end subgraph hpc2n_compute_node[Compute nodes] hpcn2_job_on_compute_node[Job on compute node] end end user_on_login_node --> |interactive| user_on_computer_node user_on_login_node --> |sbatch| job_on_compute_node user_on_computer_node --> |exit| user_on_login_node user_on_computer_node --> |srun| user_on_computer_node hpc2n_user_on_login_node --> |salloc| hpc2n_user_in_interactive_mode hpc2n_user_in_interactive_mode --> |exit| hpc2n_user_on_login_node hpc2n_user_on_login_node --> |sbatch| hpcn2_job_on_compute_node hpc2n_user_in_interactive_mode --> |srun| hpcn2_job_on_compute_node First, you make a request for resources with interactive / salloc , like this: Interactive jobs Short serial example for running on different clusters. NSC PDC UPPMAX LUNARC HPC2N $ interactive -n <tasks> --time = HHH:MM:SS -A naiss2025-22-934 $ salloc -n <ntasks> --time = HHH:MM:SS -A naiss2025-22-934 -p <partition> Where is shared , main or gpu We recommend shared Wait until you get the node ssh to the node given and then work there Example: $ ssh nid001057 $ interactive -n <tasks> --time = HHH:MM:SS -A uppmax2025-2-360 $ interactive -n <tasks> --time = HHH:MM:SS -A lu2025-2-94 $ salloc -n <tasks> --time = HHH:MM:SS -A hpc2n2025-151 where is the number of tasks (or cores, for default 1 task per core), time is given in hours, minutes, and seconds (maximum T168 hours), and then you give the id for your project. Then, when you get the allocation, one can run programs in parallel by: srun -n <ntasks> ./program Your request enters the job queue just like any other job, and interactive/salloc will tell you that it is waiting for the requested resources. When salloc tells you that your job has been allocated resources, you can interactively run programs on those resources with srun . The commands you run with srun will then be executed on the resources your job has been allocated. On HPC2N If you do not preface with srun the command is run on the login node! You can now run Julia scripts on the allocated resources directly instead of waiting for your batch job to return a result. This is an advantage if you want to test your Julia script or perhaps figure out which parameters are best. End an interactive session \u00b6 When you have finished using the allocation, either wait for it to end, or close it with exit $ exit logout Documentation at the centers Interactive allocation on PDC Interactive allocation on NSC Interactive allocation on UPPMAX Interactive allocation on HPC2N Interactive allocation on LUNARC Exercise interactive session \u00b6 Run from ThinLinc (web or client!) Log in page We will need it later for the notebooks! Start a terminal shell from ThinLinc. Interactive jobs Requesting 4 cores for 10 minutes, then running Julia Short serial example for running on different clusters. NSC PDC UPPMAX (Pelle) UPPMAX (Rackham/Bianca) HPC2N LUNARC [ sm_bcarl@tetralith3 ~ ] $ interactive -n 4 -t 0 :30:0 -A naiss2025-22-934 salloc: Pending job allocation 43071298 salloc: job 43071298 queued and waiting for resources salloc: job 43071298 has been allocated resources salloc: Granted job allocation 43071298 salloc: Waiting for resource configuration salloc: Nodes n760 are ready for job [ bjornc@r483 ~ ] $ module load julia/1.10.2-bdist Let us check that we actually run on the compute node: [ sm_bcarl@n760 ~ ] $ srun hostname n760 n760 n760 n760 We are. Notice that we got a response from all four cores we have allocated. claremar@login1:~> salloc --ntasks = 4 -t 0 :30:00 -p shared --qos = normal -A naiss2025-22-934 salloc: Pending job allocation 9102757 salloc: job 9102757 queued and waiting for resources salloc: job 9102757 has been allocated resources salloc: Granted job allocation 9102757 salloc: Waiting for resource configuration salloc: Nodes nid001057 are ready for job claremar@login1:~> module load PDC/23.12 julia/1.10.2-cpeGNU-23.12 Let us check that we actually run on the compute node. This has to be done differently claremar@login1:~> srun hostname nid001064 nid001063 nid001064 nid001063 Now, it seems that Dardel allows for \u201chyperthreading\u201d, that is 2 threads per core. claremar@login1:~> srun -n 8 hostname nid001064 nid001064 nid001063 nid001063 nid001064 nid001064 nid001063 nid001063 We are. Notice that we got a response from all four cores we have allocated. [ bjornc@pelle ~ ] $ interactive -A uppmax2025-2-360 -n 4 -t 0 :30:00 You receive the high interactive priority. There are free cores, so your job is expected to start at once. Please, use no more than 6 .4 GB of RAM. Waiting for job 29556505 to start... Starting job now -- you waited for 1 second. [ bjornc@p102 ~ ] $ module load Julia/1.10.9-LTS-linux-x86_64 Let us check that we actually run on the compute node: [ bjornc@p102 ~ ] $ srun hostname p102.uppmax.uu.se p102.uppmax.uu.se p102.uppmax.uu.se p102.uppmax.uu.se We are. Notice that we got a response from all four cores we have allocated. [ bjornc@rackham2 ~ ] $ interactive -A uppmax2025-2-360 -p core -n 4 -t 0 :30:00 You receive the high interactive priority. There are free cores, so your job is expected to start at once. Please, use no more than 6 .4 GB of RAM. Waiting for job 29556505 to start... Starting job now -- you waited for 1 second. [ bjornc@r102 ~ ] $ module load julia/1.8.5 Let us check that we actually run on the compute node: [ bjornc@r102 ~ ] $ srun hostname r102.uppmax.uu.se r102.uppmax.uu.se r102.uppmax.uu.se r102.uppmax.uu.se We are. Notice that we got a response from all four cores we have allocated. [ ~ ] $ salloc -n 4 --time = 00 :30:00 -A hpc2n2025-151 salloc: Pending job allocation 20174806 salloc: job 20174806 queued and waiting for resources salloc: job 20174806 has been allocated resources salloc: Granted job allocation 20174806 salloc: Waiting for resource configuration salloc: Nodes b-cn0241 are ready for job [ ~ ] $ module load GCC/11.2.0 OpenMPI/4.1.1 julia/1.8.5 [ ~ ] $ Let us check that we actually run on the compute node: [ ~ ] $ srun hostname b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se We are. Notice that we got a response from all four cores we have allocated. [ bjornc@cosmos1 ~ ] $ interactive -A lu2025-2-94 -n 4 -t 30 :00 Cluster name: COSMOS Waiting for JOBID 930844 to start [ bjornc@cn050 ~ ] $ module load Julia/1.8.5-linux-x86_64 Let us check that we actually run on the compute node: [ bjornc@cn050 ~ ] $ echo $SLURM_CPUS_ON_NODE 4 We are, because the $SLURM environment variable gves an output. Notice that we got 4, which is not the size of the physcial node bt the allocation size. Test script : adding two numbers from user input ( serial-sum.jl ) # This program will add two numbers that are provided by the user # Get the numbers x = parse ( Int32 , ARGS [ 1 ] ) y = parse ( Int32 , ARGS [ 2 ] ) # Add the two numbers together summ = x + y println ( \"The sum of the two numbers is \" , summ ) Running the script Note that the commands should be the same for all clusters Running a Julia script in the allocation we made further up. Notice that since we asked for 4 cores, the script is run 4 times, since it is a serial script [ ~ ] $ srun julia serial-sum.jl 3 4 The sum of the two numbers is: 7 The sum of the two numbers is: 7 The sum of the two numbers is: 7 The sum of the two numbers is: 7 [ ~ ] $ Without the srun command, Julia won\u2019t understand that it can use several cores. Therefore the program is run only once. [ ~ ] $ julia serial-sum.jl 3 4 The sum of the two numbers is: 7 Running Julia REPL First start Julia using the 4 cores and check if workers are available $ julia -p 4 julia > nworkers () 4 Exit Julia julia > < CTRL - D > 4 Do not exit yet! Info Meanwhile waiting let\u2019s talk about notebooks! Notebooks \u00b6 Jupyter \u00b6 JuPyteR was written for serving notebooks for Julia, Python and R Pluto \u00b6 Pluto, like Jupyter, is a programming notebook. Reproducible (even without Pluto), gittable, extraordinarily interactive programming notebooks Offers a similar notebook experience to Jupyter, but understands global references between cells, and reactively re-evaluates cells affected by a code change. Exercise: Choose to start Jupyter OR Pluto \u00b6 Important You should be using ThinLinc. And from the terminal have started a interactive session See last exercise Start Jupyter Start Pluto Exit When you have finished using the allocation, either wait for it to end, or close it with exit NSC PDC UPPMAX HPC2N LUNARC [ sm_bcarl@n134 ~ ] $ exit logout srun: error: n134: task 0 : Exited with exit code 130 srun: Terminating StepId = 43071803 .interactive salloc: Relinquishing job allocation 43071803 salloc: Job allocation 43071803 has been revoked. [ sm_bcarl@tetralith3 ~ ] $ claremar@login1:~> exit exit salloc: Relinquishing job allocation 9103056 claremar@login1:~> [ bjornc@r483 ~ ] $ exit exit [ screen is terminating ] Connection to r483 closed. [ bjornc@rackham2 ~ ] $ [ ~ ] $ exit exit salloc: Relinquishing job allocation 20174806 salloc: Job allocation 20174806 has been revoked. [ ~ ] $ [ ~ ] $ exit exit [ screen is terminating ] Connection to cn050 closed. [ ~ ] $ Exercises run scripts \u00b6 Run scripts from an interactive session Try out one or two of the scripts from the exercise folder batchJulia . First create an interactive session with the right Slurm commands to the interactive / salloc command. use the commands from the batch job script belonging to the julia script at examples of batch scripts for julia Summary Start an interactive session on a calculation node by a SLURM allocation At HPC2N: salloc \u2026 At UPPMAX/LUNARC: interactive \u2026 Follow the same procedure as usual by loading the Julia module and possible prerequisites. Run Julia in Jupyter lab/notebook Procedure is to use the IJulia package and start a jupyter notebook from the julia command line.","title":"Interactive work on the compute nodes"},{"location":"julia/interactive/#julia-interactively","text":"Learning outcomes for today Be able to start interactive sessions Use the Julia REPL in the interactive session Be able to run Julia in Jupyter notebook OR Pluto Instructor note Intro 5 min Lecture and 10 min Notes It is possible to run Julia directly on the login (including ThinLinc) nodes. But this should only be done for shorter jobs or jobs that do not use a lot of resources, as the login nodes can otherwise become slow for all users. If you want to work interactively with your code or data, you should start an interactive session. If you rather will run a script which won\u2019t use any interactive user input while running, you can instead start a batch job, see next session. There are several ways to run Julia interactively","title":"Julia interactively"},{"location":"julia/interactive/#general","text":"In order to run interactively, you need to have compute nodes allocated to run on, and this is done through the Slurm system.","title":"General"},{"location":"julia/interactive/#interactive-sessions-at-nsc-pdc-hpc2n-uppmax-and-lunarc","text":"Here we define an interactive session as a session with direct access to a compute node. Or alternatively: an interactive session is a session, in which there is no queue before a command is run on a compute node. This differs between HPC2N and UPPMAX : HPC2N: the user remains on a login node. All commands can be sent directly to the compute node using srun UPPMAX: the user is actually on a computer node. Whatever command is done, it is run on the compute node LUNARC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node NSC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node PDC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node","title":"Interactive sessions at NSC, PDC, HPC2N, UPPMAX and LUNARC"},{"location":"julia/interactive/#start-an-interactive-session","text":"To start an interactive session, one needs to allocate resources on the cluster first. The command to request an interactive node differs per HPC cluster: Cluster interactive salloc GfxLauncher Open OnDemand HPC2N Works Recommended N/A Recommended UPPMAX Recommended Works N/A N/A LUNARC Works N/A Recommended N/A NSC Recommended N/A N/A N/A PDC N/A Recommended Possible N/A Example, HPC2N vs. UPPMAX (also valid for NSC, PDC and LUNARC): graph TD subgraph uppmax[UPPMAX] subgraph login_node[Login nodes] user_on_login_node[User on login node] end subgraph compute_node[Compute nodes] user_on_computer_node[User on compute node] job_on_compute_node[Job on compute node] end end subgraph hpc2n[HPC2N] subgraph hpc2n_login_node[Login nodes] hpc2n_user_on_login_node[User on login node] hpc2n_user_in_interactive_mode[User on login node in interactive session] end subgraph hpc2n_compute_node[Compute nodes] hpcn2_job_on_compute_node[Job on compute node] end end user_on_login_node --> |interactive| user_on_computer_node user_on_login_node --> |sbatch| job_on_compute_node user_on_computer_node --> |exit| user_on_login_node user_on_computer_node --> |srun| user_on_computer_node hpc2n_user_on_login_node --> |salloc| hpc2n_user_in_interactive_mode hpc2n_user_in_interactive_mode --> |exit| hpc2n_user_on_login_node hpc2n_user_on_login_node --> |sbatch| hpcn2_job_on_compute_node hpc2n_user_in_interactive_mode --> |srun| hpcn2_job_on_compute_node First, you make a request for resources with interactive / salloc , like this: Interactive jobs Short serial example for running on different clusters. NSC PDC UPPMAX LUNARC HPC2N $ interactive -n <tasks> --time = HHH:MM:SS -A naiss2025-22-934 $ salloc -n <ntasks> --time = HHH:MM:SS -A naiss2025-22-934 -p <partition> Where is shared , main or gpu We recommend shared Wait until you get the node ssh to the node given and then work there Example: $ ssh nid001057 $ interactive -n <tasks> --time = HHH:MM:SS -A uppmax2025-2-360 $ interactive -n <tasks> --time = HHH:MM:SS -A lu2025-2-94 $ salloc -n <tasks> --time = HHH:MM:SS -A hpc2n2025-151 where is the number of tasks (or cores, for default 1 task per core), time is given in hours, minutes, and seconds (maximum T168 hours), and then you give the id for your project. Then, when you get the allocation, one can run programs in parallel by: srun -n <ntasks> ./program Your request enters the job queue just like any other job, and interactive/salloc will tell you that it is waiting for the requested resources. When salloc tells you that your job has been allocated resources, you can interactively run programs on those resources with srun . The commands you run with srun will then be executed on the resources your job has been allocated. On HPC2N If you do not preface with srun the command is run on the login node! You can now run Julia scripts on the allocated resources directly instead of waiting for your batch job to return a result. This is an advantage if you want to test your Julia script or perhaps figure out which parameters are best.","title":"Start an interactive session"},{"location":"julia/interactive/#end-an-interactive-session","text":"When you have finished using the allocation, either wait for it to end, or close it with exit $ exit logout Documentation at the centers Interactive allocation on PDC Interactive allocation on NSC Interactive allocation on UPPMAX Interactive allocation on HPC2N Interactive allocation on LUNARC","title":"End an interactive session"},{"location":"julia/interactive/#exercise-interactive-session","text":"Run from ThinLinc (web or client!) Log in page We will need it later for the notebooks! Start a terminal shell from ThinLinc. Interactive jobs Requesting 4 cores for 10 minutes, then running Julia Short serial example for running on different clusters. NSC PDC UPPMAX (Pelle) UPPMAX (Rackham/Bianca) HPC2N LUNARC [ sm_bcarl@tetralith3 ~ ] $ interactive -n 4 -t 0 :30:0 -A naiss2025-22-934 salloc: Pending job allocation 43071298 salloc: job 43071298 queued and waiting for resources salloc: job 43071298 has been allocated resources salloc: Granted job allocation 43071298 salloc: Waiting for resource configuration salloc: Nodes n760 are ready for job [ bjornc@r483 ~ ] $ module load julia/1.10.2-bdist Let us check that we actually run on the compute node: [ sm_bcarl@n760 ~ ] $ srun hostname n760 n760 n760 n760 We are. Notice that we got a response from all four cores we have allocated. claremar@login1:~> salloc --ntasks = 4 -t 0 :30:00 -p shared --qos = normal -A naiss2025-22-934 salloc: Pending job allocation 9102757 salloc: job 9102757 queued and waiting for resources salloc: job 9102757 has been allocated resources salloc: Granted job allocation 9102757 salloc: Waiting for resource configuration salloc: Nodes nid001057 are ready for job claremar@login1:~> module load PDC/23.12 julia/1.10.2-cpeGNU-23.12 Let us check that we actually run on the compute node. This has to be done differently claremar@login1:~> srun hostname nid001064 nid001063 nid001064 nid001063 Now, it seems that Dardel allows for \u201chyperthreading\u201d, that is 2 threads per core. claremar@login1:~> srun -n 8 hostname nid001064 nid001064 nid001063 nid001063 nid001064 nid001064 nid001063 nid001063 We are. Notice that we got a response from all four cores we have allocated. [ bjornc@pelle ~ ] $ interactive -A uppmax2025-2-360 -n 4 -t 0 :30:00 You receive the high interactive priority. There are free cores, so your job is expected to start at once. Please, use no more than 6 .4 GB of RAM. Waiting for job 29556505 to start... Starting job now -- you waited for 1 second. [ bjornc@p102 ~ ] $ module load Julia/1.10.9-LTS-linux-x86_64 Let us check that we actually run on the compute node: [ bjornc@p102 ~ ] $ srun hostname p102.uppmax.uu.se p102.uppmax.uu.se p102.uppmax.uu.se p102.uppmax.uu.se We are. Notice that we got a response from all four cores we have allocated. [ bjornc@rackham2 ~ ] $ interactive -A uppmax2025-2-360 -p core -n 4 -t 0 :30:00 You receive the high interactive priority. There are free cores, so your job is expected to start at once. Please, use no more than 6 .4 GB of RAM. Waiting for job 29556505 to start... Starting job now -- you waited for 1 second. [ bjornc@r102 ~ ] $ module load julia/1.8.5 Let us check that we actually run on the compute node: [ bjornc@r102 ~ ] $ srun hostname r102.uppmax.uu.se r102.uppmax.uu.se r102.uppmax.uu.se r102.uppmax.uu.se We are. Notice that we got a response from all four cores we have allocated. [ ~ ] $ salloc -n 4 --time = 00 :30:00 -A hpc2n2025-151 salloc: Pending job allocation 20174806 salloc: job 20174806 queued and waiting for resources salloc: job 20174806 has been allocated resources salloc: Granted job allocation 20174806 salloc: Waiting for resource configuration salloc: Nodes b-cn0241 are ready for job [ ~ ] $ module load GCC/11.2.0 OpenMPI/4.1.1 julia/1.8.5 [ ~ ] $ Let us check that we actually run on the compute node: [ ~ ] $ srun hostname b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se We are. Notice that we got a response from all four cores we have allocated. [ bjornc@cosmos1 ~ ] $ interactive -A lu2025-2-94 -n 4 -t 30 :00 Cluster name: COSMOS Waiting for JOBID 930844 to start [ bjornc@cn050 ~ ] $ module load Julia/1.8.5-linux-x86_64 Let us check that we actually run on the compute node: [ bjornc@cn050 ~ ] $ echo $SLURM_CPUS_ON_NODE 4 We are, because the $SLURM environment variable gves an output. Notice that we got 4, which is not the size of the physcial node bt the allocation size. Test script : adding two numbers from user input ( serial-sum.jl ) # This program will add two numbers that are provided by the user # Get the numbers x = parse ( Int32 , ARGS [ 1 ] ) y = parse ( Int32 , ARGS [ 2 ] ) # Add the two numbers together summ = x + y println ( \"The sum of the two numbers is \" , summ ) Running the script Note that the commands should be the same for all clusters Running a Julia script in the allocation we made further up. Notice that since we asked for 4 cores, the script is run 4 times, since it is a serial script [ ~ ] $ srun julia serial-sum.jl 3 4 The sum of the two numbers is: 7 The sum of the two numbers is: 7 The sum of the two numbers is: 7 The sum of the two numbers is: 7 [ ~ ] $ Without the srun command, Julia won\u2019t understand that it can use several cores. Therefore the program is run only once. [ ~ ] $ julia serial-sum.jl 3 4 The sum of the two numbers is: 7 Running Julia REPL First start Julia using the 4 cores and check if workers are available $ julia -p 4 julia > nworkers () 4 Exit Julia julia > < CTRL - D > 4 Do not exit yet! Info Meanwhile waiting let\u2019s talk about notebooks!","title":"Exercise interactive session"},{"location":"julia/interactive/#notebooks","text":"","title":"Notebooks"},{"location":"julia/interactive/#jupyter","text":"JuPyteR was written for serving notebooks for Julia, Python and R","title":"Jupyter"},{"location":"julia/interactive/#pluto","text":"Pluto, like Jupyter, is a programming notebook. Reproducible (even without Pluto), gittable, extraordinarily interactive programming notebooks Offers a similar notebook experience to Jupyter, but understands global references between cells, and reactively re-evaluates cells affected by a code change.","title":"Pluto"},{"location":"julia/interactive/#exercise-choose-to-start-jupyter-or-pluto","text":"Important You should be using ThinLinc. And from the terminal have started a interactive session See last exercise Start Jupyter Start Pluto Exit When you have finished using the allocation, either wait for it to end, or close it with exit NSC PDC UPPMAX HPC2N LUNARC [ sm_bcarl@n134 ~ ] $ exit logout srun: error: n134: task 0 : Exited with exit code 130 srun: Terminating StepId = 43071803 .interactive salloc: Relinquishing job allocation 43071803 salloc: Job allocation 43071803 has been revoked. [ sm_bcarl@tetralith3 ~ ] $ claremar@login1:~> exit exit salloc: Relinquishing job allocation 9103056 claremar@login1:~> [ bjornc@r483 ~ ] $ exit exit [ screen is terminating ] Connection to r483 closed. [ bjornc@rackham2 ~ ] $ [ ~ ] $ exit exit salloc: Relinquishing job allocation 20174806 salloc: Job allocation 20174806 has been revoked. [ ~ ] $ [ ~ ] $ exit exit [ screen is terminating ] Connection to cn050 closed. [ ~ ] $","title":"Exercise: Choose to start Jupyter OR Pluto"},{"location":"julia/interactive/#exercises-run-scripts","text":"Run scripts from an interactive session Try out one or two of the scripts from the exercise folder batchJulia . First create an interactive session with the right Slurm commands to the interactive / salloc command. use the commands from the batch job script belonging to the julia script at examples of batch scripts for julia Summary Start an interactive session on a calculation node by a SLURM allocation At HPC2N: salloc \u2026 At UPPMAX/LUNARC: interactive \u2026 Follow the same procedure as usual by loading the Julia module and possible prerequisites. Run Julia in Jupyter lab/notebook Procedure is to use the IJulia package and start a jupyter notebook from the julia command line.","title":"Exercises run scripts"},{"location":"julia/intro/","text":"Why Julia? \u00b6 Learning outcomes for today When Julia is and isn\u2019t the right tool How to load cluster modules providing Julia How to use Julia environments, and why Get familiar with Pkg , installing and managing Julia packages How to write batch scripts for running non-interactive Julia jobs How to work interactively with Julia on the clusters This will be on Friday Explore different Julia parallelization tools Learn how Julia supports GPU computing How to run ML jobs in Julia Not covered in this course Improve Julia coding skills Other clusters or systems Why it was created \u00b6 Quoting from the 2012 blogpost that announced Julia to the world: In short, because we are greedy. They mention Matlab, Lisp, Python, Ruby, Perl, Mathematica, R and C. We love all of these languages; they are wonderful and powerful. For the work we do \u2014 scientific computing, machine learning, data mining, large-scale linear algebra, distributed and parallel computing \u2014 each one is perfect for some aspects of the work and terrible for others. Each one is a trade-off. We are greedy: we want more. No language can be everything for everyone, but Julia was created to get as close as possible for a certain set of users. If the work you do falls in the categories mentioned, as HPC work does, this target audience includes you. Reasons to choose Julia \u00b6 The strengths of Julia are based on flexibility. Moreso than forcing either your code or your workflow into anything, it enables you to choose the right way to write or work for the task at hand. Special compilation model Enables interactive use and development without the drawbacks of interpreted languages Special type system Hierarchical, deep, types can be defined as precisely or loosely as makes sense for each situation This (along with multiple dispatch and the compilation model) enables not only great flexibility but also great composability Built for numerical computing, but works well also for general tasks Built-in powerful package management (based on Git) with efficient per-project environments, very good for reproducibility Finally, the built-in or integrated documentation tools are good enough that many Julia packages have relatively good documentation. In addition to the strengths of the core language, a few standout packages that you may want to make use of: Pluto : Reproducible (even without Pluto), gittable, extraordinarily interactive programming notebooks Flux : Machine learning stack based on differentiable programming, which Julia does especially well DifferentialEquations , which is especially state-of-the-art, or other components from the large SciML toolkit for ML in scientific computing JuMP , a modeling language and toolkit for mathematical optimization Reasons to choose something else \u00b6 An interpreted language (such as R or MATLAB) will execute your instructions immediately, without compilation. If your code changes and recompiles a lot in relation to the time spent running the code and the computations, this will give you a better interactive experience and save CPU time. For really large projects, the structure and consistency enforced by a more rigid language such as Java can be an important benefit. As always, it may be faster to use a tool you already know well than to use a tool that you have not learned yet. There are some cases where existing tools are best used from their own environments (often the case for graphical tools), but in general the Julia solutions for using packages and libraries from other languages (R, MATLAB, Python, Fortran, C) are very useful. Questions? What are your first impressions, what do you hope that Julia can solve for you? Which tools do you need to find to start using Julia? What do you need to learn from us, and what can you easily learn from online resources? More on Julia \u00b6 Almost everything you could ask for can be found from or on the official website https://julialang.org . Some shortcuts: Official Julia documentation Official Julia discussion/support forum Many other channels, e.g. Slack, Zulip, Discord are found in this list The list of GitHub organizations can be a starting point for finding the packages relevant to your field And a few other extra resources: A big list of AI tools HPC2N YouTube video on Julia in HPC Extra materials in these course pages Julia documentation at the centers \u00b6 UPPMAX HPC2N LUNARC has no specific Julia documentation, but here you can find installed versions NSC PDC","title":"Intro"},{"location":"julia/intro/#why-julia","text":"Learning outcomes for today When Julia is and isn\u2019t the right tool How to load cluster modules providing Julia How to use Julia environments, and why Get familiar with Pkg , installing and managing Julia packages How to write batch scripts for running non-interactive Julia jobs How to work interactively with Julia on the clusters This will be on Friday Explore different Julia parallelization tools Learn how Julia supports GPU computing How to run ML jobs in Julia Not covered in this course Improve Julia coding skills Other clusters or systems","title":"Why Julia?"},{"location":"julia/intro/#why-it-was-created","text":"Quoting from the 2012 blogpost that announced Julia to the world: In short, because we are greedy. They mention Matlab, Lisp, Python, Ruby, Perl, Mathematica, R and C. We love all of these languages; they are wonderful and powerful. For the work we do \u2014 scientific computing, machine learning, data mining, large-scale linear algebra, distributed and parallel computing \u2014 each one is perfect for some aspects of the work and terrible for others. Each one is a trade-off. We are greedy: we want more. No language can be everything for everyone, but Julia was created to get as close as possible for a certain set of users. If the work you do falls in the categories mentioned, as HPC work does, this target audience includes you.","title":"Why it was created"},{"location":"julia/intro/#reasons-to-choose-julia","text":"The strengths of Julia are based on flexibility. Moreso than forcing either your code or your workflow into anything, it enables you to choose the right way to write or work for the task at hand. Special compilation model Enables interactive use and development without the drawbacks of interpreted languages Special type system Hierarchical, deep, types can be defined as precisely or loosely as makes sense for each situation This (along with multiple dispatch and the compilation model) enables not only great flexibility but also great composability Built for numerical computing, but works well also for general tasks Built-in powerful package management (based on Git) with efficient per-project environments, very good for reproducibility Finally, the built-in or integrated documentation tools are good enough that many Julia packages have relatively good documentation. In addition to the strengths of the core language, a few standout packages that you may want to make use of: Pluto : Reproducible (even without Pluto), gittable, extraordinarily interactive programming notebooks Flux : Machine learning stack based on differentiable programming, which Julia does especially well DifferentialEquations , which is especially state-of-the-art, or other components from the large SciML toolkit for ML in scientific computing JuMP , a modeling language and toolkit for mathematical optimization","title":"Reasons to choose Julia"},{"location":"julia/intro/#reasons-to-choose-something-else","text":"An interpreted language (such as R or MATLAB) will execute your instructions immediately, without compilation. If your code changes and recompiles a lot in relation to the time spent running the code and the computations, this will give you a better interactive experience and save CPU time. For really large projects, the structure and consistency enforced by a more rigid language such as Java can be an important benefit. As always, it may be faster to use a tool you already know well than to use a tool that you have not learned yet. There are some cases where existing tools are best used from their own environments (often the case for graphical tools), but in general the Julia solutions for using packages and libraries from other languages (R, MATLAB, Python, Fortran, C) are very useful. Questions? What are your first impressions, what do you hope that Julia can solve for you? Which tools do you need to find to start using Julia? What do you need to learn from us, and what can you easily learn from online resources?","title":"Reasons to choose something else"},{"location":"julia/intro/#more-on-julia","text":"Almost everything you could ask for can be found from or on the official website https://julialang.org . Some shortcuts: Official Julia documentation Official Julia discussion/support forum Many other channels, e.g. Slack, Zulip, Discord are found in this list The list of GitHub organizations can be a starting point for finding the packages relevant to your field And a few other extra resources: A big list of AI tools HPC2N YouTube video on Julia in HPC Extra materials in these course pages","title":"More on Julia"},{"location":"julia/intro/#julia-documentation-at-the-centers","text":"UPPMAX HPC2N LUNARC has no specific Julia documentation, but here you can find installed versions NSC PDC","title":"Julia documentation at the centers"},{"location":"julia/load_run/","text":"Load and run Julia \u00b6 Info At the Swedish HPC centers we call the applications available via the module system modules : NSC PDC UPPMAX HPC2N LUNARC Objectives Learn to load Julia Get started with the Julia command line Learn to run Julia scripts Instructor note Lecture and demo 15 min Exercise 15 min Total time 30 min Julia can be started after a Julia module is loaded. The module activates paths to a specific version of the julia interpreter and its libraries and packages. Short cheat sheet See which modules exists: module spider or ml spider . Find module versions for a particular software: module spider <software> Modules depending only on what is currently loaded: module avail or ml av See which modules are currently loaded: module list or ml Load a module: module load <module>/<version> or ml <module>/<version> Unload a module: module unload <module>/<version> or ml -<module>/<version> Unload all modules except the \u2018sticky\u2019 modules: module purge or ml purge Warning Note that the module systems at UPPMAX and HPC2N are slightly different. All modules at UPPMAX, for instance, not directly related to bio-informatics are shown by ml avail . Modules at many other centres are only available when one has loaded all prerequisites, for instance the compilers ( GNU , Intel , etc.). Check for Julia versions \u00b6 Long-term support versions So far Julia has Long-term support (LTS) for 1.6.7 1.10.X (high X is better) It might good to try to stick with such a version for future compatibility and support. Principle \u00b6 For some clusters module avail julia Example output From Tetralith: $ module avail julia ---------------------------------- /software/sse2/tetralith_el9/modules ----------------------------------- julia/recommendation (D) julia/1.6.1-nsc1-bdist julia/1.9.4-bdist julia/1.1.0-nsc1-gcc-2018a-eb julia/1.7.2-nsc1-bdist julia/1.10.2-bdist julia/1.4.1 julia/1.8.5-nsc1-bdist Where: D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Or, at clusters that hides the modules until relevant dependencies are loaded (HPC2N & PDC) module spider julia Example output From Dardel: $ module spider julia ------------------------------------------------------------------------------------------------------- julia: ------------------------------------------------------------------------------------------------------- Description: Julia is a high-level general-purpose dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science, without the typical need of separate compilation to be fast, also usable for client and server web use, low-level systems programming or as a specification language (wikipedia.org). Julia provides ease and expressiveness for high-level numerical computing, in the same way as languages such as R, MATLAB, and Python, but also supports general programming. To achieve this, Julia builds upon the lineage of mathematical programming languages, but also borrows much from popular dynamic languages, including Lisp, Perl, Python, Lua, and Ruby (julialang.org). Versions: julia/1.8.2-cpeGNU-22.06 julia/1.9.3-cpeGNU-22.06 julia/1.9.3-cpeGNU-23.03 julia/1.10.2-cpeGNU-23.03 julia/1.10.2-cpeGNU-23.12 Other possible modules matches: Julia libuv-julia ------------------------------------------------------------------------------------------------------- To find other possible module matches execute: $ module -r spider '.*julia.*' ------------------------------------------------------------------------------------------------------- For detailed information about a specific \"julia\" package (including how to load the modules) use the module's full name. Note that names that have a trailing (E) are extensions provided by other modules. For example: $ module spider julia/1.10.2-cpeGNU-23.12 ------------------------------------------------------------------------------------------------------- Load a Julia module \u00b6 For reproducibility, we recommend ALWAYS loading a specific module for the Julia version instead of using the default one. Principle \u00b6 Use the output of existing module above! Load the module! At some clusters: module load julia/1.8.5 or at cluster that includes \u201carchitecture\u201d or \u201cbuild name\u201d in module name: ml julia/1.10.2-bdist Some clusters will require other modules to be loaded (Kebnekaise and Dardel) First check how to load (see Check for Julia versions above) $ module spider julia/1.10.2-cpeGNU-23.12 ... You will need to load all module(s) on any one of the lines below before the \"julia/1.10.2-cpeGNU-23.12\" module is available to load. PDC/23.12 Load PDC/23.12 first and then the julia module ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 Run \u00b6 Run Julia as a session \u00b6 After loading the appropriate modules for Julia, you will have access to the read-eval-print-loop (REPL) command line by typing julia : julia In julia REPL Example This is what loading the Julia REPL looks like on Pelle: [username@pelle1 ~]$ ml No modules loaded [username@pelle1 ~]$ ml Julia/1.10.9 [username@pelle1 ~]$ ml Currently Loaded Modules: 1) Julia/1.10.9-LTS-linux-x86_64 [username@pelle1 ~]$ julia _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type \"?\" for help, \"]?\" for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.10.9 (2025-03-10) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia> Modes: Julian mode \u00b6 Julia has different modes, the one we arrive at is the so-called Julian mode, where one can execute commands. The description for accessing these modes will be given in the following paragraphs. Once you are done with your work in any of the modes, you can return to the Julian mode by pressing the backspace key. Shell mode \u00b6 While being on the Julian mode you can enter the shell mode by typing ; : julia>; shell>pwd /current-folder-path this will allow you to use Linux commands. Notice that the availability of these commands depend on the OS, for instance, on Windows it will depend on the terminal that you have installed and if it is visible to the Julia installation. Package manager mode \u00b6 Another mode available in Julia is the package manager mode, it can be accessed by typing ] in the Julian mode: julia>] (v1.8) pkg> this will make your interaction with the package manager Pkg easier, for instance, instead of typing the complete name of Pkg commands such as Pkg.status() in the Julian mode, you can just type status in the package mode. Help mode \u00b6 The last mode is the help mode, you can enter this mode from the Julian one by typing ? , then you may type some string from which you need more information: julia >? help ?> ans search : ans transpose transcode contains expanduser instances MathConstants readlines LinearIndices leading_ones leading_zeros ans A variable referring to the last computed value , automatically set at the interactive prompt . Exiting \u00b6 Exit with julia> <Ctrl-D> or julia> exit() The Julian modes summary enter the shell mode by typing ; go back to Julian mode by <backspace> access the package manager mode by typing ] in the Julian mode use the help mode by typing ? in the Julian mode See also More detailed information about the modes in Julia can be found . Run a Julia script \u00b6 You can run a Julia script on the Linux shell as follows: $ julia example.jl where the script is a text file could contain these lines: println ( \"hello world\" ) Exercises \u00b6 Challenge 1a. Find out which versions are on your cluster from documentation Find/search for that documentation! Solutions UPPMAX HPC2N LUNARC NSC PDC Challenge 1b. Find out which versions are on your cluster from command line Use the spider or avail module commands Solutions UPPMAX HPC2N LUNARC Tetralith Dardel Check all available Julia versions with: $ module avail julia Check all available version Julia versions with: $ module spider julia Notice that the output if you are working on the Intel ( kebnekaise.hpc2n.umu.se ) or AMD ( kebnekaise-amd.hpc2n.umu.se ) login nodes is different. In the former, you will see more installed versions of Julia as this hardware is older. To see how to load a specific version of Julia, including the prerequisites, do $ module spider Julia/<version> Example for Julia 1.8.5 $ module spider Julia/1.8.5-linux-x86_64 Check all available version Julia versions with: $ module spider Julia To see how to load a specific version of Julia, including the prerequisites, do $ module spider Julia/<version> Example for Julia 1.8.5 $ module spider Julia/1.8.5-linux-x86_64 Check all available version Julia versions with: $ module avail Julia Example for Julia 1.8.5 $ module spider julia/1.8.5-nsc1-bdist Check all available version Julia versions with: $ module spider Julia To see how to load a specific version of Julia, including the prerequisites, do $ module spider Julia/<version> Example for Julia 1.8.5 $ module spider Julia/1.8.5-linux-x86_64 Output at UPPMAX as of Oct 2025 Rackham/(Bianca) $ module avail julia ----------------------------- /sw/mf/rackham/compilers ----------------------------- julia/1.0.5_LTS julia/1.6.1 julia/1.7.2 julia/1.9.3 julia/1.1.1 julia/1.6.3 julia/1.8.5 julia/1.10.10_LTS julia/1.4.2 julia/1.6.7_LTS julia/1.9.1 julia/1.11.6 (D) Where: D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Pelle $ ml av Julia ---------------------------------------------------- /sw/arch/eb/modules/all -------------------- Julia/1.10.9-LTS-linux-x86_64 Julia/1.11.3-linux-x86_64 (D) Where: D: Default Module If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Output at HPC2N as of Oct 2025 $ module spider julia # Assuming you are working on the Intel login nodes ------------------------------------------------------------------------------------------------ Julia: ------------------------------------------------------------------------------------------------ Description: Julia is a high-level, high-performance dynamic programming language for numerical computing Versions: Julia/1.5.3-linux-x86_64 Julia/1.7.1-linux-x86_64 Julia/1.8.5-linux-x86_64 Julia/1.9.3-linux-x86_64 ------------------------------------------------------------------------------------------------ For detailed information about a specific \"Julia\" package (including how to load the modules) use the module's full name. Note that names that have a trailing (E) are extensions provided by other modules. For example: $ module spider Julia/1.8.5-linux-x86_64 ------------------------------------------------------------------------------------------------ Output at LUNARC as of Oct 2025 $ module spider julia ----------------------------------------------------------------------------------------------------- Julia: ----------------------------------------------------------------------------------------------------- Description: Julia is a high-level, high-performance dynamic programming language for numerical computing Versions: Julia/1.8.5-linux-x86_64 Julia/1.9.0-linux-x86_64 Julia/1.9.2-linux-x86_64 Julia/1.9.3-linux-x86_64 Julia/1.10.4-linux-x86_64 Output at NSC as of Mar 2025 $ module avail julia ---------------------------------- /software/sse2/tetralith_el9/modules ----------------------------------- julia/recommendation (D) julia/1.6.1-nsc1-bdist julia/1.9.4-bdist julia/1.1.0-nsc1-gcc-2018a-eb julia/1.7.2-nsc1-bdist julia/1.10.2-bdist julia/1.4.1 julia/1.8.5-nsc1-bdist Output at PDC as of Oct 2025 $ module spider julia ------------------------------------------------------------------------------------------------------- julia: ------------------------------------------------------------------------------------------------------- Description: Julia is a high-level general-purpose dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science, without the typical need of separate compilation to be fast, also usable for client and server web use, low-level systems programming or as a specification language (wikipedia.org). Julia provides ease and expressiveness for high-level numerical computing, in the same way as languages such as R, MATLAB, and Python, but also supports general programming. To achieve this, Julia builds upon the lineage of mathematical programming languages, but also borrows much from popular dynamic languages, including Lisp, Perl, Python, Lua, and Ruby (julialang.org). Versions: julia/1.8.2-cpeGNU-22.06 julia/1.9.3-cpeGNU-22.06 julia/1.9.3-cpeGNU-23.03 julia/1.10.2-cpeGNU-23.03 julia/1.10.2-cpeGNU-23.12 julia/1.11.4-cpeAMD-24.11 Other possible modules matches: Julia libuv-julia ------------------------------------------------------------------------------------------------------- To find other possible module matches execute: $ module -r spider '.*julia.*' ------------------------------------------------------------------------------------------------------- For detailed information about a specific \"julia\" package (including how to load the modules) use the module's full name. Challenge 1c. Which method to trust? Shall one trust the documentation or the commandline on the cluster more? Solution Looking for modules in a session on the cluster is closer to the truth Challenge 2. Try to start julia without having loaded julia module If you have a julia module loaded already, you may unload it with the unload command. Tip: Type: unload julia and press <tab> until the full module name is shown, then press <enter> . (If the Julia module starts with an uppercase, use that instead!) Solution $ julia It doesn\u2019t work! The Julia interpreter is not found. Challenge 3. Load and start julia the right way from the command line Solution UPPMAX HPC2N LUNARC NSC PDC Rackham/Bianca Go back and check which Julia modules were available. To load version 1.8.5, do: $ module load julia/1.8.5 Note: Lowercase j . For short, you can also use: $ ml julia/1.8.5 Pelle Go back and check which Julia modules were available. To load version 1.10.9, do: $ module load Julia/1.10.9-LTS-linux-x86_64 Note: Uppercase j . For short, you can also use: $ ml Julia/1.10.9-LTS-linux-x86_64 $ module load Julia/1.8.5-linux-x86_64 Note: Uppercase ``J``. For short, you can also use: ```console $ ml Julia/1.8.5-linux-x86_64 $ module load Julia/1.8.5-linux-x86_64 Note: Uppercase J . For short, you can also use: $ ml Julia/1.8.5-linux-x86_64 $ module load julia/1.10.2-bdist Note: lowercase ``j``. For short, you can also use: ```console $ ml julia/1.10.2-bdist $ module load PDC/23.12 julia/1.10.2-cpeGNU-23.12 Note: lowercase j . For short, you can also use: $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 Challenge 4. Getting familiar with Julia REPL It is important that you know how to navigate on the Julia command line. Here is where you work live with data and test aout things and you may install packages. This exercise will help you to become more familiar with the REPL. Do the following steps: Start a Julia session. In the Julian mode, compute the sum the numbers 5 and 6 Change to the shell mode and display the current directory Now, go to the package mode and list the currently installed packages Finally, display help information of the function println in help mode. Solution $ julia julia > 5 + 6 julia > ; shell > pwd julia > ] pkg > status julia >? help ?> println Challenge 5. Load another module and run a script Load the latest version Run the following serial script ( serial-sum.jl ) which accepts two integer arguments as input: x = parse ( Int32 , ARGS [ 1 ] ) y = parse ( Int32 , ARGS [ 2 ] ) summ = x + y println ( \"The sum of the two numbers is \" , summ ) Enter two numbers, like 2 & 3. Solution for HPC2N $ ml purge > /dev/null 2 > & 1 # recommended purge $ ml Julia/1.8.5-linux-x86_64 # Julia module $ julia serial-sum.jl Arg1 Arg2 # run the serial script Solution for UPPMAX Rackham/Bianca $ ml julia/1.8.5 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Pelle $ ml Julia/1.10.9-LTS-linux-x86_64 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Solution for LUNARC $ ml Julia/1.8.5-linux-x86_64 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Solution for NSC $ ml julia/1.10.2-bdist # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Solution for PDC $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Challenge 6. Check your understanding Check your understanding and answer in the shared document Can you start Julia without loading a Julia module? Yes? No? Which character to use to toggle to the package mode? back to the Julia mode? to the help mode? to the shell mode? Solution to the package mode? ] back to the Julia mode? <backspace> to the help mode? ? to the shell mode? ; Summary Before you can run Julia scripts or work in a Julia shell, first load a Julia module with module load <julia module> Start a Julia shell session with julia It offers several modes that can make your workflow easier, i.e. Julian shell package manager help Run scripts with julia <script.jl>","title":"Load and run"},{"location":"julia/load_run/#load-and-run-julia","text":"Info At the Swedish HPC centers we call the applications available via the module system modules : NSC PDC UPPMAX HPC2N LUNARC Objectives Learn to load Julia Get started with the Julia command line Learn to run Julia scripts Instructor note Lecture and demo 15 min Exercise 15 min Total time 30 min Julia can be started after a Julia module is loaded. The module activates paths to a specific version of the julia interpreter and its libraries and packages. Short cheat sheet See which modules exists: module spider or ml spider . Find module versions for a particular software: module spider <software> Modules depending only on what is currently loaded: module avail or ml av See which modules are currently loaded: module list or ml Load a module: module load <module>/<version> or ml <module>/<version> Unload a module: module unload <module>/<version> or ml -<module>/<version> Unload all modules except the \u2018sticky\u2019 modules: module purge or ml purge Warning Note that the module systems at UPPMAX and HPC2N are slightly different. All modules at UPPMAX, for instance, not directly related to bio-informatics are shown by ml avail . Modules at many other centres are only available when one has loaded all prerequisites, for instance the compilers ( GNU , Intel , etc.).","title":"Load and run Julia"},{"location":"julia/load_run/#check-for-julia-versions","text":"Long-term support versions So far Julia has Long-term support (LTS) for 1.6.7 1.10.X (high X is better) It might good to try to stick with such a version for future compatibility and support.","title":"Check for Julia versions"},{"location":"julia/load_run/#principle","text":"For some clusters module avail julia Example output From Tetralith: $ module avail julia ---------------------------------- /software/sse2/tetralith_el9/modules ----------------------------------- julia/recommendation (D) julia/1.6.1-nsc1-bdist julia/1.9.4-bdist julia/1.1.0-nsc1-gcc-2018a-eb julia/1.7.2-nsc1-bdist julia/1.10.2-bdist julia/1.4.1 julia/1.8.5-nsc1-bdist Where: D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Or, at clusters that hides the modules until relevant dependencies are loaded (HPC2N & PDC) module spider julia Example output From Dardel: $ module spider julia ------------------------------------------------------------------------------------------------------- julia: ------------------------------------------------------------------------------------------------------- Description: Julia is a high-level general-purpose dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science, without the typical need of separate compilation to be fast, also usable for client and server web use, low-level systems programming or as a specification language (wikipedia.org). Julia provides ease and expressiveness for high-level numerical computing, in the same way as languages such as R, MATLAB, and Python, but also supports general programming. To achieve this, Julia builds upon the lineage of mathematical programming languages, but also borrows much from popular dynamic languages, including Lisp, Perl, Python, Lua, and Ruby (julialang.org). Versions: julia/1.8.2-cpeGNU-22.06 julia/1.9.3-cpeGNU-22.06 julia/1.9.3-cpeGNU-23.03 julia/1.10.2-cpeGNU-23.03 julia/1.10.2-cpeGNU-23.12 Other possible modules matches: Julia libuv-julia ------------------------------------------------------------------------------------------------------- To find other possible module matches execute: $ module -r spider '.*julia.*' ------------------------------------------------------------------------------------------------------- For detailed information about a specific \"julia\" package (including how to load the modules) use the module's full name. Note that names that have a trailing (E) are extensions provided by other modules. For example: $ module spider julia/1.10.2-cpeGNU-23.12 -------------------------------------------------------------------------------------------------------","title":"Principle"},{"location":"julia/load_run/#load-a-julia-module","text":"For reproducibility, we recommend ALWAYS loading a specific module for the Julia version instead of using the default one.","title":"Load a Julia module"},{"location":"julia/load_run/#run","text":"","title":"Run"},{"location":"julia/load_run/#run-julia-as-a-session","text":"After loading the appropriate modules for Julia, you will have access to the read-eval-print-loop (REPL) command line by typing julia : julia In julia REPL Example This is what loading the Julia REPL looks like on Pelle: [username@pelle1 ~]$ ml No modules loaded [username@pelle1 ~]$ ml Julia/1.10.9 [username@pelle1 ~]$ ml Currently Loaded Modules: 1) Julia/1.10.9-LTS-linux-x86_64 [username@pelle1 ~]$ julia _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type \"?\" for help, \"]?\" for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.10.9 (2025-03-10) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia>","title":"Run Julia as a session"},{"location":"julia/load_run/#modes-julian-mode","text":"Julia has different modes, the one we arrive at is the so-called Julian mode, where one can execute commands. The description for accessing these modes will be given in the following paragraphs. Once you are done with your work in any of the modes, you can return to the Julian mode by pressing the backspace key.","title":"Modes: Julian mode"},{"location":"julia/load_run/#shell-mode","text":"While being on the Julian mode you can enter the shell mode by typing ; : julia>; shell>pwd /current-folder-path this will allow you to use Linux commands. Notice that the availability of these commands depend on the OS, for instance, on Windows it will depend on the terminal that you have installed and if it is visible to the Julia installation.","title":"Shell mode"},{"location":"julia/load_run/#package-manager-mode","text":"Another mode available in Julia is the package manager mode, it can be accessed by typing ] in the Julian mode: julia>] (v1.8) pkg> this will make your interaction with the package manager Pkg easier, for instance, instead of typing the complete name of Pkg commands such as Pkg.status() in the Julian mode, you can just type status in the package mode.","title":"Package manager mode"},{"location":"julia/load_run/#help-mode","text":"The last mode is the help mode, you can enter this mode from the Julian one by typing ? , then you may type some string from which you need more information: julia >? help ?> ans search : ans transpose transcode contains expanduser instances MathConstants readlines LinearIndices leading_ones leading_zeros ans A variable referring to the last computed value , automatically set at the interactive prompt .","title":"Help mode"},{"location":"julia/load_run/#exiting","text":"Exit with julia> <Ctrl-D> or julia> exit() The Julian modes summary enter the shell mode by typing ; go back to Julian mode by <backspace> access the package manager mode by typing ] in the Julian mode use the help mode by typing ? in the Julian mode See also More detailed information about the modes in Julia can be found .","title":"Exiting"},{"location":"julia/load_run/#run-a-julia-script","text":"You can run a Julia script on the Linux shell as follows: $ julia example.jl where the script is a text file could contain these lines: println ( \"hello world\" )","title":"Run a Julia script"},{"location":"julia/load_run/#exercises","text":"Challenge 1a. Find out which versions are on your cluster from documentation Find/search for that documentation! Solutions UPPMAX HPC2N LUNARC NSC PDC Challenge 1b. Find out which versions are on your cluster from command line Use the spider or avail module commands Solutions UPPMAX HPC2N LUNARC Tetralith Dardel Check all available Julia versions with: $ module avail julia Check all available version Julia versions with: $ module spider julia Notice that the output if you are working on the Intel ( kebnekaise.hpc2n.umu.se ) or AMD ( kebnekaise-amd.hpc2n.umu.se ) login nodes is different. In the former, you will see more installed versions of Julia as this hardware is older. To see how to load a specific version of Julia, including the prerequisites, do $ module spider Julia/<version> Example for Julia 1.8.5 $ module spider Julia/1.8.5-linux-x86_64 Check all available version Julia versions with: $ module spider Julia To see how to load a specific version of Julia, including the prerequisites, do $ module spider Julia/<version> Example for Julia 1.8.5 $ module spider Julia/1.8.5-linux-x86_64 Check all available version Julia versions with: $ module avail Julia Example for Julia 1.8.5 $ module spider julia/1.8.5-nsc1-bdist Check all available version Julia versions with: $ module spider Julia To see how to load a specific version of Julia, including the prerequisites, do $ module spider Julia/<version> Example for Julia 1.8.5 $ module spider Julia/1.8.5-linux-x86_64 Output at UPPMAX as of Oct 2025 Rackham/(Bianca) $ module avail julia ----------------------------- /sw/mf/rackham/compilers ----------------------------- julia/1.0.5_LTS julia/1.6.1 julia/1.7.2 julia/1.9.3 julia/1.1.1 julia/1.6.3 julia/1.8.5 julia/1.10.10_LTS julia/1.4.2 julia/1.6.7_LTS julia/1.9.1 julia/1.11.6 (D) Where: D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Pelle $ ml av Julia ---------------------------------------------------- /sw/arch/eb/modules/all -------------------- Julia/1.10.9-LTS-linux-x86_64 Julia/1.11.3-linux-x86_64 (D) Where: D: Default Module If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Output at HPC2N as of Oct 2025 $ module spider julia # Assuming you are working on the Intel login nodes ------------------------------------------------------------------------------------------------ Julia: ------------------------------------------------------------------------------------------------ Description: Julia is a high-level, high-performance dynamic programming language for numerical computing Versions: Julia/1.5.3-linux-x86_64 Julia/1.7.1-linux-x86_64 Julia/1.8.5-linux-x86_64 Julia/1.9.3-linux-x86_64 ------------------------------------------------------------------------------------------------ For detailed information about a specific \"Julia\" package (including how to load the modules) use the module's full name. Note that names that have a trailing (E) are extensions provided by other modules. For example: $ module spider Julia/1.8.5-linux-x86_64 ------------------------------------------------------------------------------------------------ Output at LUNARC as of Oct 2025 $ module spider julia ----------------------------------------------------------------------------------------------------- Julia: ----------------------------------------------------------------------------------------------------- Description: Julia is a high-level, high-performance dynamic programming language for numerical computing Versions: Julia/1.8.5-linux-x86_64 Julia/1.9.0-linux-x86_64 Julia/1.9.2-linux-x86_64 Julia/1.9.3-linux-x86_64 Julia/1.10.4-linux-x86_64 Output at NSC as of Mar 2025 $ module avail julia ---------------------------------- /software/sse2/tetralith_el9/modules ----------------------------------- julia/recommendation (D) julia/1.6.1-nsc1-bdist julia/1.9.4-bdist julia/1.1.0-nsc1-gcc-2018a-eb julia/1.7.2-nsc1-bdist julia/1.10.2-bdist julia/1.4.1 julia/1.8.5-nsc1-bdist Output at PDC as of Oct 2025 $ module spider julia ------------------------------------------------------------------------------------------------------- julia: ------------------------------------------------------------------------------------------------------- Description: Julia is a high-level general-purpose dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science, without the typical need of separate compilation to be fast, also usable for client and server web use, low-level systems programming or as a specification language (wikipedia.org). Julia provides ease and expressiveness for high-level numerical computing, in the same way as languages such as R, MATLAB, and Python, but also supports general programming. To achieve this, Julia builds upon the lineage of mathematical programming languages, but also borrows much from popular dynamic languages, including Lisp, Perl, Python, Lua, and Ruby (julialang.org). Versions: julia/1.8.2-cpeGNU-22.06 julia/1.9.3-cpeGNU-22.06 julia/1.9.3-cpeGNU-23.03 julia/1.10.2-cpeGNU-23.03 julia/1.10.2-cpeGNU-23.12 julia/1.11.4-cpeAMD-24.11 Other possible modules matches: Julia libuv-julia ------------------------------------------------------------------------------------------------------- To find other possible module matches execute: $ module -r spider '.*julia.*' ------------------------------------------------------------------------------------------------------- For detailed information about a specific \"julia\" package (including how to load the modules) use the module's full name. Challenge 1c. Which method to trust? Shall one trust the documentation or the commandline on the cluster more? Solution Looking for modules in a session on the cluster is closer to the truth Challenge 2. Try to start julia without having loaded julia module If you have a julia module loaded already, you may unload it with the unload command. Tip: Type: unload julia and press <tab> until the full module name is shown, then press <enter> . (If the Julia module starts with an uppercase, use that instead!) Solution $ julia It doesn\u2019t work! The Julia interpreter is not found. Challenge 3. Load and start julia the right way from the command line Solution UPPMAX HPC2N LUNARC NSC PDC Rackham/Bianca Go back and check which Julia modules were available. To load version 1.8.5, do: $ module load julia/1.8.5 Note: Lowercase j . For short, you can also use: $ ml julia/1.8.5 Pelle Go back and check which Julia modules were available. To load version 1.10.9, do: $ module load Julia/1.10.9-LTS-linux-x86_64 Note: Uppercase j . For short, you can also use: $ ml Julia/1.10.9-LTS-linux-x86_64 $ module load Julia/1.8.5-linux-x86_64 Note: Uppercase ``J``. For short, you can also use: ```console $ ml Julia/1.8.5-linux-x86_64 $ module load Julia/1.8.5-linux-x86_64 Note: Uppercase J . For short, you can also use: $ ml Julia/1.8.5-linux-x86_64 $ module load julia/1.10.2-bdist Note: lowercase ``j``. For short, you can also use: ```console $ ml julia/1.10.2-bdist $ module load PDC/23.12 julia/1.10.2-cpeGNU-23.12 Note: lowercase j . For short, you can also use: $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 Challenge 4. Getting familiar with Julia REPL It is important that you know how to navigate on the Julia command line. Here is where you work live with data and test aout things and you may install packages. This exercise will help you to become more familiar with the REPL. Do the following steps: Start a Julia session. In the Julian mode, compute the sum the numbers 5 and 6 Change to the shell mode and display the current directory Now, go to the package mode and list the currently installed packages Finally, display help information of the function println in help mode. Solution $ julia julia > 5 + 6 julia > ; shell > pwd julia > ] pkg > status julia >? help ?> println Challenge 5. Load another module and run a script Load the latest version Run the following serial script ( serial-sum.jl ) which accepts two integer arguments as input: x = parse ( Int32 , ARGS [ 1 ] ) y = parse ( Int32 , ARGS [ 2 ] ) summ = x + y println ( \"The sum of the two numbers is \" , summ ) Enter two numbers, like 2 & 3. Solution for HPC2N $ ml purge > /dev/null 2 > & 1 # recommended purge $ ml Julia/1.8.5-linux-x86_64 # Julia module $ julia serial-sum.jl Arg1 Arg2 # run the serial script Solution for UPPMAX Rackham/Bianca $ ml julia/1.8.5 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Pelle $ ml Julia/1.10.9-LTS-linux-x86_64 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Solution for LUNARC $ ml Julia/1.8.5-linux-x86_64 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Solution for NSC $ ml julia/1.10.2-bdist # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Solution for PDC $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 # Julia module julia serial-sum.jl Arg1 Arg2 # run the serial script Challenge 6. Check your understanding Check your understanding and answer in the shared document Can you start Julia without loading a Julia module? Yes? No? Which character to use to toggle to the package mode? back to the Julia mode? to the help mode? to the shell mode? Solution to the package mode? ] back to the Julia mode? <backspace> to the help mode? ? to the shell mode? ; Summary Before you can run Julia scripts or work in a Julia shell, first load a Julia module with module load <julia module> Start a Julia shell session with julia It offers several modes that can make your workflow easier, i.e. Julian shell package manager help Run scripts with julia <script.jl>","title":"Exercises"},{"location":"julia/notebook_jupyter/","text":"Running Julia in Jupyter \u00b6 Like for Python it is possible to run Julia in Jupyter, i.e. in a web interface with possibility of inline figures and debugging. For this you need the IJulia package that you may have to install yourself. An easy way to do this is to load a python or Jupyter module as well. For more interactiveness you can run IJulia which is Julia in Jupyter. You benefit a lot if you are using ThinLinc Installation step This was done in Exercise 2 in the Managing environments and packages section It may take 5-10 minutes or so. Use ThinLinc and interactive session Use thinLinc Allocate resources like above with salloc/interactive When these steps are ready continue below! Start Jupyter NSC PDC UPPMAX HPC2N & LUNARC $ ml Python/3.11.5-env-hpc1-gcc-2023b-eb $ ml julia/1.10.2-bdist $ julia -p 4 Note: not fully tested successfully, but this step works $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 $ ml cray-python/3.11.5 $ julia $ module load julia/1.8.5 $ module load python/3.9.5 $ julia -p 4 Like for Python it is possible to run a Julia in a Jupyter, i.e. in a web interface with possibility of inline figures and debugging. An easy way to do this is to load the JupyterLab and Julia modules. In shell: $ module load GCC/13.2.0 JupyterLab/4.2.0 $ module load Julia/1.8.5-linux-x86_64 $ julia In Julia package mode: ( @v1 .11 ) pkg > activate jupyter - env ( jupyter - env ) julia > Pkg . build ( \"IJulia\" ) ( jupyter - env ) julia > using IJulia ( jupyter - env ) julia > notebook ( dir = \".\" , detached = true ) In some centres (UPPMAX and NSC) this will start a Firefox session with the Jupyter notebook interface. The last command may not be able to start notebook, see further down how to do. If not, see below. In any case, IJulia is now installed! Starting a Jupyter session with Julia Kernel You can start up Julia in Jupyter quickly, once IJulia is installed for the combinations of Julia and Python/Jupyter you want to use. There are two ways starting from within julia REPL (not for HPC2n or PDC) starting jupyter session from terminal From Julia REPL This may not always work julia > using IJulia julia > notebook ( dir = \".\" , detached = true ) Tip With notebook(dir=\"</path/to/work/dir/>\", detached=true) the notebook will not be killed when you exit your REPL Julia session in the terminal. Jupyter session from terminal Principle Load Julia module (and prerequisites) Load Python or Jupyter module (and prerequisites) that is compatible with the python version used when building IJulia in the previous step Running IJulia in Jupyter on compute nodes Jupyter is rather slow graphically on the compute nodes. This can be fixed by starting the jupyter server on the compute node, copying the URL containing the. then starting a web browser in ThinLinc and open the URL copied in previous step One can even use the home computer, see here NSC PDC UPPMAX LUNARC HPC2N First start an interactive session $ ml Python/3.11.5-env-hpc1-gcc-2023b-eb $ ml julia/1.10.2-bdist $ jupyter-lab --ip = $HOSTNAME Start the browser from the ThinLinc menu. Copy-paste the address line containing the node name from the jupyter output You can start the Julia kernel in the upper right corner! Note: not fully tested successfully. Since Jupyter and a web browser are missing on the compute nodes, we need to find another solution here. Below are the steps that would be nice if we got working! First start an interactive session $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 cray-python/3.11.5 $ jupyter-lab --ip = $HOSTNAME Start the browser from the ThinLinc menu. Copy-paste the address line containing the node name from the jupyter output You can start the Julia kernel in the upper right corner! $ module load julia/1.8.5 $ module load python/3.9.5 $ jupyter-lab --ip = $HOSTNAME Start the browser from the ThinLinc menu. Copy-paste the address line containing the node name from the jupyter output You can start the Julia kernel in the upper right corner! $ module load GCCcore/13.2.0 JupyterLab/4.2.0 $ module load Julia/1.8.5-linux-x86_64 $ jupyter-lab --ip = $HOSTNAME Start the browser from the ThinLinc menu. Copy-paste the address line containing the node name from the jupyter output You can start the Julia kernel in the upper right corner! Write a bash script similar to this (call it job_jupyter.sh , for instance): #!/bin/bash # Here you should put your own project id #SBATCH -A hpc2n2025-062 # This example asks for 1 core #SBATCH -n 1 # Ask for a suitable amount of time. Remember, this is the time the Jupyter notebook will be available! HHH:MM:SS. #SBATCH --time=00:10:00 # Clear the environment from any previously loaded modules module purge > /dev/null 2 > & 1 # Load the module environment suitable for the job module load GCCcore/13.2.0 JupyterLab/4.2.0 # Load the Julia module ml Julia/1.8.5-linux-x86_64 # Start JupyterLab jupyter lab --no-browser --ip $( hostname ) Then, in the output file slurm- .out file, copy the url that starts with http://b-cn1403.hpc2n.umu.se:8888/lab and paste it in a Firefox browser on Kebnekaise. When the Jupyter notebook interface starts, you can choose the Julia version from the module you loaded (in this case 1.8.5). On Kebnekaise, you can run Jupyter notebooks with Julia kernels by using batch scripts See HPC2N documentation on using Jupyter Lab with Julia","title":"Running Julia in Jupyter"},{"location":"julia/notebook_jupyter/#running-julia-in-jupyter","text":"Like for Python it is possible to run Julia in Jupyter, i.e. in a web interface with possibility of inline figures and debugging. For this you need the IJulia package that you may have to install yourself. An easy way to do this is to load a python or Jupyter module as well. For more interactiveness you can run IJulia which is Julia in Jupyter. You benefit a lot if you are using ThinLinc Installation step This was done in Exercise 2 in the Managing environments and packages section It may take 5-10 minutes or so. Use ThinLinc and interactive session Use thinLinc Allocate resources like above with salloc/interactive When these steps are ready continue below! Start Jupyter NSC PDC UPPMAX HPC2N & LUNARC $ ml Python/3.11.5-env-hpc1-gcc-2023b-eb $ ml julia/1.10.2-bdist $ julia -p 4 Note: not fully tested successfully, but this step works $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 $ ml cray-python/3.11.5 $ julia $ module load julia/1.8.5 $ module load python/3.9.5 $ julia -p 4 Like for Python it is possible to run a Julia in a Jupyter, i.e. in a web interface with possibility of inline figures and debugging. An easy way to do this is to load the JupyterLab and Julia modules. In shell: $ module load GCC/13.2.0 JupyterLab/4.2.0 $ module load Julia/1.8.5-linux-x86_64 $ julia In Julia package mode: ( @v1 .11 ) pkg > activate jupyter - env ( jupyter - env ) julia > Pkg . build ( \"IJulia\" ) ( jupyter - env ) julia > using IJulia ( jupyter - env ) julia > notebook ( dir = \".\" , detached = true ) In some centres (UPPMAX and NSC) this will start a Firefox session with the Jupyter notebook interface. The last command may not be able to start notebook, see further down how to do. If not, see below. In any case, IJulia is now installed! Starting a Jupyter session with Julia Kernel You can start up Julia in Jupyter quickly, once IJulia is installed for the combinations of Julia and Python/Jupyter you want to use. There are two ways starting from within julia REPL (not for HPC2n or PDC) starting jupyter session from terminal From Julia REPL This may not always work julia > using IJulia julia > notebook ( dir = \".\" , detached = true ) Tip With notebook(dir=\"</path/to/work/dir/>\", detached=true) the notebook will not be killed when you exit your REPL Julia session in the terminal. Jupyter session from terminal Principle Load Julia module (and prerequisites) Load Python or Jupyter module (and prerequisites) that is compatible with the python version used when building IJulia in the previous step Running IJulia in Jupyter on compute nodes Jupyter is rather slow graphically on the compute nodes. This can be fixed by starting the jupyter server on the compute node, copying the URL containing the. then starting a web browser in ThinLinc and open the URL copied in previous step One can even use the home computer, see here NSC PDC UPPMAX LUNARC HPC2N First start an interactive session $ ml Python/3.11.5-env-hpc1-gcc-2023b-eb $ ml julia/1.10.2-bdist $ jupyter-lab --ip = $HOSTNAME Start the browser from the ThinLinc menu. Copy-paste the address line containing the node name from the jupyter output You can start the Julia kernel in the upper right corner! Note: not fully tested successfully. Since Jupyter and a web browser are missing on the compute nodes, we need to find another solution here. Below are the steps that would be nice if we got working! First start an interactive session $ ml PDC/23.12 julia/1.10.2-cpeGNU-23.12 cray-python/3.11.5 $ jupyter-lab --ip = $HOSTNAME Start the browser from the ThinLinc menu. Copy-paste the address line containing the node name from the jupyter output You can start the Julia kernel in the upper right corner! $ module load julia/1.8.5 $ module load python/3.9.5 $ jupyter-lab --ip = $HOSTNAME Start the browser from the ThinLinc menu. Copy-paste the address line containing the node name from the jupyter output You can start the Julia kernel in the upper right corner! $ module load GCCcore/13.2.0 JupyterLab/4.2.0 $ module load Julia/1.8.5-linux-x86_64 $ jupyter-lab --ip = $HOSTNAME Start the browser from the ThinLinc menu. Copy-paste the address line containing the node name from the jupyter output You can start the Julia kernel in the upper right corner! Write a bash script similar to this (call it job_jupyter.sh , for instance): #!/bin/bash # Here you should put your own project id #SBATCH -A hpc2n2025-062 # This example asks for 1 core #SBATCH -n 1 # Ask for a suitable amount of time. Remember, this is the time the Jupyter notebook will be available! HHH:MM:SS. #SBATCH --time=00:10:00 # Clear the environment from any previously loaded modules module purge > /dev/null 2 > & 1 # Load the module environment suitable for the job module load GCCcore/13.2.0 JupyterLab/4.2.0 # Load the Julia module ml Julia/1.8.5-linux-x86_64 # Start JupyterLab jupyter lab --no-browser --ip $( hostname ) Then, in the output file slurm- .out file, copy the url that starts with http://b-cn1403.hpc2n.umu.se:8888/lab and paste it in a Firefox browser on Kebnekaise. When the Jupyter notebook interface starts, you can choose the Julia version from the module you loaded (in this case 1.8.5). On Kebnekaise, you can run Jupyter notebooks with Julia kernels by using batch scripts See HPC2N documentation on using Jupyter Lab with Julia","title":"Running Julia in Jupyter"},{"location":"julia/notebook_pluto/","text":"A Julia notebook: Pluto \u00b6 Activate the Pluto environment we installed earlier \u00b6 (@v1.11) pkg> activate pluto-env julia> using Pluto Start Pluto \u00b6 First try: \u00b6 > Pluto.run() At some places it actually starts a web browser Otherwise, do \u00b6 julia> Pluto . run (; launch_browser = false ) and note the url given and paste it into a browser opened from the menu of ThinLinc. Otherwise, do \u00b6 julia> Pluto . run (; host = \"0.0.0.0\" ) Note the url given and paste it into a browser opened from the menu of ThinLinc. also note the node you got when starting the interactive session (like n701 or similar on Tetralith). change 0.0.0.0 to that hostname Example for Tetralith to be in web browser url: http://n701:1234/?secret=mAzEDw9R How does it look like? \u00b6 Welcome page Pluto cells","title":"A Julia notebook: Pluto"},{"location":"julia/notebook_pluto/#a-julia-notebook-pluto","text":"","title":"A Julia notebook: Pluto"},{"location":"julia/notebook_pluto/#activate-the-pluto-environment-we-installed-earlier","text":"(@v1.11) pkg> activate pluto-env julia> using Pluto","title":"Activate the Pluto environment we installed earlier"},{"location":"julia/notebook_pluto/#start-pluto","text":"","title":"Start Pluto"},{"location":"julia/notebook_pluto/#first-try","text":"> Pluto.run() At some places it actually starts a web browser","title":"First try:"},{"location":"julia/notebook_pluto/#otherwise-do","text":"julia> Pluto . run (; launch_browser = false ) and note the url given and paste it into a browser opened from the menu of ThinLinc.","title":"Otherwise, do"},{"location":"julia/notebook_pluto/#otherwise-do_1","text":"julia> Pluto . run (; host = \"0.0.0.0\" ) Note the url given and paste it into a browser opened from the menu of ThinLinc. also note the node you got when starting the interactive session (like n701 or similar on Tetralith). change 0.0.0.0 to that hostname Example for Tetralith to be in web browser url: http://n701:1234/?secret=mAzEDw9R","title":"Otherwise, do"},{"location":"julia/notebook_pluto/#how-does-it-look-like","text":"Welcome page Pluto cells","title":"How does it look like?"},{"location":"julia/schedule/","text":"Julia schedule \u00b6 Time Topic Teacher 9:00 Log in BC, LS, PO 9.45 Coffee break . 10:00 Syllabus BC 10.10 Introduction, Julia in general LS 10.25 Loading modules and running Julia codes BC 10.55 Break . 11.10 Packages and environments LS 12:00 LUNCH . 13:00 SLURM Batch scripts for Julia jobs PO 13.50 Break . 14:05 IDEs and using compute nodes interactively BC 14.55 Break 15:10 Summary ?? 15:20 Evaluation . 15:35 Q&A on-demand . 16:00 END","title":"Schedule"},{"location":"julia/schedule/#julia-schedule","text":"Time Topic Teacher 9:00 Log in BC, LS, PO 9.45 Coffee break . 10:00 Syllabus BC 10.10 Introduction, Julia in general LS 10.25 Loading modules and running Julia codes BC 10.55 Break . 11.10 Packages and environments LS 12:00 LUNCH . 13:00 SLURM Batch scripts for Julia jobs PO 13.50 Break . 14:05 IDEs and using compute nodes interactively BC 14.55 Break 15:10 Summary ?? 15:20 Evaluation . 15:35 Q&A on-demand . 16:00 END","title":"Julia schedule"},{"location":"julia/summary/","text":"Summary \u00b6 Keypoints Load and run \u00b6 Use Julia from module system Start a Julia shell session either with julia run scripts with julia <script.jl> Packages \u00b6 Check for preinstalled packages from the Julia REPL with the using or import command Isolated environments \u00b6 With a virtual environment you can tailor an environment with specific versions for Julia and packages, not interfering with other installed Julia versions and packages. Make it for each project you have for reproducibility. Julia has its own mechanism to create virtual environments. HPC2N & LUNARC The Julia installation is lean, no additional package besides the Base and and Standard Libraries are installed. UPPMAX Several packages are already installed Batch mode \u00b6 The SLURM scheduler handles allocations to the calculation nodes Batch jobs runs without interaction with user A batch script consists of a part with SLURM parameters describing the allocation and a second part describing the actual work within the job, for instance one or several Julia scripts. Remember to include possible input arguments to the Julia script in the batch script. Interactive work on calculation nodes \u00b6 Start an interactive session on a calculation node by a SLURM allocation (similar flags) At HPC2N: salloc \u2026 At UPPMAX/LUNARC: interactive \u2026 Follow the same procedure as usual by loading the Julia module and possible prerequisites. Run Julia in Jupyter lab/notebook Procedure is to use the IJulia package and start a jupyter notebook from the julia command line. Parallel \u00b6 You deploy cores and nodes via SLURM, either in interactive mode or batch In Julia, threads, distributed and MPI parallelization can be used. GPUs \u00b6 You deploy GPU nodes via SLURM, either in interactive mode or batch In Julia the CUDA package is handy Not really clear? (5 min) Discuss in breakout rooms Learn from each other Evaluation You can find the evaluation form for the Julia part of the course here: TODO:link to eval See also Documentation at the HPC centres UPPMAX and HPC2N UPPMAX: https://docs.uppmax.uu.se/software/julia/ HPC2N: https://www.hpc2n.umu.se/resources/software/julia Official Julia documentation is found here: https://docs.julialang.org/en/v1/ Slack channel for Julia and instructions for joining it are found here: https://julialang.org/slack/ HPC2N YouTube video on Julia in HPC: https://www.youtube.com/watch?v=bXHe7Kj3Xxg Julia for High Performance Computing course material from ENCCS: https://enccs.github.io/julia-for-hpc/ Final words Julia language becomes increasingly popular. We also have a web pages for Julia: UPPMAX https://docs.uppmax.uu.se/software/julia/ HPC2N https://www.hpc2n.umu.se/resources/software/julia","title":"Summary"},{"location":"julia/summary/#summary","text":"Keypoints","title":"Summary"},{"location":"julia/not_used/ideas-environments/","text":"Ideas for future teaching of environments \u00b6 Environments first \u00b6 Like, project environments first, then stacked environments, don\u2019t install a package without knowing where you install it and why you choose to install it there. Demonstrate a package getting updated \u00b6 To demonstrate the utility and functionality of version controlled environments. Have an example package with a good example bug Hello function printing without line ending Learners install this and write a loop with a workaround (TA installs this in the background) Publish fix Learners update, remove their workaround, use git to go back and forth TA demonstration: dev , fixing the bug looking at the update and finding it good go back to tracking normally Possibly tricky: there\u2019s a git repo within a git repo here, I\u2019m not sure how convoluted it is to keep full reproducibility when doing this.","title":"Ideas for future teaching of environments"},{"location":"julia/not_used/ideas-environments/#ideas-for-future-teaching-of-environments","text":"","title":"Ideas for future teaching of environments"},{"location":"julia/not_used/ideas-environments/#environments-first","text":"Like, project environments first, then stacked environments, don\u2019t install a package without knowing where you install it and why you choose to install it there.","title":"Environments first"},{"location":"julia/not_used/ideas-environments/#demonstrate-a-package-getting-updated","text":"To demonstrate the utility and functionality of version controlled environments. Have an example package with a good example bug Hello function printing without line ending Learners install this and write a loop with a workaround (TA installs this in the background) Publish fix Learners update, remove their workaround, use git to go back and forth TA demonstration: dev , fixing the bug looking at the update and finding it good go back to tracking normally Possibly tricky: there\u2019s a git repo within a git repo here, I\u2019m not sure how convoluted it is to keep full reproducibility when doing this.","title":"Demonstrate a package getting updated"},{"location":"julia/not_used/removed-section-stacked-environments/","text":"Environment stacks \u00b6 As we saw before, LOAD_PATH shows that environments can be stacked and we can place the environments we want in the path so that they are visible in our current environment. To illustrate this concept, let\u2019s create a second environment and first we can remove the content of LOAD_PATH (which path will be different for you): julia> empty!(LOAD_PATH) shell> pwd /path-to-my-project/$USER/julia shell> mkdir my-second-env shell> cd my-second-env pkg> activate . If we try to use the DFTK package we will see the error message: julia> using DFTK \u2502 Package DFTK not found, but a package named DFTK is available from a registry. \u2502 Install package? \u2502 (my-second-env) pkg> add DFTK \u2514 (y/n/o) [y]: n ERROR: ArgumentError: Package DFTK not found in current path. If you remember this package was installed in the first environment ( my-first-env ). In order to make this package available in our second environment we can push the corresponding folder\u2019s path to LOAD_PATH : julia> push!(LOAD_PATH, \"/path-to-my-project/$USER/julia/my-first-env/\") 1-element Vector{String}: \"/path-to-my-project/$USER/julia/my-first-env/\" julia> using DFTK and now the package can be loaded from the first environment without errors.","title":"Removed section stacked environments"},{"location":"julia/not_used/removed-section-stacked-environments/#environment-stacks","text":"As we saw before, LOAD_PATH shows that environments can be stacked and we can place the environments we want in the path so that they are visible in our current environment. To illustrate this concept, let\u2019s create a second environment and first we can remove the content of LOAD_PATH (which path will be different for you): julia> empty!(LOAD_PATH) shell> pwd /path-to-my-project/$USER/julia shell> mkdir my-second-env shell> cd my-second-env pkg> activate . If we try to use the DFTK package we will see the error message: julia> using DFTK \u2502 Package DFTK not found, but a package named DFTK is available from a registry. \u2502 Install package? \u2502 (my-second-env) pkg> add DFTK \u2514 (y/n/o) [y]: n ERROR: ArgumentError: Package DFTK not found in current path. If you remember this package was installed in the first environment ( my-first-env ). In order to make this package available in our second environment we can push the corresponding folder\u2019s path to LOAD_PATH : julia> push!(LOAD_PATH, \"/path-to-my-project/$USER/julia/my-first-env/\") 1-element Vector{String}: \"/path-to-my-project/$USER/julia/my-first-env/\" julia> using DFTK and now the package can be loaded from the first environment without errors.","title":"Environment stacks"},{"location":"lesson_plans/","text":"Lesson plans \u00b6 Iteration Date Language Lesson plans 3 2024-03-12 Python Lesson plan 3 2024-03-14 R Lesson plan 4 2024-10-22 Python Lesson plan . 2024-10-24 R Lesson plan 5 2025-03-24 R Lesson plan 6 2025-10-06 R Lesson plan . 2025-10-10 Advanced Lesson plan","title":"Lesson plans"},{"location":"lesson_plans/#lesson-plans","text":"Iteration Date Language Lesson plans 3 2024-03-12 Python Lesson plan 3 2024-03-14 R Lesson plan 4 2024-10-22 Python Lesson plan . 2024-10-24 R Lesson plan 5 2025-03-24 R Lesson plan 6 2025-10-06 R Lesson plan . 2025-10-10 Advanced Lesson plan","title":"Lesson plans"},{"location":"lesson_plans/20240312_richel/","text":"Lesson plan \u00b6 Teaching date: 2024-04-12 Course: Python Teacher: Richel Remote desktop websites: HPC2N: kebnekaise-tl.hpc2n.umu.se UPPMAX: rackham-gui.uppmax.uu.se Compute allocations: Rackham: naiss2024-22-107 Kebnekaise: hpc2n2024-025 Mine: uppmax2023-2-25 Storage space: Rackham: /proj/r-py-jl Kebnekaise: /proj/nobackup/hpc2n2024-025 Mine: /proj/staff Python (Tuesday 2024-03-12) \u00b6 Time Topic Teacher 9.00 Syllabus Birgitte 9.10 Python in general Birgitte 9.20 Load modules and run Birgitte 9.45 Break . 10:00 Packages (45\u2013>30) Richel 10.45 Break . 11.00 Isolated environments Richel 12.00 Lunch . 13.00 Batch Birgitte 13:20 GPU Birgitte 13.30 Kebnekaise: Jupyter Birgitte . Rackham: Interactive session and Jupyter Richel 13.45 Break . 14.00 Parallel and multithreaded functions Pedro 14.25 Conclusion & Q/A Birgitte 14.45 Evaluation . Progress \u00b6 Use Python 3.11.8 on UPPMAX [Done] Packages [Done] Virtual environments [Done] Rackham: Interactive session [Good enough] Jupyter Lesson plan notes \u00b6 10:00-10:45: Packages: Exercise 2 for HPC2N has untested TensorFlow things Put learners 11:00-11:15: Break 11:15-12:00: Isolated environments Exercise 2 is UPPMAX only Exercises 2.4 and 3.4 take 6 and 13 minutes 12:00-13:00: Lunch break 13:30-13:45: UPPMAX-only Interactive: only 1.1, 1.2, 1.3 and 1.5 Jupyter: only show and run Discussion \u00b6 Pre-requirements \u00b6 I feel the pre-requirements page, https://uppmax.github.io/R-matlab-julia-HPC/prereqs , is too extensive. I feel it should link to regular documentation and mostly show how to determine you fulfilled all pre-requirements. Interactive \u00b6 The session on starting an interactive session feels too fancy twice: create an interactive session with 1 node with more nodes <\u2014 feels beyond the teaching goals run 2 Python scripts, 1 of which is unsuitable for an interactive session with more nodes. I think, for 15 minutes, one can only do 1 node and no Python script, to achieve the teaching goals. Impressed by Birgitte \u00b6 I think it was impressive that Birgitte logs in into both clusters at the start. I want that too! Why ssh -Y \u00b6 Unrelated to the course, Birgitte does so. Loading Python \u00b6 If the session is about loading Python, maybe seeing module dependencies can removed. Also, don\u2019t care about python3? Also, don\u2019t care about IPython? Suggest to Arvid \u00b6 The Bianca portal is great. Could you do the same for Rackham? Not every user understands one needs to use 2FA now, and how Technical problems \u00b6 This was quite annoying. Due to this, I cannot see how much the learners have understood and progressed through the exercises. More time for sbatch \u00b6 There was no time for an exercise. I would have enjoy to be sure that the learners have been able to submit a job and see the results. More time for GPU \u00b6 There was no time for an exercise. More time for UPPMAX interactive and Jupyter \u00b6 There was no time for an exercise. Parallel programming \u00b6 I feel that making a script suitable (with **FIX** in it) for a parallel run is at the \u2018Synthesis\u2019 level of Blooms taxonomy. I feel that some levels lower, e.g. \u2018Apply\u2019 with a step-by-step guide would be more suitable.","title":"Lesson plan"},{"location":"lesson_plans/20240312_richel/#lesson-plan","text":"Teaching date: 2024-04-12 Course: Python Teacher: Richel Remote desktop websites: HPC2N: kebnekaise-tl.hpc2n.umu.se UPPMAX: rackham-gui.uppmax.uu.se Compute allocations: Rackham: naiss2024-22-107 Kebnekaise: hpc2n2024-025 Mine: uppmax2023-2-25 Storage space: Rackham: /proj/r-py-jl Kebnekaise: /proj/nobackup/hpc2n2024-025 Mine: /proj/staff","title":"Lesson plan"},{"location":"lesson_plans/20240312_richel/#python-tuesday-2024-03-12","text":"Time Topic Teacher 9.00 Syllabus Birgitte 9.10 Python in general Birgitte 9.20 Load modules and run Birgitte 9.45 Break . 10:00 Packages (45\u2013>30) Richel 10.45 Break . 11.00 Isolated environments Richel 12.00 Lunch . 13.00 Batch Birgitte 13:20 GPU Birgitte 13.30 Kebnekaise: Jupyter Birgitte . Rackham: Interactive session and Jupyter Richel 13.45 Break . 14.00 Parallel and multithreaded functions Pedro 14.25 Conclusion & Q/A Birgitte 14.45 Evaluation .","title":"Python (Tuesday 2024-03-12)"},{"location":"lesson_plans/20240312_richel/#progress","text":"Use Python 3.11.8 on UPPMAX [Done] Packages [Done] Virtual environments [Done] Rackham: Interactive session [Good enough] Jupyter","title":"Progress"},{"location":"lesson_plans/20240312_richel/#lesson-plan-notes","text":"10:00-10:45: Packages: Exercise 2 for HPC2N has untested TensorFlow things Put learners 11:00-11:15: Break 11:15-12:00: Isolated environments Exercise 2 is UPPMAX only Exercises 2.4 and 3.4 take 6 and 13 minutes 12:00-13:00: Lunch break 13:30-13:45: UPPMAX-only Interactive: only 1.1, 1.2, 1.3 and 1.5 Jupyter: only show and run","title":"Lesson plan notes"},{"location":"lesson_plans/20240312_richel/#discussion","text":"","title":"Discussion"},{"location":"lesson_plans/20240312_richel/#pre-requirements","text":"I feel the pre-requirements page, https://uppmax.github.io/R-matlab-julia-HPC/prereqs , is too extensive. I feel it should link to regular documentation and mostly show how to determine you fulfilled all pre-requirements.","title":"Pre-requirements"},{"location":"lesson_plans/20240312_richel/#interactive","text":"The session on starting an interactive session feels too fancy twice: create an interactive session with 1 node with more nodes <\u2014 feels beyond the teaching goals run 2 Python scripts, 1 of which is unsuitable for an interactive session with more nodes. I think, for 15 minutes, one can only do 1 node and no Python script, to achieve the teaching goals.","title":"Interactive"},{"location":"lesson_plans/20240312_richel/#impressed-by-birgitte","text":"I think it was impressive that Birgitte logs in into both clusters at the start. I want that too!","title":"Impressed by Birgitte"},{"location":"lesson_plans/20240312_richel/#why-ssh-y","text":"Unrelated to the course, Birgitte does so.","title":"Why ssh -Y"},{"location":"lesson_plans/20240312_richel/#loading-python","text":"If the session is about loading Python, maybe seeing module dependencies can removed. Also, don\u2019t care about python3? Also, don\u2019t care about IPython?","title":"Loading Python"},{"location":"lesson_plans/20240312_richel/#suggest-to-arvid","text":"The Bianca portal is great. Could you do the same for Rackham? Not every user understands one needs to use 2FA now, and how","title":"Suggest to Arvid"},{"location":"lesson_plans/20240312_richel/#technical-problems","text":"This was quite annoying. Due to this, I cannot see how much the learners have understood and progressed through the exercises.","title":"Technical problems"},{"location":"lesson_plans/20240312_richel/#more-time-for-sbatch","text":"There was no time for an exercise. I would have enjoy to be sure that the learners have been able to submit a job and see the results.","title":"More time for sbatch"},{"location":"lesson_plans/20240312_richel/#more-time-for-gpu","text":"There was no time for an exercise.","title":"More time for GPU"},{"location":"lesson_plans/20240312_richel/#more-time-for-uppmax-interactive-and-jupyter","text":"There was no time for an exercise.","title":"More time for UPPMAX interactive and Jupyter"},{"location":"lesson_plans/20240312_richel/#parallel-programming","text":"I feel that making a script suitable (with **FIX** in it) for a parallel run is at the \u2018Synthesis\u2019 level of Blooms taxonomy. I feel that some levels lower, e.g. \u2018Apply\u2019 with a step-by-step guide would be more suitable.","title":"Parallel programming"},{"location":"lesson_plans/20240314_richel/","text":"Lesson plan \u00b6 Teaching date: 2024-04-14 Course: R Teacher: Richel This is the current schedule. I will teach until 9:45 Time Topic Teacher 9:00 Syllabus Richel 9:10 R in general Richel 9:20 Load modules and run Richel 9:45 Break . The exercises are already in place at [URL] : great! There are no learning objectives in place, though, so I will add those and streamline the pages.","title":"Lesson plan"},{"location":"lesson_plans/20240314_richel/#lesson-plan","text":"Teaching date: 2024-04-14 Course: R Teacher: Richel This is the current schedule. I will teach until 9:45 Time Topic Teacher 9:00 Syllabus Richel 9:10 R in general Richel 9:20 Load modules and run Richel 9:45 Break . The exercises are already in place at [URL] : great! There are no learning objectives in place, though, so I will add those and streamline the pages.","title":"Lesson plan"},{"location":"lesson_plans/20241022_richel/","text":"Lesson plan Python by Richel \u00b6 Date: Tuesday 2024-10-22 Course: Python, part of R, Python, Julia, and Matlab in HPC Suggest new schedule \u00b6 Old schedule \u00b6 Time Topic Teacher 9.00 Syllabus Rebecca 9.10 Python in general Rebecca 9.20 Load modules and run Rebecca 9.45 Break . 10:00 Packages Richel 10.30 Break . 10.45 Isolated environments (ML, venv, conda ) Richel 11:45 Break or informal chat Richel 12.00 Lunch . 13.00 Batch Birgitte 13:20 GPU Birgitte 13.30 Kebnekaise: Jupyter Birgitte . Rackham: Interactive session and Jupyter Richel 13.45 Break . 14.00 Parallel and multithreaded functions Pedro 14.25 Conclusion & Q/A Birgitte 14.45 . . 15.00 END . Notes: 9:15-10:00 is first login 10:00-10:15 is first break Day ends at 16.00 interactive must get 15 more minutes to do 1 core + 2 core more time for sbatch and GPU Suggestion \u00b6 Current durations: Topic Current duration (mins) Syllabus 10 Python in general 10 Load modules and run 25 Packages 30 Isolated environments (ML, venv, conda ) 60 Batch 20 GPU 10 Kebnekaise: Jupyter 15 Rackham: Interactive session and Jupyter . (parallel with Kebnekaise) Parallel and multithreaded functions 25 Total 205 Session durations in time: Time Topic Duration (mins) 09:00-09:45 First login (45) 09:45-10:00 Break . 10:00-11:00 Session 1 60 11:00-11:15 Break . 11:15-12:00 Session 2 45 12:00-13:00 Lunch . 13:00-14:00 Session 3 60 14:00-14:15 Break . 14:15-15:00 Session 4 45 15:00-15:15 Break . 15:15-16:00 Session 5 45 Total 255 This means I can add 50 minutes. Here I distribute these from the notes: Topic Current duration (mins) Syllabus 10 Python in general 10 Load modules and run 25 + 5 Packages 30 Isolated environments (ML, venv, conda ) 60 Batch 20 + 10 GPU 10 + 10 Kebnekaise: Jupyter 15 + 15 Rackham: Interactive session and Jupyter . (parallel with Kebnekaise) Parallel and multithreaded functions 25 + 10 Total 205 + 50 = 255 Now fitting this into this decided schedule: Time Topic Teacher 09:00-09:45 First login BB + PO+ RB + RP 09:45-10:00 Break . 10:00-10:10 Syllabus BB, RB, RP 10:10-10:20 Python in general BB, RB RP 10:20-10:50 Load modules and run BB, RB RP 10:50-11:05 Break . 11:05-11:35 Packages 30/30 RB 11:35-12:00 Isolated environments (ML, venv, conda) 25/60 mins RB 12:00-13:00 Lunch . 13:00-13:35 Isolated environments (ML, venv, conda) 35/60 mins RB 13:35-14:05 Batch 30/30 mins BB 14:05-14:20 Break . 14:20-14:40 GPU BB 14:40-15:10 Simultaneous session 30/30 mins BB RB RP 15:10-15:25 Break . 15:25-16:00 Parallel and multithreaded functions PO 16:00-16:15 Summary and evaluation RB Simultaneous session: Kebnekaise: Jupyter, by BB Rackham: Interactive session and Jupyter, by RB LUNARC, by RP Discussion points: For this schedules, breaks were scheduled first, so that (1) schedule is simple, and (2) all session lengths are 45 or 60 mins. However, assuming all durations are correct (!), some sessions will be cut by a break. Is that acceptable? If yes: no problemo No: where should the breaks be instead? Who makes a new schedule? I picked to discuss batch (by BB) from 13:35-14:05, over having a break at 14:00 and having a 5 minute session with a schedule like below. Agreed? Time Topic Teacher 13:35-14:00 Batch 25/30 mins BB 14:00-14:15 Break . 14:15-14:20 Batch 5/30 mins BB R: There is no time for evaluation at the end. I feel 10 minutes is enough and the time is worth it. I suggest to add it. Pro: helps us improve the course, shows we care about our learners\u2019 opinion Con: takes time away Vote: Yes: No: If evaluation, suggest questions on confidence, see this earlier evaluation Pro: useful to asses stronger/weaker sessions of the course Con: maybe must use a standard evaluation form, needs teachers to formulate their lesson goals Vote: Yes: No:","title":"Lesson plan Python by Richel"},{"location":"lesson_plans/20241022_richel/#lesson-plan-python-by-richel","text":"Date: Tuesday 2024-10-22 Course: Python, part of R, Python, Julia, and Matlab in HPC","title":"Lesson plan Python by Richel"},{"location":"lesson_plans/20241022_richel/#suggest-new-schedule","text":"","title":"Suggest new schedule"},{"location":"lesson_plans/20241022_richel/#old-schedule","text":"Time Topic Teacher 9.00 Syllabus Rebecca 9.10 Python in general Rebecca 9.20 Load modules and run Rebecca 9.45 Break . 10:00 Packages Richel 10.30 Break . 10.45 Isolated environments (ML, venv, conda ) Richel 11:45 Break or informal chat Richel 12.00 Lunch . 13.00 Batch Birgitte 13:20 GPU Birgitte 13.30 Kebnekaise: Jupyter Birgitte . Rackham: Interactive session and Jupyter Richel 13.45 Break . 14.00 Parallel and multithreaded functions Pedro 14.25 Conclusion & Q/A Birgitte 14.45 . . 15.00 END . Notes: 9:15-10:00 is first login 10:00-10:15 is first break Day ends at 16.00 interactive must get 15 more minutes to do 1 core + 2 core more time for sbatch and GPU","title":"Old schedule"},{"location":"lesson_plans/20241022_richel/#suggestion","text":"Current durations: Topic Current duration (mins) Syllabus 10 Python in general 10 Load modules and run 25 Packages 30 Isolated environments (ML, venv, conda ) 60 Batch 20 GPU 10 Kebnekaise: Jupyter 15 Rackham: Interactive session and Jupyter . (parallel with Kebnekaise) Parallel and multithreaded functions 25 Total 205 Session durations in time: Time Topic Duration (mins) 09:00-09:45 First login (45) 09:45-10:00 Break . 10:00-11:00 Session 1 60 11:00-11:15 Break . 11:15-12:00 Session 2 45 12:00-13:00 Lunch . 13:00-14:00 Session 3 60 14:00-14:15 Break . 14:15-15:00 Session 4 45 15:00-15:15 Break . 15:15-16:00 Session 5 45 Total 255 This means I can add 50 minutes. Here I distribute these from the notes: Topic Current duration (mins) Syllabus 10 Python in general 10 Load modules and run 25 + 5 Packages 30 Isolated environments (ML, venv, conda ) 60 Batch 20 + 10 GPU 10 + 10 Kebnekaise: Jupyter 15 + 15 Rackham: Interactive session and Jupyter . (parallel with Kebnekaise) Parallel and multithreaded functions 25 + 10 Total 205 + 50 = 255 Now fitting this into this decided schedule: Time Topic Teacher 09:00-09:45 First login BB + PO+ RB + RP 09:45-10:00 Break . 10:00-10:10 Syllabus BB, RB, RP 10:10-10:20 Python in general BB, RB RP 10:20-10:50 Load modules and run BB, RB RP 10:50-11:05 Break . 11:05-11:35 Packages 30/30 RB 11:35-12:00 Isolated environments (ML, venv, conda) 25/60 mins RB 12:00-13:00 Lunch . 13:00-13:35 Isolated environments (ML, venv, conda) 35/60 mins RB 13:35-14:05 Batch 30/30 mins BB 14:05-14:20 Break . 14:20-14:40 GPU BB 14:40-15:10 Simultaneous session 30/30 mins BB RB RP 15:10-15:25 Break . 15:25-16:00 Parallel and multithreaded functions PO 16:00-16:15 Summary and evaluation RB Simultaneous session: Kebnekaise: Jupyter, by BB Rackham: Interactive session and Jupyter, by RB LUNARC, by RP Discussion points: For this schedules, breaks were scheduled first, so that (1) schedule is simple, and (2) all session lengths are 45 or 60 mins. However, assuming all durations are correct (!), some sessions will be cut by a break. Is that acceptable? If yes: no problemo No: where should the breaks be instead? Who makes a new schedule? I picked to discuss batch (by BB) from 13:35-14:05, over having a break at 14:00 and having a 5 minute session with a schedule like below. Agreed? Time Topic Teacher 13:35-14:00 Batch 25/30 mins BB 14:00-14:15 Break . 14:15-14:20 Batch 5/30 mins BB R: There is no time for evaluation at the end. I feel 10 minutes is enough and the time is worth it. I suggest to add it. Pro: helps us improve the course, shows we care about our learners\u2019 opinion Con: takes time away Vote: Yes: No: If evaluation, suggest questions on confidence, see this earlier evaluation Pro: useful to asses stronger/weaker sessions of the course Con: maybe must use a standard evaluation form, needs teachers to formulate their lesson goals Vote: Yes: No:","title":"Suggestion"},{"location":"lesson_plans/20241024_richel/","text":"Lesson plan R by Richel \u00b6 Date: Tuesday 2024-10-24 Course: R, part of R, Python, Julia, and Matlab in HPC Schedule \u00b6 Time Topic Teacher(s) 9:00 (optional) First login BB + PO + RB 9:45 Break . 10:00 Introduction RB 10:10 Syllabus RB 10:20 Load modules and run RB 10:45 Break . 11:00 Packages BB 11:30 Isolated environments BB 12:00 Lunch . 13:00 Batch BB 13:30 Parallel PO 14:15 Break . 14:30 Simultaneous session PO * RB * ?RP 15:15 Break . 15:30 Machine learning BB or PO 16:00 Summary and evaluation RB 16:15 Done . Preparation \u00b6 Here is the current state of my sessions: Time Topic State 10:00 Introduction 10/10 10:10 Syllabus 7/10 10:20 Load modules and run 7/10 14:30 Simultaneous session Done 16:00 Summary and evaluation Done In general: Go through the material Do the exercises for all centers Check the exercise by creating a video per center Let\u2019s do this. Also: [x] Document how to get and extract the tarball Last session: Time Topic State 10:00 Introduction Done 10:10 Syllabus Done 10:20 Load modules and run 7/10 14:30 Simultaneous session Done 16:00 Summary and evaluation Done Currently, my sessions have two setups: The tab look The dropdown look Today I feel the dropdown look to be cleaner, as it preserves vertical space better, so I\u2019ll use that one instead. I do want to change the colors of the admonition per HPC center :-) [ ] change the colors of the admonition per HPC center I gave up here Videos are done, so the material is finished. TODO: [x] Fix layout [x] Fix module load ... R/4.2.1 R/4.2.1 [x] Fix mermaid graphs to match the exercise [x] Fix mermaid graphs to remove the command it shows, especial \u2018Load an R package library\u2019 is confusing (in an earlier mermaid version, one could add a newline before library ) [x] Remove module avail , as it is not a learning objective [x] Check and prepare Priors I don\u2019t like my course material. It makes me lecture too much and I know I should not. Next course iteration, I will be pointing to the centers\u2019 documentation more. I did not make the time to properly do this, mostly to take it easier on myself after some ruthless days of preparing the Python session. [ ] Next course iteration: make \u2018Load and run R\u2019 point to documentation more What does need to be done: [x] Make summary match evaluation","title":"Lesson plan R by Richel"},{"location":"lesson_plans/20241024_richel/#lesson-plan-r-by-richel","text":"Date: Tuesday 2024-10-24 Course: R, part of R, Python, Julia, and Matlab in HPC","title":"Lesson plan R by Richel"},{"location":"lesson_plans/20241024_richel/#schedule","text":"Time Topic Teacher(s) 9:00 (optional) First login BB + PO + RB 9:45 Break . 10:00 Introduction RB 10:10 Syllabus RB 10:20 Load modules and run RB 10:45 Break . 11:00 Packages BB 11:30 Isolated environments BB 12:00 Lunch . 13:00 Batch BB 13:30 Parallel PO 14:15 Break . 14:30 Simultaneous session PO * RB * ?RP 15:15 Break . 15:30 Machine learning BB or PO 16:00 Summary and evaluation RB 16:15 Done .","title":"Schedule"},{"location":"lesson_plans/20241024_richel/#preparation","text":"Here is the current state of my sessions: Time Topic State 10:00 Introduction 10/10 10:10 Syllabus 7/10 10:20 Load modules and run 7/10 14:30 Simultaneous session Done 16:00 Summary and evaluation Done In general: Go through the material Do the exercises for all centers Check the exercise by creating a video per center Let\u2019s do this. Also: [x] Document how to get and extract the tarball Last session: Time Topic State 10:00 Introduction Done 10:10 Syllabus Done 10:20 Load modules and run 7/10 14:30 Simultaneous session Done 16:00 Summary and evaluation Done Currently, my sessions have two setups: The tab look The dropdown look Today I feel the dropdown look to be cleaner, as it preserves vertical space better, so I\u2019ll use that one instead. I do want to change the colors of the admonition per HPC center :-) [ ] change the colors of the admonition per HPC center I gave up here Videos are done, so the material is finished. TODO: [x] Fix layout [x] Fix module load ... R/4.2.1 R/4.2.1 [x] Fix mermaid graphs to match the exercise [x] Fix mermaid graphs to remove the command it shows, especial \u2018Load an R package library\u2019 is confusing (in an earlier mermaid version, one could add a newline before library ) [x] Remove module avail , as it is not a learning objective [x] Check and prepare Priors I don\u2019t like my course material. It makes me lecture too much and I know I should not. Next course iteration, I will be pointing to the centers\u2019 documentation more. I did not make the time to properly do this, mostly to take it easier on myself after some ruthless days of preparing the Python session. [ ] Next course iteration: make \u2018Load and run R\u2019 point to documentation more What does need to be done: [x] Make summary match evaluation","title":"Preparation"},{"location":"lesson_plans/20250324_richel/","text":"Lesson plan R by Richel \u00b6 Date: Tuesday 2024-03-24 Course: R Schedule \u00b6 +-------+------------------------------+--------------+ | Time | Topic | Teacher(s) | +=======+==============================+==============+ | 9:00 | (optional) First login | BB + PO + RB | +-------+------------------------------+--------------+ | 9:45 | Break | . | +-------+------------------------------+--------------+ | 10:00 | Introduction | RB | +-------+------------------------------+--------------+ | 10:10 | Syllabus | RB | +-------+------------------------------+--------------+ | 10:20 | Load modules and run | RB | +-------+------------------------------+--------------+ | 10:45 | Break | . | +-------+------------------------------+--------------+ | . | . | . | +-------+------------------------------+--------------+ | 14:15 | Break | . | +-------+------------------------------+--------------+ | 14:30 | Simultaneous session | . | +-------+------------------------------+--------------+ | . | HPC2N: ThinLinc, RStudio | PO | +-------+------------------------------+--------------+ | . | LUNARC: On-Demand, RStudio | RP | +-------+------------------------------+--------------+ | . | UPPMAX: Interactive, RStudio | RB | +-------+------------------------------+--------------+ | 15:15 | Break | . | +-------+------------------------------+--------------+ | 15:30 | Machine learning | PO | +-------+------------------------------+--------------+ | 16:00 | Summary and evaluation | RB | +-------+------------------------------+--------------+ | 16:15 | Done | . | +-------+------------------------------+--------------+ Preparation \u00b6 Here is the current state of my sessions: Time Topic State 10:00 Introduction . 10:10 Syllabus . 10:20 Load modules and run . 14:30 Simultaneous session . 16:00 Summary and evaluation . [ ] Next course iteration: make \u2018Load and run R\u2019 point to documentation more I want to make the course more useful and NAISS-wide, mostly pointing to documentation. [ ] In the next course iteration: use more active learning methods in this session [ ] In the simultaneous session, stay formal [For next time] RB: suggest to make \u2018Batch\u2019 15 minutes longer and remove a session, in the next course iteration These are the learning outcomes for which I have 45 minutes: find the module to be able to run R load the module to be able to run R run the R interpreter run the R command to get the list of installed R packages run an R script from the command-line I will have to simplify things :-) . Clusters used in the course (as taken from \u2018The important info about the course\u2019 document): \u2013 COSMOS Dardel Kebnekaise Rackham Tetralith","title":"Lesson plan R by Richel"},{"location":"lesson_plans/20250324_richel/#lesson-plan-r-by-richel","text":"Date: Tuesday 2024-03-24 Course: R","title":"Lesson plan R by Richel"},{"location":"lesson_plans/20250324_richel/#schedule","text":"+-------+------------------------------+--------------+ | Time | Topic | Teacher(s) | +=======+==============================+==============+ | 9:00 | (optional) First login | BB + PO + RB | +-------+------------------------------+--------------+ | 9:45 | Break | . | +-------+------------------------------+--------------+ | 10:00 | Introduction | RB | +-------+------------------------------+--------------+ | 10:10 | Syllabus | RB | +-------+------------------------------+--------------+ | 10:20 | Load modules and run | RB | +-------+------------------------------+--------------+ | 10:45 | Break | . | +-------+------------------------------+--------------+ | . | . | . | +-------+------------------------------+--------------+ | 14:15 | Break | . | +-------+------------------------------+--------------+ | 14:30 | Simultaneous session | . | +-------+------------------------------+--------------+ | . | HPC2N: ThinLinc, RStudio | PO | +-------+------------------------------+--------------+ | . | LUNARC: On-Demand, RStudio | RP | +-------+------------------------------+--------------+ | . | UPPMAX: Interactive, RStudio | RB | +-------+------------------------------+--------------+ | 15:15 | Break | . | +-------+------------------------------+--------------+ | 15:30 | Machine learning | PO | +-------+------------------------------+--------------+ | 16:00 | Summary and evaluation | RB | +-------+------------------------------+--------------+ | 16:15 | Done | . | +-------+------------------------------+--------------+","title":"Schedule"},{"location":"lesson_plans/20250324_richel/#preparation","text":"Here is the current state of my sessions: Time Topic State 10:00 Introduction . 10:10 Syllabus . 10:20 Load modules and run . 14:30 Simultaneous session . 16:00 Summary and evaluation . [ ] Next course iteration: make \u2018Load and run R\u2019 point to documentation more I want to make the course more useful and NAISS-wide, mostly pointing to documentation. [ ] In the next course iteration: use more active learning methods in this session [ ] In the simultaneous session, stay formal [For next time] RB: suggest to make \u2018Batch\u2019 15 minutes longer and remove a session, in the next course iteration These are the learning outcomes for which I have 45 minutes: find the module to be able to run R load the module to be able to run R run the R interpreter run the R command to get the list of installed R packages run an R script from the command-line I will have to simplify things :-) . Clusters used in the course (as taken from \u2018The important info about the course\u2019 document): \u2013 COSMOS Dardel Kebnekaise Rackham Tetralith","title":"Preparation"},{"location":"lesson_plans/20251006_richel/","text":"Lesson plan R by Richel \u00b6 Date: Tuesday 2025-10-06 Course: R 2025-09-17 \u00b6 I was happy with my progress on the advanced day and started working on the MkDocs website. 2025-09-18 \u00b6 I continued working on the MkDocs website. All CI tests are back. All CI tests pass now. As the session about parallelism has more unknown, I will now focus on that one first. 2025-09-22 \u00b6 Parallelism is in good enough shape, with no unknowns left. Let\u2019s remove the unknowns from this session too. Progress is clear from the lesson material: HPC Cluster Documentation Alvis Only OpenOnDemand doc Berzelius As good as no documentation Bianca Documentation COSMOS No documentation Dardel No documentation Kebnekaise Reasonable documentation LUMI No documentation Pelle Documentation Rackham Documentation Tetralith As good as no documentation , use UPPMAX docs Vera Only OpenOnDemand doc There are, however, videos, that are not based on user documentation: COSMOS RStudio OpenOnDemand CPU setup: I will use this one OpenOnDemand GPU setup From terminal HPC2N RStudio From terminal, no interactive session I will use: interactive session from terminal Those videos will make it easy to write that user documentation. 2025-09-23 \u00b6 All is done, except: [x] RStudio video Pelle, reported at UPPMAX that ThinLinc cannot connect 2025-09-24 \u00b6 Done!","title":"Lesson plan R by Richel"},{"location":"lesson_plans/20251006_richel/#lesson-plan-r-by-richel","text":"Date: Tuesday 2025-10-06 Course: R","title":"Lesson plan R by Richel"},{"location":"lesson_plans/20251006_richel/#2025-09-17","text":"I was happy with my progress on the advanced day and started working on the MkDocs website.","title":"2025-09-17"},{"location":"lesson_plans/20251006_richel/#2025-09-18","text":"I continued working on the MkDocs website. All CI tests are back. All CI tests pass now. As the session about parallelism has more unknown, I will now focus on that one first.","title":"2025-09-18"},{"location":"lesson_plans/20251006_richel/#2025-09-22","text":"Parallelism is in good enough shape, with no unknowns left. Let\u2019s remove the unknowns from this session too. Progress is clear from the lesson material: HPC Cluster Documentation Alvis Only OpenOnDemand doc Berzelius As good as no documentation Bianca Documentation COSMOS No documentation Dardel No documentation Kebnekaise Reasonable documentation LUMI No documentation Pelle Documentation Rackham Documentation Tetralith As good as no documentation , use UPPMAX docs Vera Only OpenOnDemand doc There are, however, videos, that are not based on user documentation: COSMOS RStudio OpenOnDemand CPU setup: I will use this one OpenOnDemand GPU setup From terminal HPC2N RStudio From terminal, no interactive session I will use: interactive session from terminal Those videos will make it easy to write that user documentation.","title":"2025-09-22"},{"location":"lesson_plans/20251006_richel/#2025-09-23","text":"All is done, except: [x] RStudio video Pelle, reported at UPPMAX that ThinLinc cannot connect","title":"2025-09-23"},{"location":"lesson_plans/20251006_richel/#2025-09-24","text":"Done!","title":"2025-09-24"},{"location":"lesson_plans/20251010_richel/","text":"Lesson plan Advanced by Richel \u00b6 Date: Tuesday 2025-10-06 Course: Advanced 2025-09-04 \u00b6 I need to improve the SCoRe user documentation on using HPC resources efficiently. This goes perfectly hand-in-hand with the parallel computing session. As the parallel session is language agnostic, I do not need to use R, Julia or MATLAB code at all. Instead, I can use BEAST2. From the first 1000 lines of UPPMAX modules, these were labelled to be the most popular: GROMACS, BWA, Bowtie2, ABySS, GATK, BEDTools. I also heard of LINPACK and BLAST. Let\u2019s start with GROMACS, as it is popular and available on Pelle. YouTube video 1UN3 RCSB page Downloaded 1UN3.pdb sudo apt install jmol jmol 1UN3.pdb GROMACS on LUMI course To run on Pelle: wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/docs-mk/lesson_plans/20251010_richel/lumi_course/run_pelle.sh sbatch run_pelle.sh Need to use srun 2025-09-11 \u00b6 I think this was a bad idea: the course is an R and MATLAB and Julia course. Let\u2019s change the learning outcome to run a script in one of those languages. 2025-09-16 \u00b6 I am going to start with the MPI scripts now. They work! And then I find out that OpenMPI is not OpenMP. I feel: Technique Uses Notes Normal 1 core . OpenMP 1 to all cores on a node A threaded mechanism, uses OMP_NUM_THREADS OpenMPI 1 to all cores on multiple nodes Rmpi package Works: $ sbatch integration2d-rackham_8.sh Submitted batch job 56823317 [ richel@rackham3 6_integration2d ] $ cat slurm-56823317.out R_packages/4.1.1: The RStudio packages pane is disabled when loading this module, due to performance issues. All packages are still available. R_packages/4.1.1: For more information and instructions to re-enable it, see 'module help R_packages/4.1.1' Loading required package: foreach Loading required package: iterators [ 1 ] \"Integral value is 9.07607322631065e-15 Error is 9.07607322631065e-15\" [ 1 ] \"Time spent: 0.263934373855591 seconds\" Using a CLI arg works fine, as long as it is convert to a numeric value: [ richel@rackham3 6_integration2d ] $ cat *.out [ ... ] nworkers: 1 [ 1 ] \"Integral value is -5.22070689324334e-16 Error is 5.22070689324334e-16\" [ 1 ] \"Time spent: 0.813078641891479 seconds\" [ ... ] nworkers: 8 [ 1 ] \"Integral value is 9.07607322631065e-15 Error is 9.07607322631065e-15\" [ 1 ] \"Time spent: 0.228480815887451 seconds\" Using a 10x bigger grid: [ richel@rackham3 6_integration2d ] $ cat slurm-56825681.out [ ... ] nworkers: 1 [ 1 ] \"Integral value is -6.2179313190142e-17 Error is 6.2179313190142e-17\" [ 1 ] \"Time spent: 64.7400653362274 seconds\" [ richel@rackham3 6_integration2d ] $ cat slurm-56825683.out [ ... ] nworkers: 8 [ 1 ] \"Integral value is -4.35873559467836e-13 Error is 4.35873559467836e-13\" [ 1 ] \"Time spent: 8.92967510223389 seconds\" Interesting, that if the grid increases 10x, 1 worker takes 80x longer and 8 workers take 40x longer. Also, the accuracy decreases with multiple works. Also, the overhead changes Grid size Time 1 worker Time 8 workers Corehours 8 workers Overhead Original 0.81 0.22 8*0.22=1.76 1.76/0.81=2.172839506 10x 64.74 8.93 8*8.93=71.44 71.44/64.74=1.103490887 Increase 80x 40x 40x NA OK, now it is time to cleanup the script a bit. I want these to display the corehours too, and less fluff. Output is cleaner: [ richel@rackham3 6_integration2d ] $ ls integration2d-cosmos.sh integration2d-rackham_1.sh schedule_rackham_runs.sh integration2d-dardel.sh integration2d-rackham_8.sh slurm-56827590.out integration2d-kebnekaise.sh integration2d_rackham_x.sh slurm-56827597.out integration2d.R integration2d-tetralith.sh [ richel@rackham3 6_integration2d ] $ cat *.out Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6 .2179313190142e-17 Time spent on 1 core: 67 .9925961494446 seconds Time spent on all cores: 67 .9925961494446 seconds Number of cores booked in Slurm: 8 Number of workers: 8 Grid size: 8400 Integral value: -4.35873559467836e-13 Integral error: 4 .35873559467836e-13 Time spent on 1 core: 9 .48788905143738 seconds Time spent on all cores: 75 .903112411499 seconds A replication experiment: [richel@rackham3 6_integration2d]$ cat *.out Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 71.8729729652405 seconds Time spent on all cores: 71.8729729652405 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 72.9636025428772 seconds Time spent on all cores: 72.9636025428772 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 75.2127022743225 seconds Time spent on all cores: 75.2127022743225 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 74.9676728248596 seconds Time spent on all cores: 74.9676728248596 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 73.961464881897 seconds Time spent on all cores: 73.961464881897 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 73.2491600513458 seconds Time spent on all cores: 73.2491600513458 seconds [richel@rackham3 6_integration2d]$ I give up calling a Slurm script with the -A or -n parameter: it does not work (that is how I got a replicate run) Number of cores booked in Slurm: 16 Number of workers: 16 Grid size: 8400 Integral value: 1.25011112572793e-13 Integral error: 1.25011112572793e-13 Time spent on 1 core (seconds): 4.89688730239868 Time spent on all cores (seconds): 78.3501968383789 Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core (seconds): 71.9557588100433 Time spent on all cores (seconds): 71.9557588100433 Number of cores booked in Slurm: 2 Number of workers: 2 Grid size: 8400 Integral value: -1.00190966634273e-11 Integral error: 1.00190966634273e-11 Time spent on 1 core (seconds): 38.2071380615234 Time spent on all cores (seconds): 76.4142761230469 Number of cores booked in Slurm: 4 Number of workers: 4 Grid size: 8400 Integral value: -2.64421817774974e-12 Integral error: 2.64421817774974e-12 Time spent on 1 core (seconds): 18.3012194633484 Time spent on all cores (seconds): 73.2048778533936 Number of cores booked in Slurm: 8 Number of workers: 8 Grid size: 8400 Integral value: -4.35873559467836e-13 Integral error: 4.35873559467836e-13 Time spent on 1 core (seconds): 8.9283595085144 Time spent on all cores (seconds): 71.4268760681152 Bigger job: Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 16384 Integral value: 1.76582323875401e-16 Integral error: 1.76582323875401e-16 Time spent on 1 core (seconds): 277.048353910446 Time spent on all cores (seconds): 277.048353910446 Number of cores booked in Slurm: 2 Number of workers: 2 Grid size: 16384 Integral value: -1.81823445188911e-11 Integral error: 1.81823445188911e-11 Time spent on 1 core (seconds): 139.744601011276 Time spent on all cores (seconds): 279.489202022552 Number of cores booked in Slurm: 4 Number of workers: 4 Grid size: 16384 Integral value: -3.75399711316504e-12 Integral error: 3.75399711316504e-12 Time spent on 1 core (seconds): 68.331995010376 Time spent on all cores (seconds): 273.327980041504 Number of cores booked in Slurm: 8 Number of workers: 8 Grid size: 16384 Integral value: -3.48418516260551e-12 Integral error: 3.48418516260551e-12 Time spent on 1 core (seconds): 35.11354637146 Time spent on all cores (seconds): 280.90837097168 Number of cores booked in Slurm: 16 Number of workers: 16 Grid size: 16384 Integral value: 2.79685996584789e-13 Integral error: 2.79685996584789e-13 Time spent on 1 core (seconds): 17.8458185195923 Time spent on all cores (seconds): 285.533096313477 Hmmm, my COSMOS jobs end up on different nodes \u2026? [ richel@cosmos1 6_integration2d ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 1587029 lu48 integrat richel R INVALID 3 cn [ 003 ,008,010 ] 1587033 lu48 integrat richel R INVALID 1 cn010 1587032 lu48 integrat richel R INVALID 1 cn010 1587031 lu48 integrat richel R INVALID 1 cn039 1587030 lu48 integrat richel R INVALID 1 cn010 Results: Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 16384 Integral value: 1.76582323875401e-16 Integral error: 1.76582323875401e-16 Time spent on 1 core (seconds): 141.671043157578 Time spent on all cores (seconds): 141.671043157578 Number of cores booked in Slurm: 2 Number of workers: 2 Grid size: 16384 Integral value: -1.81823445188911e-11 Integral error: 1.81823445188911e-11 Time spent on 1 core (seconds): 72.5188694000244 Time spent on all cores (seconds): 145.037738800049 Number of cores booked in Slurm: 4 Number of workers: 4 Grid size: 16384 Integral value: -3.75399711316504e-12 Integral error: 3.75399711316504e-12 Time spent on 1 core (seconds): 37.6001505851746 Time spent on all cores (seconds): 150.400602340698 Number of cores booked in Slurm: 8 Number of workers: 8 Grid size: 16384 Integral value: -3.48418516260551e-12 Integral error: 3.48418516260551e-12 Time spent on 1 core (seconds): 19.8832325935364 Time spent on all cores (seconds): 159.065860748291 Number of cores booked in Slurm: 16 Number of workers: 16 Grid size: 16384 Integral value: 2.79685996584789e-13 Integral error: 2.79685996584789e-13 Time spent on 1 core (seconds): 51.425624370575 Time spent on all cores (seconds): 822.809989929199 2025-09-17 \u00b6 I feel confident with the threaded parallelism. Pedro did a great job with the materials! Steps: [x] Sketch content \u2018Threaded parallelisms\u2019 [x] Julia [ ] MATLAB [x] R [ABANDON] Sketch content \u2018Distributed parallelisms\u2019 [x] Sketch content \u2018General parallelism\u2019 [ ] Julia [ ] MATLAB [x] R 2025-09-23 \u00b6 Now I have more or less carved out the content, let\u2019s restart with learning outcomes\u2026 2025-09-26 \u00b6 [ ] Make first script download all others [ ] Fix and run MATLAB script 2025-10-09 \u00b6 I\u2019ve been ill, removing 3 full days of preparation. I will reduce my goals and prepare as such: Current exercises in draft form Fix MATLAB if possible and if at least 1 participant 2025-10-10 \u00b6 I\u2019ve prepared good enough: Exercises are done Video is made MATLAB does not work","title":"Lesson plan Advanced by Richel"},{"location":"lesson_plans/20251010_richel/#lesson-plan-advanced-by-richel","text":"Date: Tuesday 2025-10-06 Course: Advanced","title":"Lesson plan Advanced by Richel"},{"location":"lesson_plans/20251010_richel/#2025-09-04","text":"I need to improve the SCoRe user documentation on using HPC resources efficiently. This goes perfectly hand-in-hand with the parallel computing session. As the parallel session is language agnostic, I do not need to use R, Julia or MATLAB code at all. Instead, I can use BEAST2. From the first 1000 lines of UPPMAX modules, these were labelled to be the most popular: GROMACS, BWA, Bowtie2, ABySS, GATK, BEDTools. I also heard of LINPACK and BLAST. Let\u2019s start with GROMACS, as it is popular and available on Pelle. YouTube video 1UN3 RCSB page Downloaded 1UN3.pdb sudo apt install jmol jmol 1UN3.pdb GROMACS on LUMI course To run on Pelle: wget https://github.com/UPPMAX/R-matlab-julia-HPC/raw/refs/heads/main/docs-mk/lesson_plans/20251010_richel/lumi_course/run_pelle.sh sbatch run_pelle.sh Need to use srun","title":"2025-09-04"},{"location":"lesson_plans/20251010_richel/#2025-09-11","text":"I think this was a bad idea: the course is an R and MATLAB and Julia course. Let\u2019s change the learning outcome to run a script in one of those languages.","title":"2025-09-11"},{"location":"lesson_plans/20251010_richel/#2025-09-16","text":"I am going to start with the MPI scripts now. They work! And then I find out that OpenMPI is not OpenMP. I feel: Technique Uses Notes Normal 1 core . OpenMP 1 to all cores on a node A threaded mechanism, uses OMP_NUM_THREADS OpenMPI 1 to all cores on multiple nodes Rmpi package Works: $ sbatch integration2d-rackham_8.sh Submitted batch job 56823317 [ richel@rackham3 6_integration2d ] $ cat slurm-56823317.out R_packages/4.1.1: The RStudio packages pane is disabled when loading this module, due to performance issues. All packages are still available. R_packages/4.1.1: For more information and instructions to re-enable it, see 'module help R_packages/4.1.1' Loading required package: foreach Loading required package: iterators [ 1 ] \"Integral value is 9.07607322631065e-15 Error is 9.07607322631065e-15\" [ 1 ] \"Time spent: 0.263934373855591 seconds\" Using a CLI arg works fine, as long as it is convert to a numeric value: [ richel@rackham3 6_integration2d ] $ cat *.out [ ... ] nworkers: 1 [ 1 ] \"Integral value is -5.22070689324334e-16 Error is 5.22070689324334e-16\" [ 1 ] \"Time spent: 0.813078641891479 seconds\" [ ... ] nworkers: 8 [ 1 ] \"Integral value is 9.07607322631065e-15 Error is 9.07607322631065e-15\" [ 1 ] \"Time spent: 0.228480815887451 seconds\" Using a 10x bigger grid: [ richel@rackham3 6_integration2d ] $ cat slurm-56825681.out [ ... ] nworkers: 1 [ 1 ] \"Integral value is -6.2179313190142e-17 Error is 6.2179313190142e-17\" [ 1 ] \"Time spent: 64.7400653362274 seconds\" [ richel@rackham3 6_integration2d ] $ cat slurm-56825683.out [ ... ] nworkers: 8 [ 1 ] \"Integral value is -4.35873559467836e-13 Error is 4.35873559467836e-13\" [ 1 ] \"Time spent: 8.92967510223389 seconds\" Interesting, that if the grid increases 10x, 1 worker takes 80x longer and 8 workers take 40x longer. Also, the accuracy decreases with multiple works. Also, the overhead changes Grid size Time 1 worker Time 8 workers Corehours 8 workers Overhead Original 0.81 0.22 8*0.22=1.76 1.76/0.81=2.172839506 10x 64.74 8.93 8*8.93=71.44 71.44/64.74=1.103490887 Increase 80x 40x 40x NA OK, now it is time to cleanup the script a bit. I want these to display the corehours too, and less fluff. Output is cleaner: [ richel@rackham3 6_integration2d ] $ ls integration2d-cosmos.sh integration2d-rackham_1.sh schedule_rackham_runs.sh integration2d-dardel.sh integration2d-rackham_8.sh slurm-56827590.out integration2d-kebnekaise.sh integration2d_rackham_x.sh slurm-56827597.out integration2d.R integration2d-tetralith.sh [ richel@rackham3 6_integration2d ] $ cat *.out Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6 .2179313190142e-17 Time spent on 1 core: 67 .9925961494446 seconds Time spent on all cores: 67 .9925961494446 seconds Number of cores booked in Slurm: 8 Number of workers: 8 Grid size: 8400 Integral value: -4.35873559467836e-13 Integral error: 4 .35873559467836e-13 Time spent on 1 core: 9 .48788905143738 seconds Time spent on all cores: 75 .903112411499 seconds A replication experiment: [richel@rackham3 6_integration2d]$ cat *.out Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 71.8729729652405 seconds Time spent on all cores: 71.8729729652405 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 72.9636025428772 seconds Time spent on all cores: 72.9636025428772 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 75.2127022743225 seconds Time spent on all cores: 75.2127022743225 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 74.9676728248596 seconds Time spent on all cores: 74.9676728248596 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 73.961464881897 seconds Time spent on all cores: 73.961464881897 seconds Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core: 73.2491600513458 seconds Time spent on all cores: 73.2491600513458 seconds [richel@rackham3 6_integration2d]$ I give up calling a Slurm script with the -A or -n parameter: it does not work (that is how I got a replicate run) Number of cores booked in Slurm: 16 Number of workers: 16 Grid size: 8400 Integral value: 1.25011112572793e-13 Integral error: 1.25011112572793e-13 Time spent on 1 core (seconds): 4.89688730239868 Time spent on all cores (seconds): 78.3501968383789 Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 8400 Integral value: -6.2179313190142e-17 Integral error: 6.2179313190142e-17 Time spent on 1 core (seconds): 71.9557588100433 Time spent on all cores (seconds): 71.9557588100433 Number of cores booked in Slurm: 2 Number of workers: 2 Grid size: 8400 Integral value: -1.00190966634273e-11 Integral error: 1.00190966634273e-11 Time spent on 1 core (seconds): 38.2071380615234 Time spent on all cores (seconds): 76.4142761230469 Number of cores booked in Slurm: 4 Number of workers: 4 Grid size: 8400 Integral value: -2.64421817774974e-12 Integral error: 2.64421817774974e-12 Time spent on 1 core (seconds): 18.3012194633484 Time spent on all cores (seconds): 73.2048778533936 Number of cores booked in Slurm: 8 Number of workers: 8 Grid size: 8400 Integral value: -4.35873559467836e-13 Integral error: 4.35873559467836e-13 Time spent on 1 core (seconds): 8.9283595085144 Time spent on all cores (seconds): 71.4268760681152 Bigger job: Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 16384 Integral value: 1.76582323875401e-16 Integral error: 1.76582323875401e-16 Time spent on 1 core (seconds): 277.048353910446 Time spent on all cores (seconds): 277.048353910446 Number of cores booked in Slurm: 2 Number of workers: 2 Grid size: 16384 Integral value: -1.81823445188911e-11 Integral error: 1.81823445188911e-11 Time spent on 1 core (seconds): 139.744601011276 Time spent on all cores (seconds): 279.489202022552 Number of cores booked in Slurm: 4 Number of workers: 4 Grid size: 16384 Integral value: -3.75399711316504e-12 Integral error: 3.75399711316504e-12 Time spent on 1 core (seconds): 68.331995010376 Time spent on all cores (seconds): 273.327980041504 Number of cores booked in Slurm: 8 Number of workers: 8 Grid size: 16384 Integral value: -3.48418516260551e-12 Integral error: 3.48418516260551e-12 Time spent on 1 core (seconds): 35.11354637146 Time spent on all cores (seconds): 280.90837097168 Number of cores booked in Slurm: 16 Number of workers: 16 Grid size: 16384 Integral value: 2.79685996584789e-13 Integral error: 2.79685996584789e-13 Time spent on 1 core (seconds): 17.8458185195923 Time spent on all cores (seconds): 285.533096313477 Hmmm, my COSMOS jobs end up on different nodes \u2026? [ richel@cosmos1 6_integration2d ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 1587029 lu48 integrat richel R INVALID 3 cn [ 003 ,008,010 ] 1587033 lu48 integrat richel R INVALID 1 cn010 1587032 lu48 integrat richel R INVALID 1 cn010 1587031 lu48 integrat richel R INVALID 1 cn039 1587030 lu48 integrat richel R INVALID 1 cn010 Results: Number of cores booked in Slurm: 1 Number of workers: 1 Grid size: 16384 Integral value: 1.76582323875401e-16 Integral error: 1.76582323875401e-16 Time spent on 1 core (seconds): 141.671043157578 Time spent on all cores (seconds): 141.671043157578 Number of cores booked in Slurm: 2 Number of workers: 2 Grid size: 16384 Integral value: -1.81823445188911e-11 Integral error: 1.81823445188911e-11 Time spent on 1 core (seconds): 72.5188694000244 Time spent on all cores (seconds): 145.037738800049 Number of cores booked in Slurm: 4 Number of workers: 4 Grid size: 16384 Integral value: -3.75399711316504e-12 Integral error: 3.75399711316504e-12 Time spent on 1 core (seconds): 37.6001505851746 Time spent on all cores (seconds): 150.400602340698 Number of cores booked in Slurm: 8 Number of workers: 8 Grid size: 16384 Integral value: -3.48418516260551e-12 Integral error: 3.48418516260551e-12 Time spent on 1 core (seconds): 19.8832325935364 Time spent on all cores (seconds): 159.065860748291 Number of cores booked in Slurm: 16 Number of workers: 16 Grid size: 16384 Integral value: 2.79685996584789e-13 Integral error: 2.79685996584789e-13 Time spent on 1 core (seconds): 51.425624370575 Time spent on all cores (seconds): 822.809989929199","title":"2025-09-16"},{"location":"lesson_plans/20251010_richel/#2025-09-17","text":"I feel confident with the threaded parallelism. Pedro did a great job with the materials! Steps: [x] Sketch content \u2018Threaded parallelisms\u2019 [x] Julia [ ] MATLAB [x] R [ABANDON] Sketch content \u2018Distributed parallelisms\u2019 [x] Sketch content \u2018General parallelism\u2019 [ ] Julia [ ] MATLAB [x] R","title":"2025-09-17"},{"location":"lesson_plans/20251010_richel/#2025-09-23","text":"Now I have more or less carved out the content, let\u2019s restart with learning outcomes\u2026","title":"2025-09-23"},{"location":"lesson_plans/20251010_richel/#2025-09-26","text":"[ ] Make first script download all others [ ] Fix and run MATLAB script","title":"2025-09-26"},{"location":"lesson_plans/20251010_richel/#2025-10-09","text":"I\u2019ve been ill, removing 3 full days of preparation. I will reduce my goals and prepare as such: Current exercises in draft form Fix MATLAB if possible and if at least 1 participant","title":"2025-10-09"},{"location":"lesson_plans/20251010_richel/#2025-10-10","text":"I\u2019ve prepared good enough: Exercises are done Video is made MATLAB does not work","title":"2025-10-10"},{"location":"lesson_plans/20251010_richel/4_lumi_course/","text":"LUMI course \u00b6 Get the data: wget https://a3s.fi/gmx-lumi/aqp-240122.tar.gz tar -xf aqp-240122.tar.gz cp aquaporin/topol.tpr . Running: gmx_mpi mdrun -g ex1.1 -nsteps -1 -maxh 0 .017 -resethway -notunepme Results in: gmx_mpi mdrun \\ -g ex1.1 \\ -nsteps -1 -maxh 0.017 -resethway -notunepme :-) GROMACS - gmx mdrun, 2023.3-Ubuntu_2023.3_1ubuntu3 (-: Executable: /usr/bin/gmx_mpi Data prefix: /usr Working dir: /home/richel/GitHubs/R-matlab-julia-HPC/docs-mk/lesson_plans/20251010_richel/lumi_course Command line: gmx_mpi mdrun -g ex1.1 -nsteps -1 -maxh 0.017 -resethway -notunepme Compiled SIMD is SSE4.1, but AVX2_256 might be faster (see log). Reading file topol.tpr, VERSION 2023.3-dev-20230922-fca514265a (single precision) Overriding nsteps with value passed on the command line: -1 steps Changing nstlist from 40 to 100, rlist from 1.2 to 1.287 Using 1 MPI process Using 16 OpenMP threads starting mdrun 'prot and bilayer membed' -1 steps, infinite ps. step 1011: resetting all time and cycle counters Step 2000: Run time exceeded 0.017 hours, will terminate the run within 100 steps Core t (s) Wall t (s) (%) Time: 613.429 38.340 1600.0 (ns/day) (hour/ns) Performance: 6.141 3.908 GROMACS reminds you: \"FORTRAN, the infantile disorder, by now nearly 20 years old, is hopelessly inadequate for whatever computer application you have in mind today: it is now too clumsy, too risky, and too expensive to use.\" (Edsger Dijkstra, 1970) Blimey, the log file knows things about my computer: Running on 1 node with total 12 cores, 16 processing units Hardware detected on host richel-latitude-7430 (the node of MPI rank 0): CPU info: Vendor: Intel Brand: 12th Gen Intel(R) Core(TM) i5-1250P Family: 6 Model: 154 Stepping: 3 Features: aes apic avx avx2 clfsh cmov cx8 cx16 f16c fma htt intel lahf mmx msr nonstop_tsc pcid pclmuldq pdcm pdpe1gb popcnt pse rdrnd rdtscp sha sse2 sse3 sse4.1 sse4.2 ssse3 tdt x2apic Hardware topology: Full, with devices Packages, cores, and logical processors: [indices refer to OS logical processors] Package 0: [ 0 1] [ 2 3] [ 4 5] [ 6 7] [ 8] [ 9] [ 10] [ 11] [ 12] [ 13] [ 14] [ 15] CPU limit set by OS: -1 Recommended max number of threads: 16 Numa nodes: Node 0 (16172212224 bytes mem): 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Latency: 0 0 1.00 Caches: L1: 49152 bytes, linesize 64 bytes, assoc. 12, shared 2 ways L2: 1310720 bytes, linesize 64 bytes, assoc. 10, shared 2 ways L3: 12582912 bytes, linesize 64 bytes, assoc. 8, shared 16 ways PCI devices: 0000:00:02.0 Id: 8086:46a6 Class: 0x0300 Numa: 0 0000:01:00.0 Id: 1c5c:1959 Class: 0x0108 Numa: 0 0000:00:14.3 Id: 8086:51f0 Class: 0x0280 Numa: 0 Likely fastest SIMD instructions supported by all nodes: AVX2_256 SIMD instructions selected at compile time: SSE4.1 Running on Pelle: Single core, 85 seconds: #SBATCH -A staff #SBATCH --time=00:10:00 # maximum execution time of #SBATCH --ntasks-per-node=1 # 1 MPI rank #SBATCH --cpus-per-task=1 # number cpus-per-task with log: $ head -n 100 ex1.1_1x_jID42696.log :-) GROMACS - gmx mdrun, 2024.4-EasyBuild_5.1.0 (-: Coordinated by the GROMACS project leaders: Berk Hess and Erik Lindahl GROMACS: gmx mdrun, version 2024.4-EasyBuild_5.1.0 Executable: /sw/arch/eb/software/GROMACS/2024.4-foss-2023b/bin/gmx_mpi Data prefix: /sw/arch/eb/software/GROMACS/2024.4-foss-2023b Working dir: /domus/h1/richel Process ID: 193730 Command line: gmx_mpi mdrun -g ex1.1_1x_jID42696 -nsteps -1 -maxh 0.017 -resethway -notunepme GROMACS version: 2024.4-EasyBuild_5.1.0 Precision: mixed Memory model: 64 bit MPI library: MPI MPI library version: Open MPI v4.1.6, package: Open MPI iusan@p204.uppmax.uu.se Distribution, ident: 4.1.6, repo rev: v4.1.6, Sep 30, 2023 OpenMP support: enabled (GMX_OPENMP_MAX_THREADS = 128) GPU support: disabled SIMD instructions: AVX_512 CPU FFT library: fftw-3.3.10-sse2-avx-avx2-avx2_128 GPU FFT library: none Multi-GPU FFT: none RDTSCP usage: enabled TNG support: enabled Hwloc support: disabled Tracing support: disabled C compiler: /sw/arch/eb/software/OpenMPI/4.1.6-GCC-13.2.0/bin/mpicc GNU 13.2.0 C compiler flags: -fexcess-precision=fast -funroll-all-loops -march=skylake-avx512 -Wno-missing-field-initializers -O3 -DNDEBUG C++ compiler: /sw/arch/eb/software/OpenMPI/4.1.6-GCC-13.2.0/bin/mpicxx GNU 13.2.0 C++ compiler flags: -fexcess-precision=fast -funroll-all-loops -march=skylake-avx512 -Wno-missing-field-initializers -Wno-cast-function-type-strict SHELL:-fopenmp -O3 -DNDEBUG BLAS library: External - user-supplied LAPACK library: External - user-supplied Running on 1 node with total 1 cores, 2 processing units Hardware detected on host p62.uppmax.uu.se (the node of MPI rank 0): CPU info: Vendor: AMD Brand: AMD EPYC 9454P 48-Core Processor Family: 25 Model: 17 Stepping: 1 Features: aes amd apic avx avx2 avx512f avx512cd avx512bw avx512vl avx512bf16 avx512secondFMA clfsh cmov cx8 cx16 f16c fma htt lahf misalignsse mmx msr nonstop_tsc pcid pclmuldq pdpe1gb popcnt pse rdrnd rdtscp sha sse2 sse3 sse4a sse4.1 sse4.2 ssse3 x2apic Hardware topology: Basic Packages, cores, and logical processors: [indices refer to OS logical processors] Package 0: [ 32 80] CPU limit set by OS: -1 Recommended max number of threads: 2","title":"LUMI course"},{"location":"lesson_plans/20251010_richel/4_lumi_course/#lumi-course","text":"Get the data: wget https://a3s.fi/gmx-lumi/aqp-240122.tar.gz tar -xf aqp-240122.tar.gz cp aquaporin/topol.tpr . Running: gmx_mpi mdrun -g ex1.1 -nsteps -1 -maxh 0 .017 -resethway -notunepme Results in: gmx_mpi mdrun \\ -g ex1.1 \\ -nsteps -1 -maxh 0.017 -resethway -notunepme :-) GROMACS - gmx mdrun, 2023.3-Ubuntu_2023.3_1ubuntu3 (-: Executable: /usr/bin/gmx_mpi Data prefix: /usr Working dir: /home/richel/GitHubs/R-matlab-julia-HPC/docs-mk/lesson_plans/20251010_richel/lumi_course Command line: gmx_mpi mdrun -g ex1.1 -nsteps -1 -maxh 0.017 -resethway -notunepme Compiled SIMD is SSE4.1, but AVX2_256 might be faster (see log). Reading file topol.tpr, VERSION 2023.3-dev-20230922-fca514265a (single precision) Overriding nsteps with value passed on the command line: -1 steps Changing nstlist from 40 to 100, rlist from 1.2 to 1.287 Using 1 MPI process Using 16 OpenMP threads starting mdrun 'prot and bilayer membed' -1 steps, infinite ps. step 1011: resetting all time and cycle counters Step 2000: Run time exceeded 0.017 hours, will terminate the run within 100 steps Core t (s) Wall t (s) (%) Time: 613.429 38.340 1600.0 (ns/day) (hour/ns) Performance: 6.141 3.908 GROMACS reminds you: \"FORTRAN, the infantile disorder, by now nearly 20 years old, is hopelessly inadequate for whatever computer application you have in mind today: it is now too clumsy, too risky, and too expensive to use.\" (Edsger Dijkstra, 1970) Blimey, the log file knows things about my computer: Running on 1 node with total 12 cores, 16 processing units Hardware detected on host richel-latitude-7430 (the node of MPI rank 0): CPU info: Vendor: Intel Brand: 12th Gen Intel(R) Core(TM) i5-1250P Family: 6 Model: 154 Stepping: 3 Features: aes apic avx avx2 clfsh cmov cx8 cx16 f16c fma htt intel lahf mmx msr nonstop_tsc pcid pclmuldq pdcm pdpe1gb popcnt pse rdrnd rdtscp sha sse2 sse3 sse4.1 sse4.2 ssse3 tdt x2apic Hardware topology: Full, with devices Packages, cores, and logical processors: [indices refer to OS logical processors] Package 0: [ 0 1] [ 2 3] [ 4 5] [ 6 7] [ 8] [ 9] [ 10] [ 11] [ 12] [ 13] [ 14] [ 15] CPU limit set by OS: -1 Recommended max number of threads: 16 Numa nodes: Node 0 (16172212224 bytes mem): 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Latency: 0 0 1.00 Caches: L1: 49152 bytes, linesize 64 bytes, assoc. 12, shared 2 ways L2: 1310720 bytes, linesize 64 bytes, assoc. 10, shared 2 ways L3: 12582912 bytes, linesize 64 bytes, assoc. 8, shared 16 ways PCI devices: 0000:00:02.0 Id: 8086:46a6 Class: 0x0300 Numa: 0 0000:01:00.0 Id: 1c5c:1959 Class: 0x0108 Numa: 0 0000:00:14.3 Id: 8086:51f0 Class: 0x0280 Numa: 0 Likely fastest SIMD instructions supported by all nodes: AVX2_256 SIMD instructions selected at compile time: SSE4.1 Running on Pelle: Single core, 85 seconds: #SBATCH -A staff #SBATCH --time=00:10:00 # maximum execution time of #SBATCH --ntasks-per-node=1 # 1 MPI rank #SBATCH --cpus-per-task=1 # number cpus-per-task with log: $ head -n 100 ex1.1_1x_jID42696.log :-) GROMACS - gmx mdrun, 2024.4-EasyBuild_5.1.0 (-: Coordinated by the GROMACS project leaders: Berk Hess and Erik Lindahl GROMACS: gmx mdrun, version 2024.4-EasyBuild_5.1.0 Executable: /sw/arch/eb/software/GROMACS/2024.4-foss-2023b/bin/gmx_mpi Data prefix: /sw/arch/eb/software/GROMACS/2024.4-foss-2023b Working dir: /domus/h1/richel Process ID: 193730 Command line: gmx_mpi mdrun -g ex1.1_1x_jID42696 -nsteps -1 -maxh 0.017 -resethway -notunepme GROMACS version: 2024.4-EasyBuild_5.1.0 Precision: mixed Memory model: 64 bit MPI library: MPI MPI library version: Open MPI v4.1.6, package: Open MPI iusan@p204.uppmax.uu.se Distribution, ident: 4.1.6, repo rev: v4.1.6, Sep 30, 2023 OpenMP support: enabled (GMX_OPENMP_MAX_THREADS = 128) GPU support: disabled SIMD instructions: AVX_512 CPU FFT library: fftw-3.3.10-sse2-avx-avx2-avx2_128 GPU FFT library: none Multi-GPU FFT: none RDTSCP usage: enabled TNG support: enabled Hwloc support: disabled Tracing support: disabled C compiler: /sw/arch/eb/software/OpenMPI/4.1.6-GCC-13.2.0/bin/mpicc GNU 13.2.0 C compiler flags: -fexcess-precision=fast -funroll-all-loops -march=skylake-avx512 -Wno-missing-field-initializers -O3 -DNDEBUG C++ compiler: /sw/arch/eb/software/OpenMPI/4.1.6-GCC-13.2.0/bin/mpicxx GNU 13.2.0 C++ compiler flags: -fexcess-precision=fast -funroll-all-loops -march=skylake-avx512 -Wno-missing-field-initializers -Wno-cast-function-type-strict SHELL:-fopenmp -O3 -DNDEBUG BLAS library: External - user-supplied LAPACK library: External - user-supplied Running on 1 node with total 1 cores, 2 processing units Hardware detected on host p62.uppmax.uu.se (the node of MPI rank 0): CPU info: Vendor: AMD Brand: AMD EPYC 9454P 48-Core Processor Family: 25 Model: 17 Stepping: 1 Features: aes amd apic avx avx2 avx512f avx512cd avx512bw avx512vl avx512bf16 avx512secondFMA clfsh cmov cx8 cx16 f16c fma htt lahf misalignsse mmx msr nonstop_tsc pcid pclmuldq pdpe1gb popcnt pse rdrnd rdtscp sha sse2 sse3 sse4a sse4.1 sse4.2 ssse3 x2apic Hardware topology: Basic Packages, cores, and logical processors: [indices refer to OS logical processors] Package 0: [ 32 80] CPU limit set by OS: -1 Recommended max number of threads: 2","title":"LUMI course"},{"location":"matlab/","text":"MATLAB content \u00b6 Introduction to MATLAB Load and run MATLAB Add-Ons/Toolbox Parallel jobs Pool Matlab batch (Shell Batch) On-Demand desktop At UPPMAX just matlab batch works Interaction with other tools? Comsol Gurobi","title":"MATLAB content"},{"location":"matlab/#matlab-content","text":"Introduction to MATLAB Load and run MATLAB Add-Ons/Toolbox Parallel jobs Pool Matlab batch (Shell Batch) On-Demand desktop At UPPMAX just matlab batch works Interaction with other tools? Comsol Gurobi","title":"MATLAB content"},{"location":"matlab/MatlabGUIslurm/","text":"MATLAB GUI and SLURM \u00b6 Here we discuss how MATLAB interacts with SLURM. Objectives Understand and use the Slurm scheduler in MATLAB Start batch jobs from MATLAB Graphical User Interface (GUI) Try example MATLAB is well integrated with SLURM and because of that there are several options to run these jobs: Writing a batch script as for any other software and submitting the job with the sbatch command from SLURM (This could be useful if you want to run long jobs, and you don\u2019t need to modify the code in the meantime). You have seen this in the previous section. Using the job scheduler ( batch command) in MATLAB graphical user interface (GUI) (This is the Recommended Use). Starting a parpool in the MATLAB GUI with a predefined cluster (This allows for more interactivity). In the following sections we will extend the last two options. Important Compute allocations in this workshop Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-062 Cosmos: lu2025-7-37 Tetralith: naiss-2025-22-262 Dardel: naiss-2025-22-262 Storage space for this workshop Rackham: /proj/r-py-jl-m-rackham Kebnekaise: /proj/nobackup/r-py-jl-m Cosmos: your home directory should have plenty of space Tetralith: /proj/r-matlab-julia-naiss/users/ Dardel: /cfs/klemming/projects/snic/r-matlab-julia-naiss Warning Any longer, resource-intensive, or parallel jobs must be run through a batch script . On login nodes, MATLAB should be started with the option -singleCompThread to stop it from using more than one thread (a couple of the clusters detect if the user is on a login node and restrict MATLAB to 1 thread automatically, but it\u2019s better to include it to be safe than forget and have your job killed by angry admins). On some clusters (e.g. COSMOS, Kebnekaise, Dardel), it is possible, and therefore recommended, to start the MATLAB GUI itself on a compute node. MATLAB Desktop/graphical interface \u00b6 Jobs can be submitted to the SLURM queue directly from the the MATLAB GUI as an alternative to the standard bash scripts that are used with the command sbatch my-script.sh , for instance. MATLAB GUI To submit a job from the GUI, you will need to create a handle for the cluster and then use this handle to send the job and control the outputs: % Get a handle to the cluster c = parcluster ( 'name-of-your-cluster' ) % Run the job on CPU j = c . batch (@ myfunction , N_out_values , { input1 , input2 , ... }, 'pool', N_workers') % alternatively, j=batch(c, @myfunction, N_out_values, {input1, input2, ...}, 'pool', N_workers') % Wait until the job has finished. Use j.State if you just want to poll the % status and be able to do other things while waiting for the job to finish. j . wait % Fetch the result after the job has finished j . fetchOutputs {:} Note that batch also accepts script names in place of function names, but these must be given in single quotes, with no @ or .m . This is useful if your script is a job farm. In the earlier session on using MATLAB with Slurm at the command line , you learned how to set/edit slurm parameters for jobs on a given cluster, after defining a handle for the cluster, by calling the AdditionalProperties attribute. Below we will discuss an alternative way to do that graphically. Job settings in the Cluster Profile Manager \u00b6 Tip You can change the job settings (or make them all together) inside the GUI. To do that, you change the job settings within the Cluster Profile Manager. Note that this is ONLY if you want to use the GUI. You can work completely from within the MATLAB terminal interface if you want. If you run MATLAB in the GUI after having configured the cluster, MATLAB will start with a default cluster profile, typically something that includes the name of the cluster. This is just the set of configurations that were set by configCluster . You can view, edit, and/or add to this profile by clicking the Parallel menu icon and selecting Create and Manage Clusters . Location of Parallel Menu in GUI. Cluster Profile Manager. If you scroll down in the window that appears when you select the right cluster, you will see a box titled Scheduler Plugin . This box lets you set SBATCH parameters like Your account name (project name), Your email address, The memory per CPU, including units, The number of processes per node, Which partition you want, Whether you need an exclusive node, The name of your reservation, and most importantly, The wall time for your job. Editing parameters of Scheduler Plugin in Cluster Profile Manager. In other words, almost anything you might otherwise set by calling c.AdditionalProperties.<insert_property>=... can be set in the GUI in this scheduler plugin. Note The settings in the Scheduler Plugin for any given cluster profile are saved between sessions. Always check them before running. If you are on Desktop On Demand on LUNARC, these settings do not override the parameters set in the GfxLauncher for the MATLAB GUI session itself, but rather to any batch jobs you submit from within the GUI. MATLAB and SLURM \u00b6 Extensive jobs must be run through SLURM. There are different manners to run MATLAB jobs with SLURM, for instance using: using the MATLAB GUI ThinLinc (LUNARC, UPPMAX, HPC2N) interactive ( salloc / interactive ) sessions Desktop On Demand (LUNARC) Open onDemand (HPC2N). batch scripts in a SSH session submitted with sbatch (less recommended) Serial jobs \u00b6 As an example consider the following serial function hostnm that is in a file called hostnm.m which gets the name of the host machine as an output: function hn = hostnm () hn = getenv ( 'HOSTNAME' ); end We can send a job to the queue which executes this function and retrieving/printing out the results as follows: c = parcluster ( 'name-of-your-cluster' ); j = c . batch (@ hostnm , 1 ,{}, 'pool' , 1 ); j . wait ; t = j . fetchOutputs {:}; fprintf ( 'Name of host: %s \\n' , t ); Parallel jobs \u00b6 Jobs can be parallelized in MATLAB using functionalities such as parfor , spmd , and parfeval . parfor \u00b6 This function will assist you if you want to parallelize a for loop . Although it will be performant, it imposes some constraints on the loops: The number of iterations must be well-defined, There can be no control over the individual workers, and There must be no data dependencies between the iterations. In the following example the name of the host machine will be printed n number of times and this number will be divided across the available number of workers: parfor i = 1 : 4 disp ( getenv ( \"HOSTNAME\" )) end spmd \u00b6 Single program multiple data (SPMD) is supported in MATLAB through the spmd functionality, here you enclose the code that will be executed by some workers independently. The workers are labeled with the variable labindex that can be used to control the workload of each worker. In the following example the name of the host will be displayed as many times as the present number of workers: spmd A = labindex ; % label for each worker disp ( getenv ( \"HOSTNAME\" )) % display the name of the host end parfeval \u00b6 This function is more advanced than the previous two and it allows you to do asynchronous calculations, which means that those calculations can start when resources are available but the execution order is not needed. The results can be fetched once the simulation finishes. f = parfeval (@ myFunction , 'nr. of outputs' , 'list of input arguments' ); results = fetchOutputs ( f ); Running parallel jobs \u00b6 Parallel jobs which include functions like parfor , spmd , and parfeval can be handled in two ways in the MATLAB GUI, either by using the batch command (we mentioned above for serial jobs) or by creating a parpool . Using batch \u00b6 It is recommended that you enclose the parallel code into a function and place it into a MATLAB script. In the parfor example mentioned above, we can write a script called hostnmp.m containing this code: function hn_all = hostnmp ( n ) hn_all = []; parfor i = 1 : n hn = ( getenv ( 'HOSTNAME' )); hn_all = [ hn_all , hn ]; % This array stores the host names for each worker end end Then, in the MATLAB GUI I can execute this function and retrieve/print out the results as follows: c = parcluster ( 'name-of-your-cluster' ); j = c . batch (@ hostnmp , 'nr. outputs' ,{ 'list of input args' }, 'pool' , 'nr. workers' ); j . wait ; % wait for the results t = j . fetchOutputs {:}; % fetch the results fprintf ( 'Name of host: %s \\n' , t ); % Print out the results Notice that if you will use this sequence of commands to launch many jobs, it will be convenient to write a MATLAB script so that next time you have these commands at hand. Creating a parpool \u00b6 This option is especially useful if you are working in an Open onDemand session because you are already working on the computing node. If you are doing continuous modifications to your code and running it to make sure that it works, using a parpool could be a better option than the batch command. Here, you create a pool of workers with the parpool function that are available to run parallel functions such as those mentioned above ( parfor , spmd , and parfeval ) until this pool is deleted. Warning Notice that if you run a serial function (that maybe consumes 100% of the CPU) inside a parpool block, this function will be executed on the local machine (maybe the login node) and not on a compute node. In the following example a pool of n workers is created that will solve a parfor loop which will display the host name: % Use parallel pool with 'parfor' parpool ( 'name-of-your-cluster' , n ); % Start parallel pool with nworkers = n workers p = gcp ; parfor i = 1 : n disp ( getenv ( \"HOSTNAME\" )) end % Clean up the parallel pool delete ( gcp ( 'nocreate' )); Notice that the host name displayed is the one where the job ran not where the MATLAB GUI is running. All parallel functionalities in MATLAB can be executed inside a parpool . If you are running the MATLAB GUI in an Open onDemand session you can use Processes as the name-of-your-cluster , as this cluster profile is used for running jobs locally. Exercises \u00b6 Challenge 1. Create and run a parallel code We have the following code in MATLAB that generates an array of 10000 random numbers and then the sum of all elements is stored in a variable called s : r = rand ( 1 , 10000 ); s = sum ( r ); We want now to repeat these steps (generating the numbers and taking the sum) 6 times so that the steps are run at the same time. Use parfor to parallelize these steps. Once your code is parallelized enclose it in a parpool section and send the job to the queue. Solution % Nr. of workers nworkers = 6 ; % Use parallel pool with 'parfor' parpool ( 'name-of-your-cluster' , nworkers ); % Start parallel pool with nworkers workers myarray = []; % Optional in this exercise to store partial results parfor i = 1 : nworkers r = rand ( 1 , 10000 ); s = sum ( r ); myarray = [ myarray , s ]; end myarray % print out the results from the workers % Clean up the parallel pool delete ( gcp ( 'nocreate' )); Challenge 2. Run a parallel code with batch MATLAB function The following function uses parfeval to do some computation (specifically it takes the average per-column of a matrix with a size nsize equal to 1000): function results = parfeval_mean ( nsize ) results = parfeval (@ mean , 1 , rand ( nsize )) end Place this function in a file called parfeval_mean.m and submit this function with the MATLAB batch command. Solution c = parcluster ( 'name-of-your-cluster' ); j = c . batch (@ parfeval_mean , 1 ,{ 1000 }, 'pool' , 1 ); j . wait ; % wait for the results t = j . fetchOutputs {:}; % fetch the results fprintf ( 'Name of host: %.5f \\n' , t ); % Print out the results Summary The SLURM scheduler handles allocations to the calculation nodes MATLAB has good integration with SLURM and because of that one can submit jobs to the queue directly from the GUI. MATLAB has several tools to parallelize your code and we have explored here parfor , spmd , and parfeval , but there are other tools available .","title":"MATLAB GUI and batch jobs"},{"location":"matlab/MatlabGUIslurm/#matlab-gui-and-slurm","text":"Here we discuss how MATLAB interacts with SLURM. Objectives Understand and use the Slurm scheduler in MATLAB Start batch jobs from MATLAB Graphical User Interface (GUI) Try example MATLAB is well integrated with SLURM and because of that there are several options to run these jobs: Writing a batch script as for any other software and submitting the job with the sbatch command from SLURM (This could be useful if you want to run long jobs, and you don\u2019t need to modify the code in the meantime). You have seen this in the previous section. Using the job scheduler ( batch command) in MATLAB graphical user interface (GUI) (This is the Recommended Use). Starting a parpool in the MATLAB GUI with a predefined cluster (This allows for more interactivity). In the following sections we will extend the last two options. Important Compute allocations in this workshop Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-062 Cosmos: lu2025-7-37 Tetralith: naiss-2025-22-262 Dardel: naiss-2025-22-262 Storage space for this workshop Rackham: /proj/r-py-jl-m-rackham Kebnekaise: /proj/nobackup/r-py-jl-m Cosmos: your home directory should have plenty of space Tetralith: /proj/r-matlab-julia-naiss/users/ Dardel: /cfs/klemming/projects/snic/r-matlab-julia-naiss Warning Any longer, resource-intensive, or parallel jobs must be run through a batch script . On login nodes, MATLAB should be started with the option -singleCompThread to stop it from using more than one thread (a couple of the clusters detect if the user is on a login node and restrict MATLAB to 1 thread automatically, but it\u2019s better to include it to be safe than forget and have your job killed by angry admins). On some clusters (e.g. COSMOS, Kebnekaise, Dardel), it is possible, and therefore recommended, to start the MATLAB GUI itself on a compute node.","title":"MATLAB GUI and SLURM"},{"location":"matlab/MatlabGUIslurm/#matlab-desktopgraphical-interface","text":"Jobs can be submitted to the SLURM queue directly from the the MATLAB GUI as an alternative to the standard bash scripts that are used with the command sbatch my-script.sh , for instance. MATLAB GUI To submit a job from the GUI, you will need to create a handle for the cluster and then use this handle to send the job and control the outputs: % Get a handle to the cluster c = parcluster ( 'name-of-your-cluster' ) % Run the job on CPU j = c . batch (@ myfunction , N_out_values , { input1 , input2 , ... }, 'pool', N_workers') % alternatively, j=batch(c, @myfunction, N_out_values, {input1, input2, ...}, 'pool', N_workers') % Wait until the job has finished. Use j.State if you just want to poll the % status and be able to do other things while waiting for the job to finish. j . wait % Fetch the result after the job has finished j . fetchOutputs {:} Note that batch also accepts script names in place of function names, but these must be given in single quotes, with no @ or .m . This is useful if your script is a job farm. In the earlier session on using MATLAB with Slurm at the command line , you learned how to set/edit slurm parameters for jobs on a given cluster, after defining a handle for the cluster, by calling the AdditionalProperties attribute. Below we will discuss an alternative way to do that graphically.","title":"MATLAB Desktop/graphical interface"},{"location":"matlab/MatlabGUIslurm/#job-settings-in-the-cluster-profile-manager","text":"Tip You can change the job settings (or make them all together) inside the GUI. To do that, you change the job settings within the Cluster Profile Manager. Note that this is ONLY if you want to use the GUI. You can work completely from within the MATLAB terminal interface if you want. If you run MATLAB in the GUI after having configured the cluster, MATLAB will start with a default cluster profile, typically something that includes the name of the cluster. This is just the set of configurations that were set by configCluster . You can view, edit, and/or add to this profile by clicking the Parallel menu icon and selecting Create and Manage Clusters . Location of Parallel Menu in GUI. Cluster Profile Manager. If you scroll down in the window that appears when you select the right cluster, you will see a box titled Scheduler Plugin . This box lets you set SBATCH parameters like Your account name (project name), Your email address, The memory per CPU, including units, The number of processes per node, Which partition you want, Whether you need an exclusive node, The name of your reservation, and most importantly, The wall time for your job. Editing parameters of Scheduler Plugin in Cluster Profile Manager. In other words, almost anything you might otherwise set by calling c.AdditionalProperties.<insert_property>=... can be set in the GUI in this scheduler plugin. Note The settings in the Scheduler Plugin for any given cluster profile are saved between sessions. Always check them before running. If you are on Desktop On Demand on LUNARC, these settings do not override the parameters set in the GfxLauncher for the MATLAB GUI session itself, but rather to any batch jobs you submit from within the GUI.","title":"Job settings in the Cluster Profile Manager"},{"location":"matlab/MatlabGUIslurm/#matlab-and-slurm","text":"Extensive jobs must be run through SLURM. There are different manners to run MATLAB jobs with SLURM, for instance using: using the MATLAB GUI ThinLinc (LUNARC, UPPMAX, HPC2N) interactive ( salloc / interactive ) sessions Desktop On Demand (LUNARC) Open onDemand (HPC2N). batch scripts in a SSH session submitted with sbatch (less recommended)","title":"MATLAB and SLURM"},{"location":"matlab/MatlabGUIslurm/#serial-jobs","text":"As an example consider the following serial function hostnm that is in a file called hostnm.m which gets the name of the host machine as an output: function hn = hostnm () hn = getenv ( 'HOSTNAME' ); end We can send a job to the queue which executes this function and retrieving/printing out the results as follows: c = parcluster ( 'name-of-your-cluster' ); j = c . batch (@ hostnm , 1 ,{}, 'pool' , 1 ); j . wait ; t = j . fetchOutputs {:}; fprintf ( 'Name of host: %s \\n' , t );","title":"Serial jobs"},{"location":"matlab/MatlabGUIslurm/#parallel-jobs","text":"Jobs can be parallelized in MATLAB using functionalities such as parfor , spmd , and parfeval .","title":"Parallel jobs"},{"location":"matlab/MatlabGUIslurm/#parfor","text":"This function will assist you if you want to parallelize a for loop . Although it will be performant, it imposes some constraints on the loops: The number of iterations must be well-defined, There can be no control over the individual workers, and There must be no data dependencies between the iterations. In the following example the name of the host machine will be printed n number of times and this number will be divided across the available number of workers: parfor i = 1 : 4 disp ( getenv ( \"HOSTNAME\" )) end","title":"parfor"},{"location":"matlab/MatlabGUIslurm/#spmd","text":"Single program multiple data (SPMD) is supported in MATLAB through the spmd functionality, here you enclose the code that will be executed by some workers independently. The workers are labeled with the variable labindex that can be used to control the workload of each worker. In the following example the name of the host will be displayed as many times as the present number of workers: spmd A = labindex ; % label for each worker disp ( getenv ( \"HOSTNAME\" )) % display the name of the host end","title":"spmd"},{"location":"matlab/MatlabGUIslurm/#parfeval","text":"This function is more advanced than the previous two and it allows you to do asynchronous calculations, which means that those calculations can start when resources are available but the execution order is not needed. The results can be fetched once the simulation finishes. f = parfeval (@ myFunction , 'nr. of outputs' , 'list of input arguments' ); results = fetchOutputs ( f );","title":"parfeval"},{"location":"matlab/MatlabGUIslurm/#running-parallel-jobs","text":"Parallel jobs which include functions like parfor , spmd , and parfeval can be handled in two ways in the MATLAB GUI, either by using the batch command (we mentioned above for serial jobs) or by creating a parpool .","title":"Running parallel jobs"},{"location":"matlab/MatlabGUIslurm/#using-batch","text":"It is recommended that you enclose the parallel code into a function and place it into a MATLAB script. In the parfor example mentioned above, we can write a script called hostnmp.m containing this code: function hn_all = hostnmp ( n ) hn_all = []; parfor i = 1 : n hn = ( getenv ( 'HOSTNAME' )); hn_all = [ hn_all , hn ]; % This array stores the host names for each worker end end Then, in the MATLAB GUI I can execute this function and retrieve/print out the results as follows: c = parcluster ( 'name-of-your-cluster' ); j = c . batch (@ hostnmp , 'nr. outputs' ,{ 'list of input args' }, 'pool' , 'nr. workers' ); j . wait ; % wait for the results t = j . fetchOutputs {:}; % fetch the results fprintf ( 'Name of host: %s \\n' , t ); % Print out the results Notice that if you will use this sequence of commands to launch many jobs, it will be convenient to write a MATLAB script so that next time you have these commands at hand.","title":"Using batch"},{"location":"matlab/MatlabGUIslurm/#creating-a-parpool","text":"This option is especially useful if you are working in an Open onDemand session because you are already working on the computing node. If you are doing continuous modifications to your code and running it to make sure that it works, using a parpool could be a better option than the batch command. Here, you create a pool of workers with the parpool function that are available to run parallel functions such as those mentioned above ( parfor , spmd , and parfeval ) until this pool is deleted. Warning Notice that if you run a serial function (that maybe consumes 100% of the CPU) inside a parpool block, this function will be executed on the local machine (maybe the login node) and not on a compute node. In the following example a pool of n workers is created that will solve a parfor loop which will display the host name: % Use parallel pool with 'parfor' parpool ( 'name-of-your-cluster' , n ); % Start parallel pool with nworkers = n workers p = gcp ; parfor i = 1 : n disp ( getenv ( \"HOSTNAME\" )) end % Clean up the parallel pool delete ( gcp ( 'nocreate' )); Notice that the host name displayed is the one where the job ran not where the MATLAB GUI is running. All parallel functionalities in MATLAB can be executed inside a parpool . If you are running the MATLAB GUI in an Open onDemand session you can use Processes as the name-of-your-cluster , as this cluster profile is used for running jobs locally.","title":"Creating a parpool"},{"location":"matlab/MatlabGUIslurm/#exercises","text":"Challenge 1. Create and run a parallel code We have the following code in MATLAB that generates an array of 10000 random numbers and then the sum of all elements is stored in a variable called s : r = rand ( 1 , 10000 ); s = sum ( r ); We want now to repeat these steps (generating the numbers and taking the sum) 6 times so that the steps are run at the same time. Use parfor to parallelize these steps. Once your code is parallelized enclose it in a parpool section and send the job to the queue. Solution % Nr. of workers nworkers = 6 ; % Use parallel pool with 'parfor' parpool ( 'name-of-your-cluster' , nworkers ); % Start parallel pool with nworkers workers myarray = []; % Optional in this exercise to store partial results parfor i = 1 : nworkers r = rand ( 1 , 10000 ); s = sum ( r ); myarray = [ myarray , s ]; end myarray % print out the results from the workers % Clean up the parallel pool delete ( gcp ( 'nocreate' )); Challenge 2. Run a parallel code with batch MATLAB function The following function uses parfeval to do some computation (specifically it takes the average per-column of a matrix with a size nsize equal to 1000): function results = parfeval_mean ( nsize ) results = parfeval (@ mean , 1 , rand ( nsize )) end Place this function in a file called parfeval_mean.m and submit this function with the MATLAB batch command. Solution c = parcluster ( 'name-of-your-cluster' ); j = c . batch (@ parfeval_mean , 1 ,{ 1000 }, 'pool' , 1 ); j . wait ; % wait for the results t = j . fetchOutputs {:}; % fetch the results fprintf ( 'Name of host: %.5f \\n' , t ); % Print out the results Summary The SLURM scheduler handles allocations to the calculation nodes MATLAB has good integration with SLURM and because of that one can submit jobs to the queue directly from the GUI. MATLAB has several tools to parallelize your code and we have explored here parfor , spmd , and parfeval , but there are other tools available .","title":"Exercises"},{"location":"matlab/evaluation-matlab/","text":"Evaluation \u00b6 The evaluation form for the Matlab part can be found here . It takes into account that one may need to leave early too. What is in the form? \u00b6 Click here to preview evaluation form Introduction to running R, Python, Julia, and Matlab in HPC, 6-8/10/2025 - DAY 2 Matlab Thanks for your feedback. This feedback will be published as-is at the end of the evaluation period (after 1 November 2024), if and only if there are no personal details (email, address, etc.) in the feedback. Do mention the teachers, assistants, etc by name! 1.Overall, how would you rate today\u2019s training event? Value from 1 through 10 2.Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): What did you like best? Open question 3.Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): Where should we improve? Open question 4.Training event organisation (e.g. announcement, registration, \u2026): What did you like best? Where should we improve? Open question 5.Length of teaching today was Adequate Too short Too long 6.Depth of content was Adequate Too superficial Too detailed 7.The pace of teaching was Adequate Too slow Too fast 8.Teaching aids used (e.g. slides) were well prepared Agree completely Agree No strong feelings Disagree Disagree completely 9.Hands-on exercises and demonstrations were Adequate Too few Too many 10.Hands-on exercises and demonstrations were well prepared Agree completely Agree No strong feelings Disagree Disagree completely How would you rate the separate sessions? Introduction Load and run Add-ons SLURM and MATLAB without GUI MATLAB GUI and SLURM Parallel MATLAB client on your desktop Matlab in Jupyter With answers Poor Fair Good Very good Excellent Did not attend Give you confidence levels of the following statements, using this scale: 0: I don\u2019t know even what this is about \u2026? 1: I have no confidence I can do this 2: I have low confidence I can do this 3: I have some confidence I can do this 4: I have good confidence I can do this 5: I absolutely can do this! Give you confidence levels of the following statements below: I can use the module system to load a specific version of Matlab I can run Matlab on the terminal I can configure the cluster I can can add job settings needed to run jobs from MAtlab I can use the IPython interpreter I can start the Matlab GUI I can work in the Matlab terminal interface I can submit jobs from inside the Matlab terminal interface I can submit jobs from inside the Matlab GUI I can write and submit a Matlab batch script I can use the Matlab client on the desktop I can check that I am in an interactive session I can work with Matlab in parallel I can start run Matlab in Jupyter I can view add-ons and toolboxes I can install add-ons 13.Did today\u2019s course meet your expectation? Yes No Not sure 14.Which future training topics would you like to be provided by the training host(s)? Open question 15.Do you have any additional comments? Open question","title":"Evaluation"},{"location":"matlab/evaluation-matlab/#evaluation","text":"The evaluation form for the Matlab part can be found here . It takes into account that one may need to leave early too.","title":"Evaluation"},{"location":"matlab/evaluation-matlab/#what-is-in-the-form","text":"Click here to preview evaluation form Introduction to running R, Python, Julia, and Matlab in HPC, 6-8/10/2025 - DAY 2 Matlab Thanks for your feedback. This feedback will be published as-is at the end of the evaluation period (after 1 November 2024), if and only if there are no personal details (email, address, etc.) in the feedback. Do mention the teachers, assistants, etc by name! 1.Overall, how would you rate today\u2019s training event? Value from 1 through 10 2.Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): What did you like best? Open question 3.Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): Where should we improve? Open question 4.Training event organisation (e.g. announcement, registration, \u2026): What did you like best? Where should we improve? Open question 5.Length of teaching today was Adequate Too short Too long 6.Depth of content was Adequate Too superficial Too detailed 7.The pace of teaching was Adequate Too slow Too fast 8.Teaching aids used (e.g. slides) were well prepared Agree completely Agree No strong feelings Disagree Disagree completely 9.Hands-on exercises and demonstrations were Adequate Too few Too many 10.Hands-on exercises and demonstrations were well prepared Agree completely Agree No strong feelings Disagree Disagree completely How would you rate the separate sessions? Introduction Load and run Add-ons SLURM and MATLAB without GUI MATLAB GUI and SLURM Parallel MATLAB client on your desktop Matlab in Jupyter With answers Poor Fair Good Very good Excellent Did not attend Give you confidence levels of the following statements, using this scale: 0: I don\u2019t know even what this is about \u2026? 1: I have no confidence I can do this 2: I have low confidence I can do this 3: I have some confidence I can do this 4: I have good confidence I can do this 5: I absolutely can do this! Give you confidence levels of the following statements below: I can use the module system to load a specific version of Matlab I can run Matlab on the terminal I can configure the cluster I can can add job settings needed to run jobs from MAtlab I can use the IPython interpreter I can start the Matlab GUI I can work in the Matlab terminal interface I can submit jobs from inside the Matlab terminal interface I can submit jobs from inside the Matlab GUI I can write and submit a Matlab batch script I can use the Matlab client on the desktop I can check that I am in an interactive session I can work with Matlab in parallel I can start run Matlab in Jupyter I can view add-ons and toolboxes I can install add-ons 13.Did today\u2019s course meet your expectation? Yes No Not sure 14.Which future training topics would you like to be provided by the training host(s)? Open question 15.Do you have any additional comments? Open question","title":"What is in the form?"},{"location":"matlab/extra/","text":"Matlab - extra reading and links \u00b6 Documentation \u00b6 Documentation at the HPC centres UPPMAX, HPC2N, LUNARC, NSC, and PDC UPPMAX HPC2N: Matlab and Parallel Matlab LUNARC NSC: general instructions and installations on Tetralith specifically PDC Official MATLAB documentation Courses - improving your programming skills \u00b6 If you have a Mathworks account that is less than 2 years old, Mathworks offers free MATLAB self-paced online training courses . Students at any academic institution with a campus-wide license can use their university email addresses to create a free account to access these resources. The Mondays with MATLAB lecture series is offered every September, and offers introductions to MATLAB, its Parallel Computing Toolbox, and AI/ML toolboxes. These and other events are posted at Mathworks events . If you have an account, you will be automatically signed up for the Mathworks mailing list, which will notify you of upcoming webinars as well as some featured packages. The webinars cover a broad range of topics and disciplines at varying skill levels, although these seminars tend to be more advanced. Debugging \u00b6 If a serial job produces an error, call the getDebugLog method to view the error log file. When submitting an independent job, specify the task. >> c . getDebugLog ( job . Tasks ) For Pool jobs, only specify the job object. >> c . getDebugLog ( job ) When troubleshooting a job, the cluster admin may request the scheduler ID of the job. This can be derived by calling getTaskSchedulerIDs (call schedID(job) before R2019b). >> job . getTaskSchedulerIDs () ans = 25539 Parallel computing \u00b6 To learn more about the MATLAB Parallel Computing Toolbox, check out these resources: Parallel Computing Coding Examples Parallel Computing Documentation Parallel Computing Overview Parallel Computing Tutorials Parallel Computing Videos Parallel Computing Webinars Interaction with other tools \u00b6 Coming? Add-ons \u00b6 Some toolboxes Matlab products Parallel Computing Toolbox MATLAB Parallel Server Deep Learning Toolbox Statistics and Machine Learning Toolbox Simulink Stateflow SimEvents Simscape Some toolboxes provides GUI for their tools Apps Matlab products Deep Network Designer - Design and visualize deep learning networks Deep Network Designer Curve Fitter - Fit curves and surfaces to data Deep Learning Toolbox Statistics and Machine Learning Toolbox Simulink Stateflow SimEvents Simscape","title":"More about Matlab"},{"location":"matlab/extra/#matlab-extra-reading-and-links","text":"","title":"Matlab - extra reading and links"},{"location":"matlab/extra/#documentation","text":"Documentation at the HPC centres UPPMAX, HPC2N, LUNARC, NSC, and PDC UPPMAX HPC2N: Matlab and Parallel Matlab LUNARC NSC: general instructions and installations on Tetralith specifically PDC Official MATLAB documentation","title":"Documentation"},{"location":"matlab/extra/#courses-improving-your-programming-skills","text":"If you have a Mathworks account that is less than 2 years old, Mathworks offers free MATLAB self-paced online training courses . Students at any academic institution with a campus-wide license can use their university email addresses to create a free account to access these resources. The Mondays with MATLAB lecture series is offered every September, and offers introductions to MATLAB, its Parallel Computing Toolbox, and AI/ML toolboxes. These and other events are posted at Mathworks events . If you have an account, you will be automatically signed up for the Mathworks mailing list, which will notify you of upcoming webinars as well as some featured packages. The webinars cover a broad range of topics and disciplines at varying skill levels, although these seminars tend to be more advanced.","title":"Courses - improving your programming skills"},{"location":"matlab/extra/#debugging","text":"If a serial job produces an error, call the getDebugLog method to view the error log file. When submitting an independent job, specify the task. >> c . getDebugLog ( job . Tasks ) For Pool jobs, only specify the job object. >> c . getDebugLog ( job ) When troubleshooting a job, the cluster admin may request the scheduler ID of the job. This can be derived by calling getTaskSchedulerIDs (call schedID(job) before R2019b). >> job . getTaskSchedulerIDs () ans = 25539","title":"Debugging"},{"location":"matlab/extra/#parallel-computing","text":"To learn more about the MATLAB Parallel Computing Toolbox, check out these resources: Parallel Computing Coding Examples Parallel Computing Documentation Parallel Computing Overview Parallel Computing Tutorials Parallel Computing Videos Parallel Computing Webinars","title":"Parallel computing"},{"location":"matlab/extra/#interaction-with-other-tools","text":"Coming?","title":"Interaction with other tools"},{"location":"matlab/extra/#add-ons","text":"Some toolboxes Matlab products Parallel Computing Toolbox MATLAB Parallel Server Deep Learning Toolbox Statistics and Machine Learning Toolbox Simulink Stateflow SimEvents Simscape Some toolboxes provides GUI for their tools Apps Matlab products Deep Network Designer - Design and visualize deep learning networks Deep Network Designer Curve Fitter - Fit curves and surfaces to data Deep Learning Toolbox Statistics and Machine Learning Toolbox Simulink Stateflow SimEvents Simscape","title":"Add-ons"},{"location":"matlab/interactive/","text":"Matlab interactively \u00b6 Learning outcomes for today Be able to start interactive sessions with several cores Be able to run MATLAB and seae the available workers Instructor note Intro 5 min Lecture and 10 min Notes It is possible to run MATLAB directly on the login (including ThinLinc) nodes. But this should only be done for shorter jobs or jobs that do not use a lot of resources, as the login nodes can otherwise become slow for all users. If you want to work interactively with your code or data, you should start an interactive session. If you rather will run a script which won\u2019t use any interactive user input while running, you can instead start a batch job, see next session. General \u00b6 In order to run interactively with more memory or more threads/processes, you need to have compute nodes allocated to run on, and this is done through the Slurm system. Interactive sessions at NSC, PDC, HPC2N, UPPMAX and LUNARC \u00b6 Here we define an interactive session as a session with direct access to a compute node. Or alternatively: an interactive session is a session, in which there is no queue before a command is run on a compute node. This differs between HPC2N and UPPMAX : HPC2N: the user remains on a login node. All commands can be sent directly to the compute node using srun UPPMAX: the user is actually on a computer node. Whatever command is done, it is run on the compute node LUNARC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node NSC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node PDC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node Start an interactive session \u00b6 To start an interactive session, one needs to allocate resources on the cluster first. The command to request an interactive node differs per HPC cluster: Cluster interactive salloc GfxLauncher Open OnDemand HPC2N Works Recommended N/A Recommended UPPMAX Recommended Works N/A N/A LUNARC Works N/A Recommended N/A NSC Recommended N/A N/A N/A PDC N/A Recommended Possible N/A Example, HPC2N vs. UPPMAX (also valid for NSC, PDC and LUNARC): graph TD subgraph uppmax[UPPMAX] subgraph login_node[Login nodes] user_on_login_node[User on login node] end subgraph compute_node[Compute nodes] user_on_computer_node[User on compute node] job_on_compute_node[Job on compute node] end end subgraph hpc2n[HPC2N] subgraph hpc2n_login_node[Login nodes] hpc2n_user_on_login_node[User on login node] hpc2n_user_in_interactive_mode[User on login node in interactive session] end subgraph hpc2n_compute_node[Compute nodes] hpcn2_job_on_compute_node[Job on compute node] end end user_on_login_node --> |interactive| user_on_computer_node user_on_login_node --> |sbatch| job_on_compute_node user_on_computer_node --> |exit| user_on_login_node user_on_computer_node --> |srun| user_on_computer_node hpc2n_user_on_login_node --> |salloc| hpc2n_user_in_interactive_mode hpc2n_user_in_interactive_mode --> |exit| hpc2n_user_on_login_node hpc2n_user_on_login_node --> |sbatch| hpcn2_job_on_compute_node hpc2n_user_in_interactive_mode --> |srun| hpcn2_job_on_compute_node First, you make a request for resources with interactive / salloc , like this: Interactive jobs Short serial example for running on different clusters. NSC PDC UPPMAX LUNARC HPC2N $ interactive -n <tasks> --time = HHH:MM:SS -A naiss2025-22-934 $ salloc -n <ntasks> --time = HHH:MM:SS -A naiss2025-22-934 -p <partition> Where is shared , main or gpu We recommend shared Wait until you get the node ssh to the node given and then work there Example: $ ssh nid001057 $ interactive -n <tasks> --time = HHH:MM:SS -A uppmax2025-2-360 $ interactive -n <tasks> --time = HHH:MM:SS -A lu2025-2-94 $ salloc -n <tasks> --time = HHH:MM:SS -A hpc2n2025-151 where is the number of tasks (or cores, for default 1 task per core), time is given in hours, minutes, and seconds (maximum T168 hours), and then you give the id for your project. Your request enters the job queue just like any other job, and interactive/salloc will tell you that it is waiting for the requested resources. When salloc tells you that your job has been allocated resources, you can interactively run programs on those resources with srun . The commands you run with srun will then be executed on the resources your job has been allocated. On HPC2N If you do not preface with srun the command is run on the login node! You can now run Matlab scripts on the allocated resources directly instead of waiting for your batch job to return a result. This is an advantage if you want to test your Matlab script or perhaps figure out which parameters are best. End an interactive session \u00b6 When you have finished using the allocation, either wait for it to end, or close it with exit $ exit logout Documentation at the centers Interactive allocation on PDC Interactive allocation on NSC Interactive allocation on UPPMAX Interactive allocation on HPC2N Interactive allocation on LUNARC Exercise \u00b6 Run from ThinLinc (web or client!) Log in page Interactive session Requesting 4 cores for 10 minutes, and load Matlab NSC PDC UPPMAX (Pelle) UPPMAX (Bianca) HPC2N LUNARC [ sm_bcarl@tetralith3 ~ ] $ interactive -n 4 -t 0 :30:0 -A naiss2025-22-934 salloc: Pending job allocation 43071298 salloc: job 43071298 queued and waiting for resources salloc: job 43071298 has been allocated resources salloc: Granted job allocation 43071298 salloc: Waiting for resource configuration salloc: Nodes n760 are ready for job [ bjornc@n760 ~ ] $ module load MATLAB/2023b-bdist Let us check that we actually run on the compute node: [ sm_bcarl@n760 ~ ] $ srun hostname n760 n760 n760 n760 We are. Notice that we got a response from all four cores we have allocated. claremar@login1:~> salloc --ntasks = 4 -t 0 :30:00 -p shared --qos = normal -A naiss2025-22-934 salloc: Pending job allocation 9102757 salloc: job 9102757 queued and waiting for resources salloc: job 9102757 has been allocated resources salloc: Granted job allocation 9102757 salloc: Waiting for resource configuration salloc: Nodes nid001057 are ready for job claremar@login1:~> module load PDC/23.12 matlab/r2024b Let us check that we actually run on the compute node. This has to be done differently claremar@login1:~> srun hostname nid001064 nid001063 nid001064 nid001063 Now, it seems that Dardel allows for \u201chyperthreading\u201d, that is 2 threads per core. claremar@login1:~> srun -n 8 hostname nid001064 nid001064 nid001063 nid001063 nid001064 nid001064 nid001063 nid001063 We are. Notice that we got a response from all four cores we have allocated. [ bjornc@pelle ~ ] $ interactive -A uppmax2025-2-360 -n 4 -t 0 :30:00 You receive the high interactive priority. There are free cores, so your job is expected to start at once. Please, use no more than 6 .4 GB of RAM. Waiting for job 29556505 to start... Starting job now -- you waited for 1 second. [ bjornc@p102 ~ ] $ module load MATLAB/2023b-update4 Let us check that we actually run on the compute node: [ bjornc@p102 ~ ] $ srun hostname r483.uppmax.uu.se r483.uppmax.uu.se r483.uppmax.uu.se r483.uppmax.uu.se We are. Notice that we got a response from all four cores we have allocated. [ bjornc@sens2017625-bianca ~ ] $ interactive -A sensXXXX -p core -n 4 -t 0 :30:00 You receive the high interactive priority. There are free cores, so your job is expected to start at once. Please, use no more than 6 .4 GB of RAM. Waiting for job 29556505 to start... Starting job now -- you waited for 1 second. [ bjornc@p102 ~ ] $ module load MATLAB/2023b-update4 Let us check that we actually run on the compute node: [ bjornc@p102 ~ ] $ srun hostname r483.uppmax.uu.se r483.uppmax.uu.se r483.uppmax.uu.se r483.uppmax.uu.se We are. Notice that we got a response from all four cores we have allocated. [ ~ ] $ salloc -n 4 --time = 00 :30:00 -A hpc2n2025-151 salloc: Pending job allocation 20174806 salloc: job 20174806 queued and waiting for resources salloc: job 20174806 has been allocated resources salloc: Granted job allocation 20174806 salloc: Waiting for resource configuration salloc: Nodes b-cn0241 are ready for job [ ~ ] $ module load MATLAB/2023b [ ~ ] $ Let us check that we actually run on the compute node: [ ~ ] $ srun hostname b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se We are. Notice that we got a response from all four cores we have allocated. [ bjornc@cosmos1 ~ ] $ interactive -A lu2025-2-94 -n 4 -t 30 :00 Cluster name: COSMOS Waiting for JOBID 930844 to start [ bjornc@cn050 ~ ] $ module load matlab/2024b Let us check that we actually run on the compute node: [ bjornc@cn050 ~ ] $ echo $SLURM_CPUS_ON_NODE 4 We are, because the $SLURM environment variable gives an output. Notice that we got 4, which is not the size of the physcial node but the allocation size. Check from Matlab Start matlab (same for all clusters) matlab & In MATLAB test how big parpool you can make (should be limited by the allocated resources above) p = parpool ( \"local\" ) p . NumWorkers Output of parpool command on UPPMAX According to the below output Starting parallel pool (parpool) using the 'Processes' profile ... Connected to parallel pool with 4 workers. p = ProcessPool with properties : Connected : true NumWorkers : 4 Busy : false Cluster : Processes ( Local Cluster ) AttachedFiles : {} AutoAddClientPath : true FileStore : [ 1 x1 parallel . FileStore ] ValueStore : [ 1 x1 parallel . ValueStore ] IdleTimeout : 30 minutes ( 30 minutes remaining ) SpmdEnabled : true Quit MATLAB End Matlab by closing the GUI. Exit interactive session When you have finished using the allocation, either wait for it to end, or close it with exit NSC PDC UPPMAX HPC2N LUNARC [ sm_bcarl@n134 ~ ] $ exit logout srun: error: n134: task 0 : Exited with exit code 130 srun: Terminating StepId = 43071803 .interactive salloc: Relinquishing job allocation 43071803 salloc: Job allocation 43071803 has been revoked. [ sm_bcarl@tetralith3 ~ ] $ claremar@login1:~> exit exit salloc: Relinquishing job allocation 9103056 claremar@login1:~> [ bjornc@r483 ~ ] $ exit exit [ screen is terminating ] Connection to r483 closed. [ bjornc@rackham2 ~ ] $ [ ~ ] $ exit exit salloc: Relinquishing job allocation 20174806 salloc: Job allocation 20174806 has been revoked. [ ~ ] $ [ ~ ] $ exit exit [ screen is terminating ] Connection to cn050 closed. [ ~ ] $","title":"Interactive work on the compute nodes"},{"location":"matlab/interactive/#matlab-interactively","text":"Learning outcomes for today Be able to start interactive sessions with several cores Be able to run MATLAB and seae the available workers Instructor note Intro 5 min Lecture and 10 min Notes It is possible to run MATLAB directly on the login (including ThinLinc) nodes. But this should only be done for shorter jobs or jobs that do not use a lot of resources, as the login nodes can otherwise become slow for all users. If you want to work interactively with your code or data, you should start an interactive session. If you rather will run a script which won\u2019t use any interactive user input while running, you can instead start a batch job, see next session.","title":"Matlab interactively"},{"location":"matlab/interactive/#general","text":"In order to run interactively with more memory or more threads/processes, you need to have compute nodes allocated to run on, and this is done through the Slurm system.","title":"General"},{"location":"matlab/interactive/#interactive-sessions-at-nsc-pdc-hpc2n-uppmax-and-lunarc","text":"Here we define an interactive session as a session with direct access to a compute node. Or alternatively: an interactive session is a session, in which there is no queue before a command is run on a compute node. This differs between HPC2N and UPPMAX : HPC2N: the user remains on a login node. All commands can be sent directly to the compute node using srun UPPMAX: the user is actually on a computer node. Whatever command is done, it is run on the compute node LUNARC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node NSC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node PDC: the user is actually on a computer node if the correct menu option is chosen. Whatever command is done, it is run on the compute node","title":"Interactive sessions at NSC, PDC, HPC2N, UPPMAX and LUNARC"},{"location":"matlab/interactive/#start-an-interactive-session","text":"To start an interactive session, one needs to allocate resources on the cluster first. The command to request an interactive node differs per HPC cluster: Cluster interactive salloc GfxLauncher Open OnDemand HPC2N Works Recommended N/A Recommended UPPMAX Recommended Works N/A N/A LUNARC Works N/A Recommended N/A NSC Recommended N/A N/A N/A PDC N/A Recommended Possible N/A Example, HPC2N vs. UPPMAX (also valid for NSC, PDC and LUNARC): graph TD subgraph uppmax[UPPMAX] subgraph login_node[Login nodes] user_on_login_node[User on login node] end subgraph compute_node[Compute nodes] user_on_computer_node[User on compute node] job_on_compute_node[Job on compute node] end end subgraph hpc2n[HPC2N] subgraph hpc2n_login_node[Login nodes] hpc2n_user_on_login_node[User on login node] hpc2n_user_in_interactive_mode[User on login node in interactive session] end subgraph hpc2n_compute_node[Compute nodes] hpcn2_job_on_compute_node[Job on compute node] end end user_on_login_node --> |interactive| user_on_computer_node user_on_login_node --> |sbatch| job_on_compute_node user_on_computer_node --> |exit| user_on_login_node user_on_computer_node --> |srun| user_on_computer_node hpc2n_user_on_login_node --> |salloc| hpc2n_user_in_interactive_mode hpc2n_user_in_interactive_mode --> |exit| hpc2n_user_on_login_node hpc2n_user_on_login_node --> |sbatch| hpcn2_job_on_compute_node hpc2n_user_in_interactive_mode --> |srun| hpcn2_job_on_compute_node First, you make a request for resources with interactive / salloc , like this: Interactive jobs Short serial example for running on different clusters. NSC PDC UPPMAX LUNARC HPC2N $ interactive -n <tasks> --time = HHH:MM:SS -A naiss2025-22-934 $ salloc -n <ntasks> --time = HHH:MM:SS -A naiss2025-22-934 -p <partition> Where is shared , main or gpu We recommend shared Wait until you get the node ssh to the node given and then work there Example: $ ssh nid001057 $ interactive -n <tasks> --time = HHH:MM:SS -A uppmax2025-2-360 $ interactive -n <tasks> --time = HHH:MM:SS -A lu2025-2-94 $ salloc -n <tasks> --time = HHH:MM:SS -A hpc2n2025-151 where is the number of tasks (or cores, for default 1 task per core), time is given in hours, minutes, and seconds (maximum T168 hours), and then you give the id for your project. Your request enters the job queue just like any other job, and interactive/salloc will tell you that it is waiting for the requested resources. When salloc tells you that your job has been allocated resources, you can interactively run programs on those resources with srun . The commands you run with srun will then be executed on the resources your job has been allocated. On HPC2N If you do not preface with srun the command is run on the login node! You can now run Matlab scripts on the allocated resources directly instead of waiting for your batch job to return a result. This is an advantage if you want to test your Matlab script or perhaps figure out which parameters are best.","title":"Start an interactive session"},{"location":"matlab/interactive/#end-an-interactive-session","text":"When you have finished using the allocation, either wait for it to end, or close it with exit $ exit logout Documentation at the centers Interactive allocation on PDC Interactive allocation on NSC Interactive allocation on UPPMAX Interactive allocation on HPC2N Interactive allocation on LUNARC","title":"End an interactive session"},{"location":"matlab/interactive/#exercise","text":"Run from ThinLinc (web or client!) Log in page Interactive session Requesting 4 cores for 10 minutes, and load Matlab NSC PDC UPPMAX (Pelle) UPPMAX (Bianca) HPC2N LUNARC [ sm_bcarl@tetralith3 ~ ] $ interactive -n 4 -t 0 :30:0 -A naiss2025-22-934 salloc: Pending job allocation 43071298 salloc: job 43071298 queued and waiting for resources salloc: job 43071298 has been allocated resources salloc: Granted job allocation 43071298 salloc: Waiting for resource configuration salloc: Nodes n760 are ready for job [ bjornc@n760 ~ ] $ module load MATLAB/2023b-bdist Let us check that we actually run on the compute node: [ sm_bcarl@n760 ~ ] $ srun hostname n760 n760 n760 n760 We are. Notice that we got a response from all four cores we have allocated. claremar@login1:~> salloc --ntasks = 4 -t 0 :30:00 -p shared --qos = normal -A naiss2025-22-934 salloc: Pending job allocation 9102757 salloc: job 9102757 queued and waiting for resources salloc: job 9102757 has been allocated resources salloc: Granted job allocation 9102757 salloc: Waiting for resource configuration salloc: Nodes nid001057 are ready for job claremar@login1:~> module load PDC/23.12 matlab/r2024b Let us check that we actually run on the compute node. This has to be done differently claremar@login1:~> srun hostname nid001064 nid001063 nid001064 nid001063 Now, it seems that Dardel allows for \u201chyperthreading\u201d, that is 2 threads per core. claremar@login1:~> srun -n 8 hostname nid001064 nid001064 nid001063 nid001063 nid001064 nid001064 nid001063 nid001063 We are. Notice that we got a response from all four cores we have allocated. [ bjornc@pelle ~ ] $ interactive -A uppmax2025-2-360 -n 4 -t 0 :30:00 You receive the high interactive priority. There are free cores, so your job is expected to start at once. Please, use no more than 6 .4 GB of RAM. Waiting for job 29556505 to start... Starting job now -- you waited for 1 second. [ bjornc@p102 ~ ] $ module load MATLAB/2023b-update4 Let us check that we actually run on the compute node: [ bjornc@p102 ~ ] $ srun hostname r483.uppmax.uu.se r483.uppmax.uu.se r483.uppmax.uu.se r483.uppmax.uu.se We are. Notice that we got a response from all four cores we have allocated. [ bjornc@sens2017625-bianca ~ ] $ interactive -A sensXXXX -p core -n 4 -t 0 :30:00 You receive the high interactive priority. There are free cores, so your job is expected to start at once. Please, use no more than 6 .4 GB of RAM. Waiting for job 29556505 to start... Starting job now -- you waited for 1 second. [ bjornc@p102 ~ ] $ module load MATLAB/2023b-update4 Let us check that we actually run on the compute node: [ bjornc@p102 ~ ] $ srun hostname r483.uppmax.uu.se r483.uppmax.uu.se r483.uppmax.uu.se r483.uppmax.uu.se We are. Notice that we got a response from all four cores we have allocated. [ ~ ] $ salloc -n 4 --time = 00 :30:00 -A hpc2n2025-151 salloc: Pending job allocation 20174806 salloc: job 20174806 queued and waiting for resources salloc: job 20174806 has been allocated resources salloc: Granted job allocation 20174806 salloc: Waiting for resource configuration salloc: Nodes b-cn0241 are ready for job [ ~ ] $ module load MATLAB/2023b [ ~ ] $ Let us check that we actually run on the compute node: [ ~ ] $ srun hostname b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se We are. Notice that we got a response from all four cores we have allocated. [ bjornc@cosmos1 ~ ] $ interactive -A lu2025-2-94 -n 4 -t 30 :00 Cluster name: COSMOS Waiting for JOBID 930844 to start [ bjornc@cn050 ~ ] $ module load matlab/2024b Let us check that we actually run on the compute node: [ bjornc@cn050 ~ ] $ echo $SLURM_CPUS_ON_NODE 4 We are, because the $SLURM environment variable gives an output. Notice that we got 4, which is not the size of the physcial node but the allocation size. Check from Matlab Start matlab (same for all clusters) matlab & In MATLAB test how big parpool you can make (should be limited by the allocated resources above) p = parpool ( \"local\" ) p . NumWorkers Output of parpool command on UPPMAX According to the below output Starting parallel pool (parpool) using the 'Processes' profile ... Connected to parallel pool with 4 workers. p = ProcessPool with properties : Connected : true NumWorkers : 4 Busy : false Cluster : Processes ( Local Cluster ) AttachedFiles : {} AutoAddClientPath : true FileStore : [ 1 x1 parallel . FileStore ] ValueStore : [ 1 x1 parallel . ValueStore ] IdleTimeout : 30 minutes ( 30 minutes remaining ) SpmdEnabled : true Quit MATLAB End Matlab by closing the GUI. Exit interactive session When you have finished using the allocation, either wait for it to end, or close it with exit NSC PDC UPPMAX HPC2N LUNARC [ sm_bcarl@n134 ~ ] $ exit logout srun: error: n134: task 0 : Exited with exit code 130 srun: Terminating StepId = 43071803 .interactive salloc: Relinquishing job allocation 43071803 salloc: Job allocation 43071803 has been revoked. [ sm_bcarl@tetralith3 ~ ] $ claremar@login1:~> exit exit salloc: Relinquishing job allocation 9103056 claremar@login1:~> [ bjornc@r483 ~ ] $ exit exit [ screen is terminating ] Connection to r483 closed. [ bjornc@rackham2 ~ ] $ [ ~ ] $ exit exit salloc: Relinquishing job allocation 20174806 salloc: Job allocation 20174806 has been revoked. [ ~ ] $ [ ~ ] $ exit exit [ screen is terminating ] Connection to cn050 closed. [ ~ ] $","title":"Exercise"},{"location":"matlab/intro-matlab/","text":"Introduction to MATLAB \u00b6 Please find the exercise files at this link Objectives Load MATLAB modules and site-installed MATLAB packages Create a MATLAB environment Install MATLAB packages with Add-Ons manager Write a batch script for running MATLAB Use MATLAB in parallel mode Use GPUs with MATLAB Use MATLAB for ML Your expectations? Find best practices for using MATLAB at UPPMAX, HPC2N, LUNARC, NSC (Tetralith), and PDC (Dardel) Toolboxes and Add-Ons HPC performance with MATLAB Not covered Improve MATLAB coding skills Other clusters Schedule \u00b6 See schedule here. What is MATLAB? \u00b6 MATLAB is a numerical computing environment and a high-level programming language. Developed by MathWorks, MATLAB allows matrix manipulation, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs in other languages. Although it is numeric only, an optional toolbox uses the MuPAD symbolic engine, allowing access to computer algebra capabilities. Features of MATLAB \u00b6 Distinguishing pros of MATLAB include: A \u201clow-code\u201d interactive development environment (IDE) in which many common data import methods, analysis techniques, plotting formats, and even AI/ML techniques can be run from menus and generate the code required to reproduce the results automatically A rich library of Toolboxes and Add-Ons for different STEM disciplines, especially for modeling and simulations, all written and tested by professionals The ability to set cluster configurations and parallelisation settings graphically, and save them to profiles that can be reloaded at a click. Automatic multi-threading (note: this can also be a drawback) Full documentation available straight from the command line (requires internet) MATLAB also has some drawbacks: It is proprietary software, so you need a license and to sign up for an account (students and faculty at universities typically get these through the university). Many Add-Ons require a separate license. With respect to the 2-language problem (where one can optimise for either performance or ease of prototyping, but not both), MATLAB even moreso than Python is geared toward usability. It can be slow. The way MATLAB automates multithreading means it will typically hog a full node unless you explicitly tell it not to by setting -singleCompThread as an option at startup, or unless your local cluster is configured to automatically limit the number of processes started on a login node. Built-in plotting functions generate very low-resolution raster graphics with poor anti-aliasing in the GUI, and what the GUI shows is not necessarily how the saved image will appear. Making publication-ready plots requires extensive tuning of graphics parameters and repeated exports to special file formats to check your work. MATLAB documentation at NAISS HPC centers \u00b6 Documentation at the HPC centres UPPMAX, HPC2N, LUNARC, NSC, and PDC UPPMAX Matlab docs HPC2N: Matlab docs and parallel Matlab docs LUNARC Matlab docs NSC: click here for general instructions and here for installations on Tetralith specifically PDC Matlab docs Official MATLAB documentation is found here Material for improving your programming skills Extra material Summary MATLAB is a high-level computing language with an interactive environment that can generate code to handle common problems for you. Parallelisation is easy with the GUI, but be careful to set -singleCompThread when starting MATLAB at the command line, or else it may hog a full node.","title":"Intro"},{"location":"matlab/intro-matlab/#introduction-to-matlab","text":"Please find the exercise files at this link Objectives Load MATLAB modules and site-installed MATLAB packages Create a MATLAB environment Install MATLAB packages with Add-Ons manager Write a batch script for running MATLAB Use MATLAB in parallel mode Use GPUs with MATLAB Use MATLAB for ML Your expectations? Find best practices for using MATLAB at UPPMAX, HPC2N, LUNARC, NSC (Tetralith), and PDC (Dardel) Toolboxes and Add-Ons HPC performance with MATLAB Not covered Improve MATLAB coding skills Other clusters","title":"Introduction to MATLAB"},{"location":"matlab/intro-matlab/#schedule","text":"See schedule here.","title":"Schedule"},{"location":"matlab/intro-matlab/#what-is-matlab","text":"MATLAB is a numerical computing environment and a high-level programming language. Developed by MathWorks, MATLAB allows matrix manipulation, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs in other languages. Although it is numeric only, an optional toolbox uses the MuPAD symbolic engine, allowing access to computer algebra capabilities.","title":"What is MATLAB?"},{"location":"matlab/intro-matlab/#features-of-matlab","text":"Distinguishing pros of MATLAB include: A \u201clow-code\u201d interactive development environment (IDE) in which many common data import methods, analysis techniques, plotting formats, and even AI/ML techniques can be run from menus and generate the code required to reproduce the results automatically A rich library of Toolboxes and Add-Ons for different STEM disciplines, especially for modeling and simulations, all written and tested by professionals The ability to set cluster configurations and parallelisation settings graphically, and save them to profiles that can be reloaded at a click. Automatic multi-threading (note: this can also be a drawback) Full documentation available straight from the command line (requires internet) MATLAB also has some drawbacks: It is proprietary software, so you need a license and to sign up for an account (students and faculty at universities typically get these through the university). Many Add-Ons require a separate license. With respect to the 2-language problem (where one can optimise for either performance or ease of prototyping, but not both), MATLAB even moreso than Python is geared toward usability. It can be slow. The way MATLAB automates multithreading means it will typically hog a full node unless you explicitly tell it not to by setting -singleCompThread as an option at startup, or unless your local cluster is configured to automatically limit the number of processes started on a login node. Built-in plotting functions generate very low-resolution raster graphics with poor anti-aliasing in the GUI, and what the GUI shows is not necessarily how the saved image will appear. Making publication-ready plots requires extensive tuning of graphics parameters and repeated exports to special file formats to check your work.","title":"Features of MATLAB"},{"location":"matlab/intro-matlab/#matlab-documentation-at-naiss-hpc-centers","text":"Documentation at the HPC centres UPPMAX, HPC2N, LUNARC, NSC, and PDC UPPMAX Matlab docs HPC2N: Matlab docs and parallel Matlab docs LUNARC Matlab docs NSC: click here for general instructions and here for installations on Tetralith specifically PDC Matlab docs Official MATLAB documentation is found here Material for improving your programming skills Extra material Summary MATLAB is a high-level computing language with an interactive environment that can generate code to handle common problems for you. Parallelisation is easy with the GUI, but be careful to set -singleCompThread when starting MATLAB at the command line, or else it may hog a full node.","title":"MATLAB documentation at NAISS HPC centers"},{"location":"matlab/jupyterMatlab/","text":"Session: Matlab in Jupyter \u00b6 Questions How to reach the calculation nodes How do I proceed to work interactively? Objectives Show how to reach the calculation nodes on UPPMAX and HPC2N by using Jupyter-lab Test some commands on the calculation nodes Note It is possible to run Matlab directly on the login (including ThinLinc) nodes. This should only be done for shorter jobs or jobs that do not use a lot of resources, as the login nodes can otherwise become slow for all users. If you want to work interactively with your code or data, you should start an interactive session. In the present lesson you will see how to run interactive jobs by using Jupyter-lab. Important Compute allocations in this workshop Rackham: uppmax2025-2-272 Kebnekaise: hpc2n2025-062 Cosmos: lu2025-7-24 Storage space for this workshop Rackham: /proj/r-py-jl-m-rackham Kebnekaise: /proj/nobackup/r-py-jl-m Cosmos: your home directory should have plenty of space There are several ways to run Matlab interactively Directly on the login nodes: only do this for short jobs that do not take a lot of resources As an interactive job on the computer nodes, launched via the batch system or Desktop On-Demand (LUNARC) Jupyter notebooks (HPC2N, UPPMAX) As with Python, it is possible to run Matlab in a notebook, i.e. in a web interface with possibility of inline figures and debugging. An easy way to do this is to load Python and Matlab modules. In shell: UPPMAX HPC2N # Load Matlab $ ml matlab/2023a # Load a Python version compatible with Matlab $ ml python/3.10.8 # Create an environment called matlabenv (you can change this name) $ python -m venv ./matlabenv # Activate this environment $ source matlabenv/bin/activate # Perform installations: upgrade pip, and packages that you will need $ pip install --upgrade pip $ pip install -U scikit-learn # Install Jupyterlab $ pip install jupyterlab # Install the Matlab proxy $ pip install jupyter-matlab-proxy $ deactivate # Ask for a suitable amount of time. Remember, this is the time the Jupyter notebook will be available! HHH:MM:SS. $ interactive -A Project_ID -p core -n 1 -t 1 :0:0 # If you use a GPU node add this snippet to the interactive command (for clarity after project ID # -M snowy --gres:gpu:1 $ ml matlab/2023a $ ml python/3.10.8 # Source the environment $ source matlabenv/bin/activate # Start JupyterLab $ jupyter lab --no-browser & $ <press enter to get to active prompt> $ firefox <url from output above> & When the Jupyter notebook interface starts, you can choose the MATLAB kernel version from the module you loaded. When you try to run a notebook, Matlab will ask for a type of license. Because you are running this notebook on our HPC center, you can choose the option Existing License and then Start MATLAB. It can take a minute or so to start. # Load Matlab $ ml MATLAB/2023a.Update4 # Load a Python version compatible with Matlab and also CUDA (if you will run on GPUs) $ ml GCCcore/11.3.0 Python/3.10.4 CUDA/11.7.0 # Create an environment called matlabenv (you can change this name) $ python -m venv ./matlabenv # Activate this environment $ source matlabenv/bin/activate # Perform installations: upgrade pip, and packages that you will need $ pip install --upgrade pip $ pip install -U scikit-learn # Install Jupyterlab $ pip install jupyterlab # Install the Matlab proxy $ pip install jupyter-matlab-proxy $ deactivate Fix the project ID in this batch job job.sh and send it to the queue: #!/bin/bash # Here you should put your own project id #SBATCH -A Project_ID # This example asks for 1 core #SBATCH -n 1 # Ask for a suitable amount of time. Remember, this is the time the Jupyter notebook will be available! HHH:MM:SS. #SBATCH --time=06:20:00 # If you use the GPU nodes uncomment the following lines #SBATCH --gpus=l40s:1 # Clear the environment from any previously loaded modules module purge > /dev/null 2 > & 1 # Load the module environment suitable for the job ml MATLAB/2023a.Update4 ml GCCcore/11.3.0 Python/3.10.4 ml CUDA/11.7.0 # Source the environment source matlabenv/bin/activate # Start JupyterLab jupyter lab --no-browser --ip $( hostname ) Then, in the output file slurm- .out file, copy the url that starts with http://b-cn1403.hpc2n.umu.se:8888/lab and paste it in a Firefox browser on Kebnekaise. When the Jupyter notebook interface starts, you can choose the MATLAB kernel version from the module you loaded. When you try to run a notebook, Matlab will ask for a type of license. Because you are running this notebook on our HPC center, you can choose the option Existing License and then Start MATLAB. If you don\u2019t need any longer the Jupyter job, you should cancel the job with scancel job_ID . Otherwise it will keep running until the time limit of your batch script is reached. Running Matlab in Jupyter on compute nodes at HPC2N On Kebnekaise, you can run Jupyter notebooks with Matlab kernels by using batch scripts Notebook example here Summary There are several ways to run MATLAB jobs interactively. One can use the MATLAB GUI, the interactive sessions: salloc (HPC2N), interactive (UPPMAX), and running a Jupyter notebook on the computing nodes. To run a Jupyter notebook on the computing nodes you will need to follow the prerequisites mentioned in the lesson. If the Jupyter job is no longer needed, cancel it with scancel job_ID . It will not be canceled automatically if you just close the web browser.","title":"Session: Matlab in Jupyter"},{"location":"matlab/jupyterMatlab/#session-matlab-in-jupyter","text":"Questions How to reach the calculation nodes How do I proceed to work interactively? Objectives Show how to reach the calculation nodes on UPPMAX and HPC2N by using Jupyter-lab Test some commands on the calculation nodes Note It is possible to run Matlab directly on the login (including ThinLinc) nodes. This should only be done for shorter jobs or jobs that do not use a lot of resources, as the login nodes can otherwise become slow for all users. If you want to work interactively with your code or data, you should start an interactive session. In the present lesson you will see how to run interactive jobs by using Jupyter-lab. Important Compute allocations in this workshop Rackham: uppmax2025-2-272 Kebnekaise: hpc2n2025-062 Cosmos: lu2025-7-24 Storage space for this workshop Rackham: /proj/r-py-jl-m-rackham Kebnekaise: /proj/nobackup/r-py-jl-m Cosmos: your home directory should have plenty of space There are several ways to run Matlab interactively Directly on the login nodes: only do this for short jobs that do not take a lot of resources As an interactive job on the computer nodes, launched via the batch system or Desktop On-Demand (LUNARC) Jupyter notebooks (HPC2N, UPPMAX) As with Python, it is possible to run Matlab in a notebook, i.e. in a web interface with possibility of inline figures and debugging. An easy way to do this is to load Python and Matlab modules. In shell: UPPMAX HPC2N # Load Matlab $ ml matlab/2023a # Load a Python version compatible with Matlab $ ml python/3.10.8 # Create an environment called matlabenv (you can change this name) $ python -m venv ./matlabenv # Activate this environment $ source matlabenv/bin/activate # Perform installations: upgrade pip, and packages that you will need $ pip install --upgrade pip $ pip install -U scikit-learn # Install Jupyterlab $ pip install jupyterlab # Install the Matlab proxy $ pip install jupyter-matlab-proxy $ deactivate # Ask for a suitable amount of time. Remember, this is the time the Jupyter notebook will be available! HHH:MM:SS. $ interactive -A Project_ID -p core -n 1 -t 1 :0:0 # If you use a GPU node add this snippet to the interactive command (for clarity after project ID # -M snowy --gres:gpu:1 $ ml matlab/2023a $ ml python/3.10.8 # Source the environment $ source matlabenv/bin/activate # Start JupyterLab $ jupyter lab --no-browser & $ <press enter to get to active prompt> $ firefox <url from output above> & When the Jupyter notebook interface starts, you can choose the MATLAB kernel version from the module you loaded. When you try to run a notebook, Matlab will ask for a type of license. Because you are running this notebook on our HPC center, you can choose the option Existing License and then Start MATLAB. It can take a minute or so to start. # Load Matlab $ ml MATLAB/2023a.Update4 # Load a Python version compatible with Matlab and also CUDA (if you will run on GPUs) $ ml GCCcore/11.3.0 Python/3.10.4 CUDA/11.7.0 # Create an environment called matlabenv (you can change this name) $ python -m venv ./matlabenv # Activate this environment $ source matlabenv/bin/activate # Perform installations: upgrade pip, and packages that you will need $ pip install --upgrade pip $ pip install -U scikit-learn # Install Jupyterlab $ pip install jupyterlab # Install the Matlab proxy $ pip install jupyter-matlab-proxy $ deactivate Fix the project ID in this batch job job.sh and send it to the queue: #!/bin/bash # Here you should put your own project id #SBATCH -A Project_ID # This example asks for 1 core #SBATCH -n 1 # Ask for a suitable amount of time. Remember, this is the time the Jupyter notebook will be available! HHH:MM:SS. #SBATCH --time=06:20:00 # If you use the GPU nodes uncomment the following lines #SBATCH --gpus=l40s:1 # Clear the environment from any previously loaded modules module purge > /dev/null 2 > & 1 # Load the module environment suitable for the job ml MATLAB/2023a.Update4 ml GCCcore/11.3.0 Python/3.10.4 ml CUDA/11.7.0 # Source the environment source matlabenv/bin/activate # Start JupyterLab jupyter lab --no-browser --ip $( hostname ) Then, in the output file slurm- .out file, copy the url that starts with http://b-cn1403.hpc2n.umu.se:8888/lab and paste it in a Firefox browser on Kebnekaise. When the Jupyter notebook interface starts, you can choose the MATLAB kernel version from the module you loaded. When you try to run a notebook, Matlab will ask for a type of license. Because you are running this notebook on our HPC center, you can choose the option Existing License and then Start MATLAB. If you don\u2019t need any longer the Jupyter job, you should cancel the job with scancel job_ID . Otherwise it will keep running until the time limit of your batch script is reached. Running Matlab in Jupyter on compute nodes at HPC2N On Kebnekaise, you can run Jupyter notebooks with Matlab kernels by using batch scripts Notebook example here Summary There are several ways to run MATLAB jobs interactively. One can use the MATLAB GUI, the interactive sessions: salloc (HPC2N), interactive (UPPMAX), and running a Jupyter notebook on the computing nodes. To run a Jupyter notebook on the computing nodes you will need to follow the prerequisites mentioned in the lesson. If the Jupyter job is no longer needed, cancel it with scancel job_ID . It will not be canceled automatically if you just close the web browser.","title":"Session: Matlab in Jupyter"},{"location":"matlab/load_runMatlab/","text":"Load and Run MATLAB \u00b6 Objectives Load MATLAB Run MATLAB scripts Start the MATLAB graphical user interface (GUI) Important Different recommended procedures for each HPC center: UPPMAX, NSC, and HPC2N : use module system to load at command line LUNARC : recommended to use Desktop On-Demand menu, but interactive and non-interactive command lines available PDC : recommended to load at command line; can run interactively on a compute node with X-forwarding and salloc . ThinLinc access is restricted to 30 users for the whole Dardel cluster. Most HPC centres in Sweden use the same or a similar module system for their software. The difference lies in which modules are installed and their versions/naming. The general examples below will be similar for all HPC centres in Sweden, with some variation in naming and available versions. Short cheat sheet See which modules exists: module spider or ml spider Find module versions for a particular software: module spider <software> Modules depending only on what is currently loaded: module avail or ml av See which modules are currently loaded: module list or ml Load a module: module load <module>/<version> or ml <module>/<version> Unload a module: module unload <module>/<version> or ml -<module>/<version> More information about a module: module show <module>/<version> or ml show <module>/<version> Unload all modules except the \u201csticky\u201d modules: module purge or ml purge Save currently loaded modules: module save <collection_name> (especially useful on Dardel) (Re)load modules from saved collection: module restore <collection_name> Caution Note that the module systems at UPPMAX, HPC2N, LUNARC, NSC, and PDC are slightly different. There is no system MATLAB that comes preloaded like Python at any of these HPC resources, but ml load matlab with no release date will load the latest release, which is periodically updated. For reproducibility reasons, you should be sure to load the same release throughout a given project. While all modules at UPPMAX and NSC not directly related to bio-informatics are shown by ml avail , modules at the other centers may be hidden until one has loaded a prerequisite, like the compiler GCC . You need to use module spider to see all modules at HPC2N, LUNARC, and PDC, and ml avail for those available to load given your currently loaded prerequisites. New sessions on Dardel (PDC) start with 13 modules loaded, but only one of them is sticky (i.e. will remain loaded after a ml purge command). We highly recommended that you save the preloaded modules as a collection so that you can quickly restore the default modules if you accidentally use ml purge instead of ml unload <module> . Check for MATLAB versions \u00b6 Type-Along \u00b6 Below we have examples for how to check for MATLAB versions on different clusters. Follow along at your cluster. UPPMAX (Rackham and Pelle) and NSC (Tetralith) HPC2N LUNARC PDC (Dardel) Check all available MATLAB versions with: module avail matlab Output on Rackham as of 22 September 2025: ---------------------------- /sw/mf/rackham/applications ---------------------------- matlab/R2014a matlab/R2018a matlab/R2022b matlab/7.10 matlab/R2015a matlab/R2018b matlab/R2023a matlab/7.13 matlab/R2015b matlab/R2019a matlab/R2023b ( D ) matlab/8.0 matlab/R2016a matlab/R2020b matlab/7.4 matlab/8.1 matlab/R2017a matlab/R2022a matlab/7.8 Where: D: Default Module Output on Pelle as of 22 September 2025: ---------------------------- /sw/mf/pelle/applications ---------------------------- MATLAB/2023b-update4 MATLAB/2024a Where: D: Default Module Output on Tetralith as of 27 Feb 2025: --------------------- /software/sse2/tetralith_el9/modules --------------------- MATLAB/recommendation ( D ) MATLAB/2023b-bdist MATLAB/2023a-bdist MATLAB/2024a-hpc1-bdist Where: D: Default Module Check all available MATLAB versions with: module spider MATLAB As of 26 Sep 2024, the above outputs the following on Kebnekaise: ---------------------------------------------------------------------------- MATLAB: ---------------------------------------------------------------------------- Description: MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. Versions: MATLAB/2019b.Update2 MATLAB/2021a MATLAB/2021b MATLAB/2022b.Update3 MATLAB/2023a.Update4 Other possible modules matches: MATLAB-parallel-support ---------------------------------------------------------------------------- Note that it is case-sensitive and must be in ALL-CAPS. There will be results if you type matlab , but they won\u2019t be the ones you want. To see how to load a specific version of MATLAB, including the prerequisites, do module spider MATLAB/<version> Example for MATLAB 2023a.Update4 module spider MATLAB/2023a.Update4 See all available MATLAB versions at the command line with: ml spider matlab Or, if on Desktop On-Demand, select Applications in the top left corner and hover over Applications - Matlab (see also GUI section below). As of 27 Feb 2025, ml spider matlab on COSMOS outputs the following: ---------------------------------------------------------------------------- matlab: ---------------------------------------------------------------------------- Versions: matlab/2022a matlab/2023a matlab/2023b matlab/2024b ---------------------------------------------------------------------------- For detailed information about a specific \"matlab\" package ( including how to load the modules ) use the module ' s full name. Note that names that have a trailing ( E ) are extensions provided by other modules. For example: module spider matlab/2023b ---------------------------------------------------------------------------- See all available MATLAB versions at the command line with: ml spider matlab As of 17 Mar 2025, the above command outputs the following on Dardel: Versions: matlab/r2020b matlab/r2021b matlab/r2022b matlab/r2023a matlab/r2024a-ps matlab/r2024a matlab/r2024b On Dardel, all MATLAB versions have a prerequisite that needs to be loaded (it will called something like PDC/xx.xx or PDCOLD/xx.xx). To view the prerequisites for a specific version of MATLAB, do module spider matlab/<version> For example, module spider matlab/r2024b outputs the following: matlab: matlab/r2024b You will need to load all module ( s ) on any one of the lines below before the \"matlab/r2024b\" module is available to load. PDC/23.12 Help: For more information, visit: https://www.mathworks.com Load a MATLAB module \u00b6 For reproducibility, we recommend ALWAYS loading a specific module instead of using the default version! For this course, we recommend using MATLAB R2023x at UPPMAX (R2023b), NSC (2023b), and HPC2N (2023b), or R2024b at LUNARC (2024b). At PDC, we recommend r2024b. Type-along \u00b6 Try loading a MATLAB module at the command line. First, go back and check which MATLAB modules were available. UPPMAX HPC2N (Kebnekaise) LUNARC (COSMOS) NSC (Tetralith) PDC (Dardel) Rackham. To load version 2023b, do: module load matlab/R2023b Note: all lowercase except the R. For short, you can also use ml instead of module load Pelle. To load version 2023b-update4, do: module load MATLAB/2023b-update4 Note: MATLAB is all uppercase. For short, you can also use ml instead of module load To load version 2023b, do: module load MATLAB/2023b Note: all Uppercase except for the letter after the year. For short, you can also use: ml MATLAB/2023b To load version 2024b, do: module load matlab/2024b Note: all lowercase. For short, you can also use: ml matlab/2024b To load version 2023b, do: module load MATLAB/2023b-bdist Note: all Uppercase except for the letter after the year. For short, you can also use: ml MATLAB/2023b If you check with ml which version is loaded, you will see the -bdist suffix was added automatically. Versions without -bdist at the end only appear with ml spider matlab and they do not appear to be loadable. Go back and check which MATLAB modules were available, and what their prerequisites are. To load version 2024b, do: module load PDC/23.12 module load matlab/r2024b Notes: the module name is all lowercase including the r before the year. For short, you can also use: ml PDC/23.12 matlab/r2024b Start MATLAB at the Command Line \u00b6 Most of the time, you will run either MATLAB live scripts ( .mlx ) or basic script or function files ( .m ). Live scripts can only be opened and worked on in the GUI, while basic function or script files can also be run from a batch script and/or at the command line. Note At the command line, function definition is typically not supported unless the function is short and anonymous. User-defined functions must generally be written up and saved to separate .m files. The GUI is often the recommended interface where it is offered. SLURM jobs can be set up through the Parallel Computing Toolbox , which will be discussed later. The resources required to run the GUI and those required to run a job submitted to SLURM are separate, so do not worry if the maximum allocation time for a MATLAB GUI session is much less than the limit for a SLURM job. -singleCompThread When starting MATLAB from the command line, the -singleCompThread flag is usually required to prevent MATLAB from spawning as many processes as it thinks it needs up to the full capacity of a node. At most HPC centers, terminal instances launch MATLAB (either the GUI or command line) on a login node by default. Hogging a login node can disrupt jobs and access for other users, and violates the NAISS user agreement. HPC center staff reserve the right to kill disruptive tasks without warning. A few HPC centers detect if you have started on a login node and set maxCompThreads to 1 automatically, but when in doubt, use -singleCompThread to be safe. Setting -singleCompThread does not prevent MATLAB from sending parallelised and/or multi-threaded jobs to SLURM or the MATLAB Distributed Computing Server (MDCS). Type-along \u00b6 Try starting MATLAB at the command line. We will use the same versions as in the previous section. UPPMAX (Rackham and Pelle?) and NSC (Tetralith) HPC2N (Kebnekaise) LUNARC (COSMOS) PDC (Dardel) Once you\u2019ve loaded your preferred version of MATLAB, type: matlab -nodisplay to start MATLAB in the terminal. The maximum number of computational threads will be set to 1 automatically if you are on a log-in node. The GUI can be started in a ThinLinc session by going to \u201cApplication\u201d \u2192 \u201cHPC2N Applications\u201d \u2192 \u201cApplications\u201d \u2192 \u201cMatlab \u201d and clicking the desired version. To start MATLAB in the terminal, load matlab/2023b or your preferred version, and then type: matlab -singleCompThread -nodisplay to start MATLAB in the terminal. The -singleCompThread is important to prevent MATLAB from hogging a whole node, and the -nodisplay flag prevents the GUI from launching. It is recommended that GUI be started in ThinLinc at the LUNARC HPC Desktop On-Demand. To start MATLAB in the terminal, you must first choose the correct terminal. There are several in the Applications menu: three in Applications - General , which can safely launch either the MATLAB GUI or MATLAB command line on a compute node, one in Applications - Visualization , which should be reserved for very graphics-heavy jobs, and one in Favorites , which MATLAB users should avoid because it runs on a login node. Starting any of the ones under menu headings starting with Applications - will open a GfxLauncher popup that prompts you for your account and resource choices. Even users who have been awarded GPU time are encouraged to use the CPU-only terminal whenever possible, as they are abundant, less resource intensive, and allow longer wall times. Once you\u2019ve opened a terminal session and loaded your preferred version type: matlab -singleCompThread -nodisplay Starting a MATLAB session on Dardel looks different depending on whether or not you are affiliated KTH or have been added to the MATLAB user group by PDC support (e.g. as part of this course, if you signed up in time). If you are a KTH affiliate or another PDC-approved academic user, you are covered by KTH\u2019s university license. Otherwise, you will need to have a MathWorks account and be logged into it in your browser before getting started, because MATLAB on Dardel will ask you provide the email associated with your MathWorks account and a one-time password (OTP) sent to that account online (which you will have to copy from your browser). Either way, you will first need to load the corresponding PDC, PDCOLD, or PDCTEST prerequisite. The current default is PDC/23.12, and that makes available any MATLAB version from 2024. The only version that reliably works for non-KTH users without asking for credentials is r2024-ps. If you, for example, wanted to start matlab/r2024a-ps in the terminal, the sequence would look as follows: ml PDC/23.12 matlab/r2024a-ps matlab -singleCompThread -nosplash -nodesktop -nodisplay If you are an academic user who has been vetted by PDC staff, the above should be enough to eventually take you to the MATLAB command prompt. Otherwise, you will see the following: Please enter your MathWorks Account email address and press Enter: <your.email@your.institute.se> You need a one-time password to sign in . To get a one-time password, follow these steps: 1 . Go to https://www.mathworks.com/mwa/otp 2 . Enter your MathWorks Account email address. 3 . Copy the generated one-time password. 4 . Return here and enter the password. Enter the one-time password: If you are a student or staff member at KTH and you see these credential requests, that means PDC support did not receive your request for MATLAB access. !!! note PDC users may also go through ThinLinc and open MATLAB from the On-Demand Applications menu, but this is not recommended. Only 30 users are allowed to interface with Dardel via ThinLinc at any one time, and queue times are very long. Start the MATLAB GUI \u00b6 Running the MATLAB GUI requires that users be logged into a ThinLinc session. Refer to the login section for help. HPC2N (Kebnekaise) UPPMAX (Pelle) NSC (Tetralith) LUNARC (COSMOS) PDC (Dardel) For HPC2N, once logged into the remote desktop, the procedure for starting the MATLAB GUI is the same as what was shown above to start it at the command line, except that the -nodisplay flag is omitted (as are -nodesktop -nosplash if applicable). You should still include -singleCompThread ! It is possible to start the MATLAB GUI on a compute node on Kebnekaise (HPC2N). The procedure uses salloc and srun to achieve what other facilities often do with the interactive command (edit time, resources, project ID, and MATLAB version as needed): salloc -N 1 -t 00 :30:00 -A hpc2n20YY-XXX module load MATLAB/2023b srun matlab Finally, HPC2N also has MATLAB available through the Open OnDemand portal. We will cover this briefly in a later lecture , and there is more specific documentation in the HPC2N connection guides . For UPPMAX users, once logged into the remote desktop, the procedure for starting the MATLAB GUI is the same as what was shown above to start it at the command line, except that the -nodisplay flag is omitted (as are -nodesktop -nosplash if applicable). You should still include -singleCompThread ! It is also possible to start the MATLAB GUI on the login node. A MATLAB GUI session on a compute node can be run with the following commands (again, edit time, resources, project ID, and MATLAB version as needed): interactive -n 4 -t 00 :30:00 -A uppmax2025-2-360 module load MATLAB/2023b-update4 matlab In the above example, -n 4 means the job will run on 4 cores. Pelle nodes have 96 cores. Please refer to this link for allowed combinations of interactive parameters. The best way to start the MATLAB GUI on Tetralith depends on how intensively you plan to use the GUI. Most of the time, it is recommended to use the interactive command first to get an allocation on a compute node. The commands you will need to enter look like the following (change the MATLAB version and interactive job specifications as needed): interactive -N 1 --exclusive -t 4 :00:00 module load MATLAB/2024a-hpc1-bdist matlab -softwareopengl For short (<<1 hour) tasks requiring few resources, once you\u2019ve loaded your preferred version of MATLAB, you can use the following command to start MATLAB on a login node: vglrun matlab -nosoftwareopengl You do not need to include -singleCompThread because maxNumCompThreads will be set to 1 automatically to account for starting on a login node. The recommended way to open the MATLAB GUI on COSMOS is by using Desktop On-Demand . We will go over this in the first session after lunch. There are 3 versions per MATLAB release in the Apps menu\u2014regular, (CPU), and (HEP,CPU)\u2014and your resource choices partly depend on which of those you select. The (HEP,CPU) nodes are private. The regular versions may run on either AMD/NVIDIA A40 48-core nodes or Intel/NVIDIA A40 32-core nodes, and the maximum allowed wall times for each are 24 and 48 hours, respectively. If you do not plan to do any intensive graphical work inside the GUI, you can choose the (CPU) version of your preferred release to access an AMD A100 48-core node, which also allows you to run for up to 7 days (168:00:00). There are 2 ways to run MATLAB interactively on Dardel: via SSH with X-forwarding (recommended), or via Desktop On-Demand in a ThinLinc window. ThinLinc access to Dardel is limited to 30 users total, and even if you do get through, interactive jobs are likely to be stuck in queues for a long time. SSH with X-forwarding. When you first ssh into Dardel, you will need to have added the -X flag between ssh and your username. Once you are logged in, you need to book a compute node with salloc , and then SSH (again with the -X flag) into whichever node you are assigned to by salloc . The salloc step and its output will look something like this: salloc -N 1 -t 00 :30:00 -A naiss2025-22-262 -p main if you request a full node, or salloc -c 16 -t 00 :30:00 -A naiss2025-22-262 -p shared if you request, for example, 16 cores on a shared node. Adjust the time, resources, and account number as needed. salloc will then print a handful of lines about the progress of allocating and queuing your job. The last line of output, something like salloc: Nodes nid00xxxx are ready for job , will contain the ID of the node you booked. Copy that, and then run the following lines (after the :~> prompt; adjust the node ID as needed): <user>@login1:~> ssh -X nid001756 <user>@nid001756:~> ml PDC/23.12 <user>@nid001756:~> ml matlab/r2024a-ps <user>@nid001756:~> matlab The GUI will take a few minutes to load. If you are lucky enough to get into Dardel via ThinLinc, the process looks very similar to LUNARC because Dardel uses the same Desktop On-Demand interface, except that the MATLAB versions in the app menu are under PDC - Matlab and are not separated by the types of nodes to run on. Exercises \u00b6 Try them yourself! Note that $ indicates a bash prompt and >> indicates the MATLAB prompt; those characters should not be included at the front of your input. Challenge 1. Load MATLAB in the terminal or GUI. Then do a few simple commands at the command line. For example, $ ml matlab/2023b $ matlab -singleCompThread -nodisplay < M A T L A B ( R ) > Copyright 1984 -2023 The MathWorks, Inc. R2023b Update 7 ( 23 .2.0.2515942 ) 64 -bit ( glnxa64 ) January 30 , 2024 To get started, type doc. For product information, visit www.mathworks.com. >> a = 5 ; >> b = eye ( 2 ) ; >> c = a+b c = 6 5 5 6 Challenge 2. Run a function. Copy the example function below to a file called add2.m in your working directory or the MATLAB directory that the configuration step created for you in your Documents folder. Then run it at the MATLAB command line. function result = add2 ( x,y ) result = x + y disp ( \"The sum of \" + x + \" and \" + y + \" is \" + result ) end Solution >> add2 ( 5 , 8 ) result = 13 The sum of 5 and 8 is 13 >> Challenge 3. Exit the MATLAB command line. Use quit or exit (this can take a few seconds). Solution >> exit Summary You can start MATLAB either in a GUI or, with the -nodisplay flag, run it in the terminal. If you start either interface from the terminal, you must first load the correct module(s). For interactive work, you will usually have to reserve time on a compute node, but some facilities have a Desktop On-Demand interface that makes that easier. If you take the risk of starting on a login node, use the -singleCompThread flag in the starting command to avoid hogging the node, unless you are sure that your cluster sets that constraint on the login node automatically.","title":"Load and run"},{"location":"matlab/load_runMatlab/#load-and-run-matlab","text":"Objectives Load MATLAB Run MATLAB scripts Start the MATLAB graphical user interface (GUI) Important Different recommended procedures for each HPC center: UPPMAX, NSC, and HPC2N : use module system to load at command line LUNARC : recommended to use Desktop On-Demand menu, but interactive and non-interactive command lines available PDC : recommended to load at command line; can run interactively on a compute node with X-forwarding and salloc . ThinLinc access is restricted to 30 users for the whole Dardel cluster. Most HPC centres in Sweden use the same or a similar module system for their software. The difference lies in which modules are installed and their versions/naming. The general examples below will be similar for all HPC centres in Sweden, with some variation in naming and available versions. Short cheat sheet See which modules exists: module spider or ml spider Find module versions for a particular software: module spider <software> Modules depending only on what is currently loaded: module avail or ml av See which modules are currently loaded: module list or ml Load a module: module load <module>/<version> or ml <module>/<version> Unload a module: module unload <module>/<version> or ml -<module>/<version> More information about a module: module show <module>/<version> or ml show <module>/<version> Unload all modules except the \u201csticky\u201d modules: module purge or ml purge Save currently loaded modules: module save <collection_name> (especially useful on Dardel) (Re)load modules from saved collection: module restore <collection_name> Caution Note that the module systems at UPPMAX, HPC2N, LUNARC, NSC, and PDC are slightly different. There is no system MATLAB that comes preloaded like Python at any of these HPC resources, but ml load matlab with no release date will load the latest release, which is periodically updated. For reproducibility reasons, you should be sure to load the same release throughout a given project. While all modules at UPPMAX and NSC not directly related to bio-informatics are shown by ml avail , modules at the other centers may be hidden until one has loaded a prerequisite, like the compiler GCC . You need to use module spider to see all modules at HPC2N, LUNARC, and PDC, and ml avail for those available to load given your currently loaded prerequisites. New sessions on Dardel (PDC) start with 13 modules loaded, but only one of them is sticky (i.e. will remain loaded after a ml purge command). We highly recommended that you save the preloaded modules as a collection so that you can quickly restore the default modules if you accidentally use ml purge instead of ml unload <module> .","title":"Load and Run MATLAB"},{"location":"matlab/load_runMatlab/#check-for-matlab-versions","text":"","title":"Check for MATLAB versions"},{"location":"matlab/load_runMatlab/#type-along","text":"Below we have examples for how to check for MATLAB versions on different clusters. Follow along at your cluster. UPPMAX (Rackham and Pelle) and NSC (Tetralith) HPC2N LUNARC PDC (Dardel) Check all available MATLAB versions with: module avail matlab Output on Rackham as of 22 September 2025: ---------------------------- /sw/mf/rackham/applications ---------------------------- matlab/R2014a matlab/R2018a matlab/R2022b matlab/7.10 matlab/R2015a matlab/R2018b matlab/R2023a matlab/7.13 matlab/R2015b matlab/R2019a matlab/R2023b ( D ) matlab/8.0 matlab/R2016a matlab/R2020b matlab/7.4 matlab/8.1 matlab/R2017a matlab/R2022a matlab/7.8 Where: D: Default Module Output on Pelle as of 22 September 2025: ---------------------------- /sw/mf/pelle/applications ---------------------------- MATLAB/2023b-update4 MATLAB/2024a Where: D: Default Module Output on Tetralith as of 27 Feb 2025: --------------------- /software/sse2/tetralith_el9/modules --------------------- MATLAB/recommendation ( D ) MATLAB/2023b-bdist MATLAB/2023a-bdist MATLAB/2024a-hpc1-bdist Where: D: Default Module Check all available MATLAB versions with: module spider MATLAB As of 26 Sep 2024, the above outputs the following on Kebnekaise: ---------------------------------------------------------------------------- MATLAB: ---------------------------------------------------------------------------- Description: MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. Versions: MATLAB/2019b.Update2 MATLAB/2021a MATLAB/2021b MATLAB/2022b.Update3 MATLAB/2023a.Update4 Other possible modules matches: MATLAB-parallel-support ---------------------------------------------------------------------------- Note that it is case-sensitive and must be in ALL-CAPS. There will be results if you type matlab , but they won\u2019t be the ones you want. To see how to load a specific version of MATLAB, including the prerequisites, do module spider MATLAB/<version> Example for MATLAB 2023a.Update4 module spider MATLAB/2023a.Update4 See all available MATLAB versions at the command line with: ml spider matlab Or, if on Desktop On-Demand, select Applications in the top left corner and hover over Applications - Matlab (see also GUI section below). As of 27 Feb 2025, ml spider matlab on COSMOS outputs the following: ---------------------------------------------------------------------------- matlab: ---------------------------------------------------------------------------- Versions: matlab/2022a matlab/2023a matlab/2023b matlab/2024b ---------------------------------------------------------------------------- For detailed information about a specific \"matlab\" package ( including how to load the modules ) use the module ' s full name. Note that names that have a trailing ( E ) are extensions provided by other modules. For example: module spider matlab/2023b ---------------------------------------------------------------------------- See all available MATLAB versions at the command line with: ml spider matlab As of 17 Mar 2025, the above command outputs the following on Dardel: Versions: matlab/r2020b matlab/r2021b matlab/r2022b matlab/r2023a matlab/r2024a-ps matlab/r2024a matlab/r2024b On Dardel, all MATLAB versions have a prerequisite that needs to be loaded (it will called something like PDC/xx.xx or PDCOLD/xx.xx). To view the prerequisites for a specific version of MATLAB, do module spider matlab/<version> For example, module spider matlab/r2024b outputs the following: matlab: matlab/r2024b You will need to load all module ( s ) on any one of the lines below before the \"matlab/r2024b\" module is available to load. PDC/23.12 Help: For more information, visit: https://www.mathworks.com","title":"Type-Along"},{"location":"matlab/load_runMatlab/#load-a-matlab-module","text":"For reproducibility, we recommend ALWAYS loading a specific module instead of using the default version! For this course, we recommend using MATLAB R2023x at UPPMAX (R2023b), NSC (2023b), and HPC2N (2023b), or R2024b at LUNARC (2024b). At PDC, we recommend r2024b.","title":"Load a MATLAB module"},{"location":"matlab/load_runMatlab/#type-along_1","text":"Try loading a MATLAB module at the command line. First, go back and check which MATLAB modules were available. UPPMAX HPC2N (Kebnekaise) LUNARC (COSMOS) NSC (Tetralith) PDC (Dardel) Rackham. To load version 2023b, do: module load matlab/R2023b Note: all lowercase except the R. For short, you can also use ml instead of module load Pelle. To load version 2023b-update4, do: module load MATLAB/2023b-update4 Note: MATLAB is all uppercase. For short, you can also use ml instead of module load To load version 2023b, do: module load MATLAB/2023b Note: all Uppercase except for the letter after the year. For short, you can also use: ml MATLAB/2023b To load version 2024b, do: module load matlab/2024b Note: all lowercase. For short, you can also use: ml matlab/2024b To load version 2023b, do: module load MATLAB/2023b-bdist Note: all Uppercase except for the letter after the year. For short, you can also use: ml MATLAB/2023b If you check with ml which version is loaded, you will see the -bdist suffix was added automatically. Versions without -bdist at the end only appear with ml spider matlab and they do not appear to be loadable. Go back and check which MATLAB modules were available, and what their prerequisites are. To load version 2024b, do: module load PDC/23.12 module load matlab/r2024b Notes: the module name is all lowercase including the r before the year. For short, you can also use: ml PDC/23.12 matlab/r2024b","title":"Type-along"},{"location":"matlab/load_runMatlab/#start-matlab-at-the-command-line","text":"Most of the time, you will run either MATLAB live scripts ( .mlx ) or basic script or function files ( .m ). Live scripts can only be opened and worked on in the GUI, while basic function or script files can also be run from a batch script and/or at the command line. Note At the command line, function definition is typically not supported unless the function is short and anonymous. User-defined functions must generally be written up and saved to separate .m files. The GUI is often the recommended interface where it is offered. SLURM jobs can be set up through the Parallel Computing Toolbox , which will be discussed later. The resources required to run the GUI and those required to run a job submitted to SLURM are separate, so do not worry if the maximum allocation time for a MATLAB GUI session is much less than the limit for a SLURM job. -singleCompThread When starting MATLAB from the command line, the -singleCompThread flag is usually required to prevent MATLAB from spawning as many processes as it thinks it needs up to the full capacity of a node. At most HPC centers, terminal instances launch MATLAB (either the GUI or command line) on a login node by default. Hogging a login node can disrupt jobs and access for other users, and violates the NAISS user agreement. HPC center staff reserve the right to kill disruptive tasks without warning. A few HPC centers detect if you have started on a login node and set maxCompThreads to 1 automatically, but when in doubt, use -singleCompThread to be safe. Setting -singleCompThread does not prevent MATLAB from sending parallelised and/or multi-threaded jobs to SLURM or the MATLAB Distributed Computing Server (MDCS).","title":"Start MATLAB at the Command Line"},{"location":"matlab/load_runMatlab/#type-along_2","text":"Try starting MATLAB at the command line. We will use the same versions as in the previous section. UPPMAX (Rackham and Pelle?) and NSC (Tetralith) HPC2N (Kebnekaise) LUNARC (COSMOS) PDC (Dardel) Once you\u2019ve loaded your preferred version of MATLAB, type: matlab -nodisplay to start MATLAB in the terminal. The maximum number of computational threads will be set to 1 automatically if you are on a log-in node. The GUI can be started in a ThinLinc session by going to \u201cApplication\u201d \u2192 \u201cHPC2N Applications\u201d \u2192 \u201cApplications\u201d \u2192 \u201cMatlab \u201d and clicking the desired version. To start MATLAB in the terminal, load matlab/2023b or your preferred version, and then type: matlab -singleCompThread -nodisplay to start MATLAB in the terminal. The -singleCompThread is important to prevent MATLAB from hogging a whole node, and the -nodisplay flag prevents the GUI from launching. It is recommended that GUI be started in ThinLinc at the LUNARC HPC Desktop On-Demand. To start MATLAB in the terminal, you must first choose the correct terminal. There are several in the Applications menu: three in Applications - General , which can safely launch either the MATLAB GUI or MATLAB command line on a compute node, one in Applications - Visualization , which should be reserved for very graphics-heavy jobs, and one in Favorites , which MATLAB users should avoid because it runs on a login node. Starting any of the ones under menu headings starting with Applications - will open a GfxLauncher popup that prompts you for your account and resource choices. Even users who have been awarded GPU time are encouraged to use the CPU-only terminal whenever possible, as they are abundant, less resource intensive, and allow longer wall times. Once you\u2019ve opened a terminal session and loaded your preferred version type: matlab -singleCompThread -nodisplay Starting a MATLAB session on Dardel looks different depending on whether or not you are affiliated KTH or have been added to the MATLAB user group by PDC support (e.g. as part of this course, if you signed up in time). If you are a KTH affiliate or another PDC-approved academic user, you are covered by KTH\u2019s university license. Otherwise, you will need to have a MathWorks account and be logged into it in your browser before getting started, because MATLAB on Dardel will ask you provide the email associated with your MathWorks account and a one-time password (OTP) sent to that account online (which you will have to copy from your browser). Either way, you will first need to load the corresponding PDC, PDCOLD, or PDCTEST prerequisite. The current default is PDC/23.12, and that makes available any MATLAB version from 2024. The only version that reliably works for non-KTH users without asking for credentials is r2024-ps. If you, for example, wanted to start matlab/r2024a-ps in the terminal, the sequence would look as follows: ml PDC/23.12 matlab/r2024a-ps matlab -singleCompThread -nosplash -nodesktop -nodisplay If you are an academic user who has been vetted by PDC staff, the above should be enough to eventually take you to the MATLAB command prompt. Otherwise, you will see the following: Please enter your MathWorks Account email address and press Enter: <your.email@your.institute.se> You need a one-time password to sign in . To get a one-time password, follow these steps: 1 . Go to https://www.mathworks.com/mwa/otp 2 . Enter your MathWorks Account email address. 3 . Copy the generated one-time password. 4 . Return here and enter the password. Enter the one-time password: If you are a student or staff member at KTH and you see these credential requests, that means PDC support did not receive your request for MATLAB access. !!! note PDC users may also go through ThinLinc and open MATLAB from the On-Demand Applications menu, but this is not recommended. Only 30 users are allowed to interface with Dardel via ThinLinc at any one time, and queue times are very long.","title":"Type-along"},{"location":"matlab/load_runMatlab/#start-the-matlab-gui","text":"Running the MATLAB GUI requires that users be logged into a ThinLinc session. Refer to the login section for help. HPC2N (Kebnekaise) UPPMAX (Pelle) NSC (Tetralith) LUNARC (COSMOS) PDC (Dardel) For HPC2N, once logged into the remote desktop, the procedure for starting the MATLAB GUI is the same as what was shown above to start it at the command line, except that the -nodisplay flag is omitted (as are -nodesktop -nosplash if applicable). You should still include -singleCompThread ! It is possible to start the MATLAB GUI on a compute node on Kebnekaise (HPC2N). The procedure uses salloc and srun to achieve what other facilities often do with the interactive command (edit time, resources, project ID, and MATLAB version as needed): salloc -N 1 -t 00 :30:00 -A hpc2n20YY-XXX module load MATLAB/2023b srun matlab Finally, HPC2N also has MATLAB available through the Open OnDemand portal. We will cover this briefly in a later lecture , and there is more specific documentation in the HPC2N connection guides . For UPPMAX users, once logged into the remote desktop, the procedure for starting the MATLAB GUI is the same as what was shown above to start it at the command line, except that the -nodisplay flag is omitted (as are -nodesktop -nosplash if applicable). You should still include -singleCompThread ! It is also possible to start the MATLAB GUI on the login node. A MATLAB GUI session on a compute node can be run with the following commands (again, edit time, resources, project ID, and MATLAB version as needed): interactive -n 4 -t 00 :30:00 -A uppmax2025-2-360 module load MATLAB/2023b-update4 matlab In the above example, -n 4 means the job will run on 4 cores. Pelle nodes have 96 cores. Please refer to this link for allowed combinations of interactive parameters. The best way to start the MATLAB GUI on Tetralith depends on how intensively you plan to use the GUI. Most of the time, it is recommended to use the interactive command first to get an allocation on a compute node. The commands you will need to enter look like the following (change the MATLAB version and interactive job specifications as needed): interactive -N 1 --exclusive -t 4 :00:00 module load MATLAB/2024a-hpc1-bdist matlab -softwareopengl For short (<<1 hour) tasks requiring few resources, once you\u2019ve loaded your preferred version of MATLAB, you can use the following command to start MATLAB on a login node: vglrun matlab -nosoftwareopengl You do not need to include -singleCompThread because maxNumCompThreads will be set to 1 automatically to account for starting on a login node. The recommended way to open the MATLAB GUI on COSMOS is by using Desktop On-Demand . We will go over this in the first session after lunch. There are 3 versions per MATLAB release in the Apps menu\u2014regular, (CPU), and (HEP,CPU)\u2014and your resource choices partly depend on which of those you select. The (HEP,CPU) nodes are private. The regular versions may run on either AMD/NVIDIA A40 48-core nodes or Intel/NVIDIA A40 32-core nodes, and the maximum allowed wall times for each are 24 and 48 hours, respectively. If you do not plan to do any intensive graphical work inside the GUI, you can choose the (CPU) version of your preferred release to access an AMD A100 48-core node, which also allows you to run for up to 7 days (168:00:00). There are 2 ways to run MATLAB interactively on Dardel: via SSH with X-forwarding (recommended), or via Desktop On-Demand in a ThinLinc window. ThinLinc access to Dardel is limited to 30 users total, and even if you do get through, interactive jobs are likely to be stuck in queues for a long time. SSH with X-forwarding. When you first ssh into Dardel, you will need to have added the -X flag between ssh and your username. Once you are logged in, you need to book a compute node with salloc , and then SSH (again with the -X flag) into whichever node you are assigned to by salloc . The salloc step and its output will look something like this: salloc -N 1 -t 00 :30:00 -A naiss2025-22-262 -p main if you request a full node, or salloc -c 16 -t 00 :30:00 -A naiss2025-22-262 -p shared if you request, for example, 16 cores on a shared node. Adjust the time, resources, and account number as needed. salloc will then print a handful of lines about the progress of allocating and queuing your job. The last line of output, something like salloc: Nodes nid00xxxx are ready for job , will contain the ID of the node you booked. Copy that, and then run the following lines (after the :~> prompt; adjust the node ID as needed): <user>@login1:~> ssh -X nid001756 <user>@nid001756:~> ml PDC/23.12 <user>@nid001756:~> ml matlab/r2024a-ps <user>@nid001756:~> matlab The GUI will take a few minutes to load. If you are lucky enough to get into Dardel via ThinLinc, the process looks very similar to LUNARC because Dardel uses the same Desktop On-Demand interface, except that the MATLAB versions in the app menu are under PDC - Matlab and are not separated by the types of nodes to run on.","title":"Start the MATLAB GUI"},{"location":"matlab/load_runMatlab/#exercises","text":"Try them yourself! Note that $ indicates a bash prompt and >> indicates the MATLAB prompt; those characters should not be included at the front of your input. Challenge 1. Load MATLAB in the terminal or GUI. Then do a few simple commands at the command line. For example, $ ml matlab/2023b $ matlab -singleCompThread -nodisplay < M A T L A B ( R ) > Copyright 1984 -2023 The MathWorks, Inc. R2023b Update 7 ( 23 .2.0.2515942 ) 64 -bit ( glnxa64 ) January 30 , 2024 To get started, type doc. For product information, visit www.mathworks.com. >> a = 5 ; >> b = eye ( 2 ) ; >> c = a+b c = 6 5 5 6 Challenge 2. Run a function. Copy the example function below to a file called add2.m in your working directory or the MATLAB directory that the configuration step created for you in your Documents folder. Then run it at the MATLAB command line. function result = add2 ( x,y ) result = x + y disp ( \"The sum of \" + x + \" and \" + y + \" is \" + result ) end Solution >> add2 ( 5 , 8 ) result = 13 The sum of 5 and 8 is 13 >> Challenge 3. Exit the MATLAB command line. Use quit or exit (this can take a few seconds). Solution >> exit Summary You can start MATLAB either in a GUI or, with the -nodisplay flag, run it in the terminal. If you start either interface from the terminal, you must first load the correct module(s). For interactive work, you will usually have to reserve time on a compute node, but some facilities have a Desktop On-Demand interface that makes that easier. If you take the risk of starting on a login node, use the -singleCompThread flag in the starting command to avoid hogging the node, unless you are sure that your cluster sets that constraint on the login node automatically.","title":"Exercises"},{"location":"matlab/matlab-addons/","text":"Add-Ons \u00b6 Add-ons extend the capabilities of MATLAB\u00ae by providing additional functionality for specific tasks and applications, such as: connecting to hardware devices additional algorithms interactive apps Encompass a wide variety of resources products apps toolboxes support packages Available from: MathWorks\u00ae the global MATLAB user community For more information about Add-Ons, Mathworks . Objectives Navigate to toolboxes and Add-Ons View Add-Ons and toolboxes Install and use Add-Ons Before going into installing Add-Ons, let\u2019s have a background to the MATLAB environments and ecosystem! Want a video? See Matlab add-ons (Oct 2024) MATLAB Add-Ons manager \u00b6 Toolbar of Add-Ons. In the GUI, the Add-Ons manager can be selected from the menu at the top. The drop-down menu options allow users to: Browse a library of Add-Ons to download. Note that some Add-Ons require a separate license. What does that look like? Manage Add-Ons already downloaded. What does that look like? Package user-generated code as a Toolbox or App Get hardware-related support packages Here we will only focus on the first two options. Important Note that very many packages are already included in the Academic installation and license. You can go to the Add-On explorer and select \u201cView My Products\u201d to see what is available. See also Some typical toolboxes include: Parallel Computing Simulink Symbolic Math Signal Processing Machine Learning Some toolboxes provide a GUI for their tools/Apps We won\u2019t cover the usage of most toolboxes here! Read more about Add-Ons Tip Check which toolboxes are installed by the MATLAB command ver at a MATLAB prompt. Example output from Pelle >> ver --------------------------------------------------------------------------------------------------------------- MATLAB Version: 24.1.0.2689473 (R2024a) Update 6 MATLAB License Number: 863987 Operating System: Linux 5.14.0-570.42.2.el9_6.x86_64 #1 SMP PREEMPT_DYNAMIC Sun Sep 14 13:59:34 UTC 2025 x86_64 Java Version: Java 1.8.0_202-b08 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode --------------------------------------------------------------------------------------------------------------- MATLAB Version 24.1 (R2024a) Simulink Version 24.1 (R2024a) 5G Toolbox Version 24.1 (R2024a) AUTOSAR Blockset Version 24.1 (R2024a) Aerospace Blockset Version 24.1 (R2024a) Aerospace Toolbox Version 24.1 (R2024a) Antenna Toolbox Version 24.1 (R2024a) Audio Toolbox Version 24.1 (R2024a) Automated Driving Toolbox Version 24.1 (R2024a) Bioinformatics Toolbox Version 24.1 (R2024a) Bluetooth Toolbox Version 24.1 (R2024a) C2000 Microcontroller Blockset Version 24.1 (R2024a) Communications Toolbox Version 24.1 (R2024a) Computer Vision Toolbox Version 24.1 (R2024a) Control System Toolbox Version 24.1 (R2024a) Curve Fitting Toolbox Version 24.1 (R2024a) DDS Blockset Version 24.1 (R2024a) DSP HDL Toolbox Version 24.1 (R2024a) DSP System Toolbox Version 24.1 (R2024a) Database Toolbox Version 24.1 (R2024a) Datafeed Toolbox Version 24.1 (R2024a) Deep Learning Toolbox Version 24.1 (R2024a) Econometrics Toolbox Version 24.1 (R2024a) Embedded Coder Version 24.1 (R2024a) Filter Design HDL Coder Version 24.1 (R2024a) Financial Instruments Toolbox Version 24.1 (R2024a) Financial Toolbox Version 24.1 (R2024a) Fixed - Point Designer Version 24.1 ( R2024a ) Fuzzy Logic Toolbox Version 24.1 (R2024a) GPU Coder Version 24.1 (R2024a) Global Optimization Toolbox Version 24.1 (R2024a) HDL Coder Version 24.1 (R2024a) HDL Verifier Version 24.1 (R2024a) Image Acquisition Toolbox Version 24.1 (R2024a) Image Processing Toolbox Version 24.1 (R2024a) Industrial Communication Toolbox Version 24.1 (R2024a) Instrument Control Toolbox Version 24.1 (R2024a) LTE Toolbox Version 24.1 (R2024a) Lidar Toolbox Version 24.1 (R2024a) MATLAB Coder Version 24.1 (R2024a) MATLAB Parallel Server Version 24.1 (R2024a) MATLAB Report Generator Version 24.1 (R2024a) MATLAB Test Version 24.1 (R2024a) Mapping Toolbox Version 24.1 (R2024a) Medical Imaging Toolbox Version 24.1 (R2024a) Mixed - Signal Blockset Version 24.1 ( R2024a ) Model Predictive Control Toolbox Version 24.1 (R2024a) ... Install Add-Ons \u00b6 Search in Add-Ons explorer and install. Installation goes in local folder, ~/MATLAB Add-Ons , and should be accessible wherever you are in the file tree. It\u2019s in the path so it should be possible to run directly if you don\u2019t need to run an installation file. For more information about a specific support package install location, see the documentation for the package. Warning To be able to install you need to use the email for a personal mathworks account . You can install some Add-Ons manually using an installation file. This is useful in several situations: The add-on is not available for installation through the Add-On Explorer, for example, if you create a custom add-on yourself or receive one from someone else. You downloaded the add-on from the Add-On Explorer without installing it. You downloaded the add-on from the File Exchange at MATLAB Central\u2122. Bianca One way to get a MATLAB package to Bianca is to download it on your working computer without installing it. Then you use the wharf to get the (tared) file(s) on Bianca. Then move it to the ~/MATLAB Add-Ons folder on Bianca. See also MathWorks page on getting Add-Ons Demo Search for kalmanf Click \u201cLearning the Kalman Filter\u201d Look at the documentation Test if the command works today: >> kalmanf Unrecognized function or variable 'kalmanf'. OK, it is not there Click \u201cAdd\u201d, and \u201cDownload and Add to path\u201d Type email address connected to your MathWorks account (not needed for some versions at Dardel) Installation starts It will end up in the ~/MATLAB\\ Add-Ons/ folder This is how the file tree looked for me ( tree command is available at some centres) $ tree MATLAB \\ Add-Ons/ MATLAB\\ Add-Ons/ \u2514\u2500\u2500 Collections | \u2514\u2500\u2500 Efficient\\ GRIB1\\ data\\ reader | \u251c\u2500\u2500 core.28328 | \u251c\u2500\u2500 license.txt | \u251c\u2500\u2500 readGRIB1.c | \u251c\u2500\u2500 readGRIB1.mexa64 | \u2514\u2500\u2500 resources | \u251c\u2500\u2500 addons_core.xml | \u251c\u2500\u2500 matlab_path_entries.xml | \u251c\u2500\u2500 metadata.xml | \u251c\u2500\u2500 previewImage.png | \u251c\u2500\u2500 readGRIB1.zip | \u2514\u2500\u2500 screenshot.png \u2514\u2500\u2500 Functions \u2514\u2500\u2500 Learning\\ the\\ Kalman\\ Filter \u251c\u2500\u2500 kalmanf.m \u2514\u2500\u2500 resources \u251c\u2500\u2500 addons_core.xml \u251c\u2500\u2500 kalmanf.zip \u251c\u2500\u2500 matlab_path_entries.xml \u251c\u2500\u2500 metadata.xml \u251c\u2500\u2500 previewImage.png \u2514\u2500\u2500 screenshot.png Evidently it is a function . Note that I already have something classified as collections Now test: >> kalmanf () 'kalmanf' requires Learning the Kalman Filter version 1.0 . 0.0 to be enabled . OK. It is installed but may need some other things. Just an example! Exercises \u00b6 Exercise Find what Toolboxes are available in MATLAB at your place. Exercise Find the kalmanf add-on, install it, and run a test command. Use the Demo as instruction. Exercise Browse the add-ons and get inspired for your own work. Use the Demo as inspiration. Summary Many Add-Ons, like toolboxes and packages are available at the Clusters You can view Add-Ons and toolboxes with the Add-on manager It is all more or less graphical Use Add-Ons explorer to find and install add-ons/toolboxes. Add-ons installed by you can be found in ~/MATLAB\\ Add-Ons/ , and folder is automatically added to the MATLAB PATH, so it should be found no matter which working directory you are working in.","title":"MATLAB Add-Ons"},{"location":"matlab/matlab-addons/#add-ons","text":"Add-ons extend the capabilities of MATLAB\u00ae by providing additional functionality for specific tasks and applications, such as: connecting to hardware devices additional algorithms interactive apps Encompass a wide variety of resources products apps toolboxes support packages Available from: MathWorks\u00ae the global MATLAB user community For more information about Add-Ons, Mathworks . Objectives Navigate to toolboxes and Add-Ons View Add-Ons and toolboxes Install and use Add-Ons Before going into installing Add-Ons, let\u2019s have a background to the MATLAB environments and ecosystem! Want a video? See Matlab add-ons (Oct 2024)","title":"Add-Ons"},{"location":"matlab/matlab-addons/#matlab-add-ons-manager","text":"Toolbar of Add-Ons. In the GUI, the Add-Ons manager can be selected from the menu at the top. The drop-down menu options allow users to: Browse a library of Add-Ons to download. Note that some Add-Ons require a separate license. What does that look like? Manage Add-Ons already downloaded. What does that look like? Package user-generated code as a Toolbox or App Get hardware-related support packages Here we will only focus on the first two options. Important Note that very many packages are already included in the Academic installation and license. You can go to the Add-On explorer and select \u201cView My Products\u201d to see what is available. See also Some typical toolboxes include: Parallel Computing Simulink Symbolic Math Signal Processing Machine Learning Some toolboxes provide a GUI for their tools/Apps We won\u2019t cover the usage of most toolboxes here! Read more about Add-Ons Tip Check which toolboxes are installed by the MATLAB command ver at a MATLAB prompt. Example output from Pelle >> ver --------------------------------------------------------------------------------------------------------------- MATLAB Version: 24.1.0.2689473 (R2024a) Update 6 MATLAB License Number: 863987 Operating System: Linux 5.14.0-570.42.2.el9_6.x86_64 #1 SMP PREEMPT_DYNAMIC Sun Sep 14 13:59:34 UTC 2025 x86_64 Java Version: Java 1.8.0_202-b08 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode --------------------------------------------------------------------------------------------------------------- MATLAB Version 24.1 (R2024a) Simulink Version 24.1 (R2024a) 5G Toolbox Version 24.1 (R2024a) AUTOSAR Blockset Version 24.1 (R2024a) Aerospace Blockset Version 24.1 (R2024a) Aerospace Toolbox Version 24.1 (R2024a) Antenna Toolbox Version 24.1 (R2024a) Audio Toolbox Version 24.1 (R2024a) Automated Driving Toolbox Version 24.1 (R2024a) Bioinformatics Toolbox Version 24.1 (R2024a) Bluetooth Toolbox Version 24.1 (R2024a) C2000 Microcontroller Blockset Version 24.1 (R2024a) Communications Toolbox Version 24.1 (R2024a) Computer Vision Toolbox Version 24.1 (R2024a) Control System Toolbox Version 24.1 (R2024a) Curve Fitting Toolbox Version 24.1 (R2024a) DDS Blockset Version 24.1 (R2024a) DSP HDL Toolbox Version 24.1 (R2024a) DSP System Toolbox Version 24.1 (R2024a) Database Toolbox Version 24.1 (R2024a) Datafeed Toolbox Version 24.1 (R2024a) Deep Learning Toolbox Version 24.1 (R2024a) Econometrics Toolbox Version 24.1 (R2024a) Embedded Coder Version 24.1 (R2024a) Filter Design HDL Coder Version 24.1 (R2024a) Financial Instruments Toolbox Version 24.1 (R2024a) Financial Toolbox Version 24.1 (R2024a) Fixed - Point Designer Version 24.1 ( R2024a ) Fuzzy Logic Toolbox Version 24.1 (R2024a) GPU Coder Version 24.1 (R2024a) Global Optimization Toolbox Version 24.1 (R2024a) HDL Coder Version 24.1 (R2024a) HDL Verifier Version 24.1 (R2024a) Image Acquisition Toolbox Version 24.1 (R2024a) Image Processing Toolbox Version 24.1 (R2024a) Industrial Communication Toolbox Version 24.1 (R2024a) Instrument Control Toolbox Version 24.1 (R2024a) LTE Toolbox Version 24.1 (R2024a) Lidar Toolbox Version 24.1 (R2024a) MATLAB Coder Version 24.1 (R2024a) MATLAB Parallel Server Version 24.1 (R2024a) MATLAB Report Generator Version 24.1 (R2024a) MATLAB Test Version 24.1 (R2024a) Mapping Toolbox Version 24.1 (R2024a) Medical Imaging Toolbox Version 24.1 (R2024a) Mixed - Signal Blockset Version 24.1 ( R2024a ) Model Predictive Control Toolbox Version 24.1 (R2024a) ...","title":"MATLAB Add-Ons manager"},{"location":"matlab/matlab-addons/#install-add-ons","text":"Search in Add-Ons explorer and install. Installation goes in local folder, ~/MATLAB Add-Ons , and should be accessible wherever you are in the file tree. It\u2019s in the path so it should be possible to run directly if you don\u2019t need to run an installation file. For more information about a specific support package install location, see the documentation for the package. Warning To be able to install you need to use the email for a personal mathworks account . You can install some Add-Ons manually using an installation file. This is useful in several situations: The add-on is not available for installation through the Add-On Explorer, for example, if you create a custom add-on yourself or receive one from someone else. You downloaded the add-on from the Add-On Explorer without installing it. You downloaded the add-on from the File Exchange at MATLAB Central\u2122. Bianca One way to get a MATLAB package to Bianca is to download it on your working computer without installing it. Then you use the wharf to get the (tared) file(s) on Bianca. Then move it to the ~/MATLAB Add-Ons folder on Bianca. See also MathWorks page on getting Add-Ons Demo Search for kalmanf Click \u201cLearning the Kalman Filter\u201d Look at the documentation Test if the command works today: >> kalmanf Unrecognized function or variable 'kalmanf'. OK, it is not there Click \u201cAdd\u201d, and \u201cDownload and Add to path\u201d Type email address connected to your MathWorks account (not needed for some versions at Dardel) Installation starts It will end up in the ~/MATLAB\\ Add-Ons/ folder This is how the file tree looked for me ( tree command is available at some centres) $ tree MATLAB \\ Add-Ons/ MATLAB\\ Add-Ons/ \u2514\u2500\u2500 Collections | \u2514\u2500\u2500 Efficient\\ GRIB1\\ data\\ reader | \u251c\u2500\u2500 core.28328 | \u251c\u2500\u2500 license.txt | \u251c\u2500\u2500 readGRIB1.c | \u251c\u2500\u2500 readGRIB1.mexa64 | \u2514\u2500\u2500 resources | \u251c\u2500\u2500 addons_core.xml | \u251c\u2500\u2500 matlab_path_entries.xml | \u251c\u2500\u2500 metadata.xml | \u251c\u2500\u2500 previewImage.png | \u251c\u2500\u2500 readGRIB1.zip | \u2514\u2500\u2500 screenshot.png \u2514\u2500\u2500 Functions \u2514\u2500\u2500 Learning\\ the\\ Kalman\\ Filter \u251c\u2500\u2500 kalmanf.m \u2514\u2500\u2500 resources \u251c\u2500\u2500 addons_core.xml \u251c\u2500\u2500 kalmanf.zip \u251c\u2500\u2500 matlab_path_entries.xml \u251c\u2500\u2500 metadata.xml \u251c\u2500\u2500 previewImage.png \u2514\u2500\u2500 screenshot.png Evidently it is a function . Note that I already have something classified as collections Now test: >> kalmanf () 'kalmanf' requires Learning the Kalman Filter version 1.0 . 0.0 to be enabled . OK. It is installed but may need some other things. Just an example!","title":"Install Add-Ons"},{"location":"matlab/matlab-addons/#exercises","text":"Exercise Find what Toolboxes are available in MATLAB at your place. Exercise Find the kalmanf add-on, install it, and run a test command. Use the Demo as instruction. Exercise Browse the add-ons and get inspired for your own work. Use the Demo as inspiration. Summary Many Add-Ons, like toolboxes and packages are available at the Clusters You can view Add-Ons and toolboxes with the Add-on manager It is all more or less graphical Use Add-Ons explorer to find and install add-ons/toolboxes. Add-ons installed by you can be found in ~/MATLAB\\ Add-Ons/ , and folder is automatically added to the MATLAB PATH, so it should be found no matter which working directory you are working in.","title":"Exercises"},{"location":"matlab/matlab-local-desktop/","text":"Session-UPPMAX: MATLAB client on the desktop \u00b6 Use own computer\u2019s MATLAB Would you like to try run batch jobs on the Rackham or Snowy cluster but use the faster graphics that you can achieve on your own computer? Do you have all your work locally but sometimes need the cluster to do parallel runs? UPPMAX offers this now. Warning This solution is possible only if - you have an UPPMAX compute project - a working matlab on your computer with one of the version available on the cluster: Check with module avail matlab (output shown below): $ ml avail matlab ---------------------------- /sw/mf/rackham/applications ---------------------------- matlab/R2014a matlab/R2018a matlab/R2022b matlab/7.10 matlab/R2015a matlab/R2018b matlab/R2023a matlab/7.13 matlab/R2015b matlab/R2019a matlab/R2023b ( L,D ) matlab/8.0 matlab/R2016a matlab/R2020b matlab/7.4 matlab/8.1 matlab/R2017a matlab/R2022a matlab/7.8 Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" . Want a video? See UPPMAX Matlab client on the desktop (Oct 2024) Let\u2019s get started! \u00b6 Try to type along with the following demonstration. The Rackham MATLAB support package can be found in uppsala.Desktop.zip Download the ZIP file and start MATLAB. Unzip the ZIP file in the location returned by calling >> userpath You can unzip from within MATLAB\u2019s Command window. Configure MATLAB to run parallel jobs on the cluster by calling configCluster . configCluster only needs to be called once per version of MATLAB. >> configCluster Username on RACKHAM (e.g. jdoe): Type your Rackham user name. The resulting output will be (using the 2022b release, for example,): >> Complete . Default cluster profile set to \"Rackham R2022b\" . Note To submit jobs to the local machine instead of the cluster, run the following: >> % Get a handle to the local resources >> c = parcluster ( 'local' ); Configuring Slurm details \u00b6 Prior to submitting the job, various parameters can be assigned, such as queue, e-mail, walltime, etc. See AdditionalProperties for the complete list. You must always set AccountName , WallTime , and ProcsPerNode . Other parameters, like Partition , MemPerCPU , GPUcard , etc. depend on the type, amount, and distribution of resources you need. >> % Get a handle to the cluster >> c = parcluster ; c = Generic Cluster Properties : Profile : Rackham R2022b Modified : false Host : UUC - 4 GM8L33 . user . uu . se NumWorkers : 100000 NumThreads : 1 JobStorageLocation : < path to job outputs locally > ClusterMatlabRoot : / sw / apps / matlab / x86_64 / R2022b OperatingSystem : unix Set some additional parameters related to Slurm on Rackham: >> % Specify the account >> c . AdditionalProperties . AccountName = 'uppmax2025-2-272' ; >> % Specify the wall time (e.g., 1 day, 5 hours, 30 minutes >> c . AdditionalProperties . WallTime = '00:30:00' ; >> % Specify cores per node >> c . AdditionalProperties . ProcsPerNode = 20 ; (Optional parameters:) >> % Specify the partition >> c . AdditionalProperties . Partition = 'devcore' ; >> % Specify another cluster: 'snowy' >> c . AdditionalProperties . ClusterName = 'snowy' >> c . AdditionalProperties . ProcsPerNode = 16 ; >> % Specify number of GPUs >> c . AdditionalProperties . GPUsPerNode = 1 ; >> c . AdditionalProperties . GPUCard = 'gpu-card' ; Do not forget to save your changes! >> c . saveProfile To see the values of the current configuration options, display AdditionalProperties : >> % To view current properties >> c . AdditionalProperties Unset a value when no longer needed: >> % Example Turn off email notifications >> c . AdditionalProperties . EmailAddress = '' ; >> c . saveProfile Start job \u00b6 Copy the script below and paste into a new file called parallel_example_local.m . Save it in your current working directory (check with pwd in the MATLAB Command Window). function t = parallel_example_local ( nLoopIters, sleepTime ) t0 = tic ; parfor idx = 1 : nLoopIters A ( idx ) = idx ; pause ( sleepTime ); end t = toc ( t0 ); The script is supposed to loop over sleepTime seconds of work nLoopIters times. We will define the number of processes in the batch submit line. job = c . batch (@ parallel_example_local , 1 , { 16 , 1 }, 'Pool' , 8 , 'CurrentFolder' , '.' ); Submission to the cluster requires SSH credentials. You will be prompted for username and password or identity file (private key). It will not ask again until you define a new cluster handle c or in next session. Jobs will now default to the cluster rather than submit to the local machine. >> job . State ans = 'running' You can run this several times until it gives >> job . State ans = 'finished' You can also watch the queue Example on Rackham (it really runs there!): [ bjornc2@rackham2 ~ ] $ squeue -u bjornc2 JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 50827312 devcore MATLAB_R bjornc2 R 2 :20 1 r483 Then fetch the output: >> job . fetchOutputs {:} ans = 2.4853 The script looped over 1 s work 16 times, but with 8 processes. In an ideal world it would have taken 16 / 8 = 2 s . Here it took 2.5 s because there is some \u201coverhead.\u201d Run on Snowy >> c . AdditionalProperties . ClusterName = 'snowy' >> c . AdditionalProperties . ProcsPerNode = 16 ; Exercises \u00b6 Challenge 1. Configure your local MATLAB to talk to UPPMAX Use the instructions above to try to make it work! Challenge 2. Run a script on Snowy. Try to run a script from the MATLAB GUI and SLURM session Check in a rackham terminal: squeue -M snowy --me Summary Steps to configure first time download and decompress UPPMAX configure file. run configCluster on local MATLAB and set user name Steps to run set parcluster settings, like you do otherwise. Note: only parcluster will work, not parpool . Did you have problems? Send a support ticket to UPPMAX via supr.naiss.se/support Extra: Helper Functions Function Description Applies Only to Desktop clusterFeatures List of cluster features/constraints \u2014 clusterGpuCards List of cluster GPU cards \u2014 clusterPartitionNames List of cluster partitions \u2014 disableArchiving Modify file archive to resolve file mirroring issue True fixConnection Reestablish cluster connection (e.g., after VPN disruption) True willRun Explain why job is queued \u2014","title":"Session-UPPMAX: MATLAB client on the desktop"},{"location":"matlab/matlab-local-desktop/#session-uppmax-matlab-client-on-the-desktop","text":"Use own computer\u2019s MATLAB Would you like to try run batch jobs on the Rackham or Snowy cluster but use the faster graphics that you can achieve on your own computer? Do you have all your work locally but sometimes need the cluster to do parallel runs? UPPMAX offers this now. Warning This solution is possible only if - you have an UPPMAX compute project - a working matlab on your computer with one of the version available on the cluster: Check with module avail matlab (output shown below): $ ml avail matlab ---------------------------- /sw/mf/rackham/applications ---------------------------- matlab/R2014a matlab/R2018a matlab/R2022b matlab/7.10 matlab/R2015a matlab/R2018b matlab/R2023a matlab/7.13 matlab/R2015b matlab/R2019a matlab/R2023b ( L,D ) matlab/8.0 matlab/R2016a matlab/R2020b matlab/7.4 matlab/8.1 matlab/R2017a matlab/R2022a matlab/7.8 Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" . Want a video? See UPPMAX Matlab client on the desktop (Oct 2024)","title":"Session-UPPMAX: MATLAB client on the desktop"},{"location":"matlab/matlab-local-desktop/#lets-get-started","text":"Try to type along with the following demonstration. The Rackham MATLAB support package can be found in uppsala.Desktop.zip Download the ZIP file and start MATLAB. Unzip the ZIP file in the location returned by calling >> userpath You can unzip from within MATLAB\u2019s Command window. Configure MATLAB to run parallel jobs on the cluster by calling configCluster . configCluster only needs to be called once per version of MATLAB. >> configCluster Username on RACKHAM (e.g. jdoe): Type your Rackham user name. The resulting output will be (using the 2022b release, for example,): >> Complete . Default cluster profile set to \"Rackham R2022b\" . Note To submit jobs to the local machine instead of the cluster, run the following: >> % Get a handle to the local resources >> c = parcluster ( 'local' );","title":"Let&rsquo;s get started!"},{"location":"matlab/matlab-local-desktop/#configuring-slurm-details","text":"Prior to submitting the job, various parameters can be assigned, such as queue, e-mail, walltime, etc. See AdditionalProperties for the complete list. You must always set AccountName , WallTime , and ProcsPerNode . Other parameters, like Partition , MemPerCPU , GPUcard , etc. depend on the type, amount, and distribution of resources you need. >> % Get a handle to the cluster >> c = parcluster ; c = Generic Cluster Properties : Profile : Rackham R2022b Modified : false Host : UUC - 4 GM8L33 . user . uu . se NumWorkers : 100000 NumThreads : 1 JobStorageLocation : < path to job outputs locally > ClusterMatlabRoot : / sw / apps / matlab / x86_64 / R2022b OperatingSystem : unix Set some additional parameters related to Slurm on Rackham: >> % Specify the account >> c . AdditionalProperties . AccountName = 'uppmax2025-2-272' ; >> % Specify the wall time (e.g., 1 day, 5 hours, 30 minutes >> c . AdditionalProperties . WallTime = '00:30:00' ; >> % Specify cores per node >> c . AdditionalProperties . ProcsPerNode = 20 ; (Optional parameters:) >> % Specify the partition >> c . AdditionalProperties . Partition = 'devcore' ; >> % Specify another cluster: 'snowy' >> c . AdditionalProperties . ClusterName = 'snowy' >> c . AdditionalProperties . ProcsPerNode = 16 ; >> % Specify number of GPUs >> c . AdditionalProperties . GPUsPerNode = 1 ; >> c . AdditionalProperties . GPUCard = 'gpu-card' ; Do not forget to save your changes! >> c . saveProfile To see the values of the current configuration options, display AdditionalProperties : >> % To view current properties >> c . AdditionalProperties Unset a value when no longer needed: >> % Example Turn off email notifications >> c . AdditionalProperties . EmailAddress = '' ; >> c . saveProfile","title":"Configuring Slurm details"},{"location":"matlab/matlab-local-desktop/#start-job","text":"Copy the script below and paste into a new file called parallel_example_local.m . Save it in your current working directory (check with pwd in the MATLAB Command Window). function t = parallel_example_local ( nLoopIters, sleepTime ) t0 = tic ; parfor idx = 1 : nLoopIters A ( idx ) = idx ; pause ( sleepTime ); end t = toc ( t0 ); The script is supposed to loop over sleepTime seconds of work nLoopIters times. We will define the number of processes in the batch submit line. job = c . batch (@ parallel_example_local , 1 , { 16 , 1 }, 'Pool' , 8 , 'CurrentFolder' , '.' ); Submission to the cluster requires SSH credentials. You will be prompted for username and password or identity file (private key). It will not ask again until you define a new cluster handle c or in next session. Jobs will now default to the cluster rather than submit to the local machine. >> job . State ans = 'running' You can run this several times until it gives >> job . State ans = 'finished' You can also watch the queue Example on Rackham (it really runs there!): [ bjornc2@rackham2 ~ ] $ squeue -u bjornc2 JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 50827312 devcore MATLAB_R bjornc2 R 2 :20 1 r483 Then fetch the output: >> job . fetchOutputs {:} ans = 2.4853 The script looped over 1 s work 16 times, but with 8 processes. In an ideal world it would have taken 16 / 8 = 2 s . Here it took 2.5 s because there is some \u201coverhead.\u201d Run on Snowy >> c . AdditionalProperties . ClusterName = 'snowy' >> c . AdditionalProperties . ProcsPerNode = 16 ;","title":"Start job"},{"location":"matlab/matlab-local-desktop/#exercises","text":"Challenge 1. Configure your local MATLAB to talk to UPPMAX Use the instructions above to try to make it work! Challenge 2. Run a script on Snowy. Try to run a script from the MATLAB GUI and SLURM session Check in a rackham terminal: squeue -M snowy --me Summary Steps to configure first time download and decompress UPPMAX configure file. run configCluster on local MATLAB and set user name Steps to run set parcluster settings, like you do otherwise. Note: only parcluster will work, not parpool . Did you have problems? Send a support ticket to UPPMAX via supr.naiss.se/support Extra: Helper Functions Function Description Applies Only to Desktop clusterFeatures List of cluster features/constraints \u2014 clusterGpuCards List of cluster GPU cards \u2014 clusterPartitionNames List of cluster partitions \u2014 disableArchiving Modify file archive to resolve file mirroring issue True fixConnection Reestablish cluster connection (e.g., after VPN disruption) True willRun Explain why job is queued \u2014","title":"Exercises"},{"location":"matlab/matlab-summary/","text":"Summary \u00b6 Load and run You can use the module system to load a specific version of MATLAB module load matlab You can start the MATLAB GUI with matlab & except on LUNARC; then you should use On-Demand instead You can run MATLAB on the terminal using matlab -nodisplay [ -singleCompThread -nosplash -nodesktop ] Slurm job scheduler and MATLAB in terminal You can configure the cluster before starting MATLAB, like this: configCluster.sh <project-id if on UPPMAX or LUNARC> or at the MATLAB command line, like this: configCluster You can can add job settings needed to run jobs from MATLAB. c . AdditionalProperties . < properties like AccountName / WallTime > You can work in the MATLAB terminal interface. It works almost the same as with GUI. You can submit jobs from inside the MATLAB terminal interface. job = batch ( 'myScript' ); You can write and submit a MATLAB batch script. Create batchscript.sh : !/bin/bash #SBATCH -A my_account #SBATCH -t 00:10:00 module load <matlab version> matlab -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" Run it: sbatch batchscript.sh `` You can use GPUs with MATLAB c . AdditionalProperties . GpusPerNode = 1 ; MATLAB GUI and Slurm You can submit jobs from inside the MATLAB GUI using the following syntax: c = parcluster ( 'name-of-your-cluster' ); %extra slurm settings j = c . batch (@ myfunction , 'nr. outputs' ,{ 'list of input args' }, 'pool' , 'nr. workers' ); j . wait ; % wait for the results j . fetchOutputs {:}; % fetch the results Note that batch also accepts script names in place of function names, but these must be given in single quotes, with no @ or .m . You can work with MATLAB in parallel parfor : for simple for-loops with independent iterations spmd : for worker-to-worker communication and collaboration parfeval : for tasks that may depend on other tasks or need to run in the background You can check that you are in an interactive session. After running interactive.. on UPPMAX or LUNARC you will see that the linux prompt shows another hostname. Add-Ons You can view add-ons and toolboxes. It is all more or less graphical. You can install add-ons Search in add-ons explorer and install. Installation folder will be in MATLAB\u2019s search path, and so should be accessible from anywhere in the file tree. Session-UPPMAX: MATLAB client on the desktop You can use the MATLAB client on the desktop download and decompress UPPMAX configure file. run configCluster on local MATLAB and set user name. Set parcluster settings, like you do otherwise. Session: MATLAB in Jupyter You can start run MATLAB in Jupyter You need to configure and install some python packages the first time. After that Jupyter will find the MATLAB kernel during startup.","title":"Summary"},{"location":"matlab/matlab-summary/#summary","text":"Load and run You can use the module system to load a specific version of MATLAB module load matlab You can start the MATLAB GUI with matlab & except on LUNARC; then you should use On-Demand instead You can run MATLAB on the terminal using matlab -nodisplay [ -singleCompThread -nosplash -nodesktop ] Slurm job scheduler and MATLAB in terminal You can configure the cluster before starting MATLAB, like this: configCluster.sh <project-id if on UPPMAX or LUNARC> or at the MATLAB command line, like this: configCluster You can can add job settings needed to run jobs from MATLAB. c . AdditionalProperties . < properties like AccountName / WallTime > You can work in the MATLAB terminal interface. It works almost the same as with GUI. You can submit jobs from inside the MATLAB terminal interface. job = batch ( 'myScript' ); You can write and submit a MATLAB batch script. Create batchscript.sh : !/bin/bash #SBATCH -A my_account #SBATCH -t 00:10:00 module load <matlab version> matlab -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" Run it: sbatch batchscript.sh `` You can use GPUs with MATLAB c . AdditionalProperties . GpusPerNode = 1 ; MATLAB GUI and Slurm You can submit jobs from inside the MATLAB GUI using the following syntax: c = parcluster ( 'name-of-your-cluster' ); %extra slurm settings j = c . batch (@ myfunction , 'nr. outputs' ,{ 'list of input args' }, 'pool' , 'nr. workers' ); j . wait ; % wait for the results j . fetchOutputs {:}; % fetch the results Note that batch also accepts script names in place of function names, but these must be given in single quotes, with no @ or .m . You can work with MATLAB in parallel parfor : for simple for-loops with independent iterations spmd : for worker-to-worker communication and collaboration parfeval : for tasks that may depend on other tasks or need to run in the background You can check that you are in an interactive session. After running interactive.. on UPPMAX or LUNARC you will see that the linux prompt shows another hostname. Add-Ons You can view add-ons and toolboxes. It is all more or less graphical. You can install add-ons Search in add-ons explorer and install. Installation folder will be in MATLAB\u2019s search path, and so should be accessible from anywhere in the file tree. Session-UPPMAX: MATLAB client on the desktop You can use the MATLAB client on the desktop download and decompress UPPMAX configure file. run configCluster on local MATLAB and set user name. Set parcluster settings, like you do otherwise. Session: MATLAB in Jupyter You can start run MATLAB in Jupyter You need to configure and install some python packages the first time. After that Jupyter will find the MATLAB kernel during startup.","title":"Summary"},{"location":"matlab/schedule/","text":"MATLAB schedule \u00b6 Time Topic Teacher(s) 09:00 Log in Several 09:45 Coffee break 10:00 Syllabus RP 10:15 Introduction, MATLAB in general RP 10:25 Loading modules and running MATLAB codes RP 10:50 Break 11:05 Slurm job scheduler and MATLAB BB 12:00 LUNCH 13:00 Room 1 MATLAB with Desktop On Demand RP 13:00 Room 2 MATLAB from interactive session BC 13:30 MATLAB GUI and Slurm PO, RP 13:50 Break 14:05 MATLAB GUI and Slurm PO, RP 14:30 Add-Ons BC 14:55 Coffee break 15:10 Summary Several 15:20 Evaluation Several 15:35 Q&A on-demand Several 16:00 END","title":"Schedule"},{"location":"matlab/schedule/#matlab-schedule","text":"Time Topic Teacher(s) 09:00 Log in Several 09:45 Coffee break 10:00 Syllabus RP 10:15 Introduction, MATLAB in general RP 10:25 Loading modules and running MATLAB codes RP 10:50 Break 11:05 Slurm job scheduler and MATLAB BB 12:00 LUNCH 13:00 Room 1 MATLAB with Desktop On Demand RP 13:00 Room 2 MATLAB from interactive session BC 13:30 MATLAB GUI and Slurm PO, RP 13:50 Break 14:05 MATLAB GUI and Slurm PO, RP 14:30 Add-Ons BC 14:55 Coffee break 15:10 Summary Several 15:20 Evaluation Several 15:35 Q&A on-demand Several 16:00 END","title":"MATLAB schedule"},{"location":"matlab/slurmMatlab/","text":"Slurm job scheduler and MATLAB in terminal \u00b6 Objectives Understand and use the Slurm scheduler Configure the cluster Start (MATLAB) batch jobs from the command line Try example Important Compute allocations in this workshop Pelle/Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934 Storage space for this workshop Rackham: /proj/r-matlab-julia-pelle Kebnekaise: /proj/nobackup/fall-courses Tetralith: /proj/courses-fall-2025/users/ Dardel: /cfs/klemming/projects/snic/courses-fall-2025 Alvis: /mimer/NOBACKUP/groups/courses-fall-2025/ Warning Any longer, resource-intensive, or parallel jobs must be run through a batch script . The batch system used at UPPMAX, HPC2N, LUNARC, NSC, and PDC is called SLURM. The same is the case at most of the Swedish HPC centres. SLURM is an Open Source job scheduler, which provides three key functions: - Keeps track of available system resources - Enforces local system resource usage and job scheduling policies - Manages a job queue, distributing work across resources according to policies Users ask for compute resources via the sbatch command (as those who attended yesterday\u2019s R session will already know). To run a batch job, you must create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script). Guides and documentation at: https://docs.hpc2n.umu.se/documentation/batchsystem/intro/ https://docs.uppmax.uu.se/cluster_guides/slurm/ https://lunarc-documentation.readthedocs.io/en/latest/manual/submitting_jobs/manual_basic_job/ https://www.nsc.liu.se/support/batch-jobs/introduction/ https://support.pdc.kth.se/doc/run_jobs/job_scheduling/ https://www.c3se.chalmers.se/documentation/submitting_jobs/running_jobs/ MATLAB is well integrated with SLURM, so there are several ways to run these jobs: Using the job scheduler ( batch command) in MATLAB Desktop/graphical interface (This is the Recommended Use). Starting a parpool with a predefined cluster (This allows for more interactivity). Writing a batch script as for any other software and submitting the job with the sbatch command from SLURM (This could be particularly useful if you want to run long jobs and you don\u2019t need to modify the code in the meantime). In the following sections we will extend these concepts. Useful commands to the batch system \u00b6 Before going into MATLAB specifics for batch jobs, we should look briefly at some useful commands: Submit job: sbatch <jobscript.sh> Get list of your jobs: squeue --me Check on a specific job: scontrol show job <job-id> Delete a specific job: scancel <job-id> Useful info about a job: sacct -l -j <job-id> | less -S Url to a page with info about the job (Kebnekaise only): job-usage <job-id> First time configuration \u00b6 In order to be able to submit jobs to the SLURM queue, you usually need to configure MATLAB the first time you load any particular version. This configuration step will provide a set of default specifications for batch and parallel jobs called a cluster profile . Configuration only needs to be run once per version of MATLAB on each cluster. These specifications can be changed or added to at runtime, and it is possible to have more than one profile for a single release. The instructions for each facility are linked below: HPC2N: Configure MATLAB UPPMAX: MATLAB configuration LUNARC: Configuration at the command line NSC: MATLAB (scroll down to \u201cBefore submitting your first job\u201d) . PDC: no need to configure C3SE: no need to configure Warning On Dardel you need to either have your own Mathworks account, or contact them ahead of time and ask for access to run MATLAB there! Do NOT run configCluster(.sh) on Dardel! For most clusters, configuration can be done at the regular terminal using a shell script called configCluster.sh . At most clusters, it is possible to run configCluster (without \u201c.sh\u201d) on the MATLAB command line; for NSC, this is the only way to run the configuration program. The tabs below demonstrate the preferred method for each cluster where configuration is required the first time you use a given version of MATLAB. UPPMAX HPC2N LUNARC NSC configCluster.sh <project-id> On Pelle, configCluster does not work properly inside the MATLAB GUI. configCluster.sh Example: running configCluster.sh at HPC2N configCluster.sh <project-id> Choose \u201ccosmos\u201d when prompted. Example: [ bbrydsoe@cosmos3 ~ ] $ configCluster.sh lu2024-7-68 salloc: Granted job allocation 927531 salloc: Waiting for resource configuration salloc: Nodes cn011 are ready for job < M A T L A B ( R ) > Copyright 1984 -2023 The MathWorks, Inc. R2023b Update 7 ( 23 .2.0.2515942 ) 64 -bit ( glnxa64 ) January 30 , 2024 To get started, type doc. For product information, visit www.mathworks.com. ip = \"10.21.0.11\" [ 1 ] aurora [ 2 ] cosmos 2 Select a cluster [ 1 -2 ] : >>Complete. Default cluster profile set to \"cosmos R2023b\" . Must set AccountName and WallTime before submitting jobs to COSMOS. E.g. >> c = parcluster ; >> c.AdditionalProperties.AccountName = 'account-name' ; >> % 5 hour walltime >> c.AdditionalProperties.WallTime = '05:00:00' ; >> c.saveProfile MATLAB is configured for multi-node parallelism. salloc: Relinquishing job allocation 927531 salloc: Job allocation 927531 has been revoked. [ bbrydsoe@cosmos3 ~ ] $ module load MATLAB/2024a-hpc1-bdist matlab -nodisplay -nodesktop -nosplash -softwareopengl >> configCluster Choose \u201ctetralith\u201d when prompted. Challenge 1. Log in and Configure MATLAB. Log into your chosen HPC cluster if you have not already. Load the newest version of MATLAB (find with ml spider MATLAB ). Note that on Dardel it has a prerequisite which you must load first, and that only the matlab/r2024a-ps (prerequisite PDCOLD/23.12) allows access from the shell/terminal without you having to give your own Mathworks credentials. Depending on cluster, now do NSC: Run configCluster inside MATLAB on the terminal (start with matlab -singleCompThread -nodisplay -nosplash -nodesktop ). PDC: You do not do configCluster C3SE: You do not do configCluster HPC2N. Run configCluster.sh on the terminal UPPMAX: Run configCluster.sh <project-id> on the terminal LUNARC. Run ``configCluster.sh on the terminl MATLAB terminal interface \u00b6 Here we will discuss: Starting MATLAB on the command line Job settings c.parcluster c.AdditionalProperties. c.batch Starting a job from within MATLAB This section will show you how to use MATLAB completely from the shell/terminal without having to open the GUI. This can be useful if you only have a regular SSH connection or otherwise need to run something fast and lightweight instead of having to open the GUI. This is an extra advantage when you have a poor network connection. Starting MATLAB \u00b6 To start MATLAB on the command line, without running the GUI, load the MATLAB version and do matlab -singleCompThread -nodisplay -nosplash -nodesktop This starts MATLAB. Not all of the flags shown are needed at every center, but it does no harm to include them. Danger On the login-nodes MATLAB must be started with the option -singleCompThread to prevent MATLAB from starting an arbitrary number of threads, potentially occupying the whole node. Working in MATLAB \u00b6 Of course, we can work in MATLAB like this in exactly the same way as in the GUI, as the example below shows $ matlab -singleCompThread -nodisplay -nosplash -nodesktop Opening log file: /home/b/bbrydsoe/java.log.43927 < M A T L A B ( R ) > Copyright 1984 -2023 The MathWorks, Inc. R2023a Update 4 ( 9 .14.0.2306882 ) 64 -bit ( glnxa64 ) June 19 , 2023 To get started, type doc. For product information, visit www.mathworks.com. >> a = [ 1 2 3 ; 4 5 6 ; 7 8 9 ] ; >> b = [ 7 5 6 ; 2 0 8 ; 5 7 1 ] ; >> c = a + b c = 8 7 9 6 5 14 12 15 10 >> d = a - b d = -6 -3 -3 2 5 -2 2 1 8 >> e = c + d ; >> e e = 2 4 6 8 10 12 14 16 18 >> Running MATLAB code with batch \u00b6 Now going to look at running in batch on the compute nodes. Job settings at the command line \u00b6 If you want to run a MATLAB program on the cluster with batch, you have to set some things for the job. Start MATLAB and input the following: NSC, HPC2N, LUNARC, UPPMAX >> c = parcluster ( 'CLUSTER' ); >> c . AdditionalProperties . AccountName = 'PROJECT-ID' ; >> c . AdditionalProperties . WallTime = 'HHH1:MM:SS' ; >> c . saveProfile To list the content of your profile, enter c.AdditionalProperties at the prompt. Important On UPPMAX and PDC you should use c=parcluster; instead of c=parcluster('CLUSTER') . On UPPMAX you also need to add c.AdditionalProperties.ProcsPerNode=20; . C3SE The parallel toolbox does not allow multi-node jobs. You should run an interactive job on a compute node, so do this first: srun --account = naiss2025-22-934 --gpus-per-node = T4:1 --time = 01 :00:00 --pty /bin/bash Now start Matlab on the compute node: matlab -singleCompThread -nodisplay -nosplash -nodesktop And do the example (we already set account and walltime when allocating the compute node): >> c = parcluster ( 'local' ); Additional instructions specific to Dardel The process at PDC (Dardel) is more involved than at other facilities: At PDC, you do NOT set any AdditionalProperties . You instead work in an interactive session. To start an interactive session at PDC \u2026 \u2026on a full node: use salloc -N 1 -t 00:30:00 -A naiss2025-22-934 -p main \u2026on a subset of the cores on a node (here 24): use salloc -c 24 -t 1:00:00 -A naiss2025-22-262 -p shared When the job is allocated, start an SSH connection to the compute node. If you need the GUI on Dardel, you need to start both the SSH connection to the Dardel login node and to the compute node with ssh -X (i.e. use ssh -X <node-allocated-to-you> ). Then, you can load MATLAB and start it (on shell) as usual with ml PDCOLD/23.12 matlab/r2024a-ps matlab -nodisplay -nodesktop -nosplash Example, for C3SE Asking for one hour. Starting from the login node. $ srun --account = naiss2025-22-934 --gpus-per-node = T4:1 --time = 01 :00:00 --pty /bin/bash Then on the compute node: $ ml MATLAB/2024b $ matlab -singleCompThread -nodisplay -nosplash -nodesktop Opening log file: /cephyr/users/brydso/Alvis/java.log.65485 < M A T L A B ( R ) > Copyright 1984 -2024 The MathWorks, Inc. R2024b ( 24 .2.0.2712019 ) 64 -bit ( glnxa64 ) August 22 , 2024 To get started, type doc. For product information, visit www.mathworks.com. >> c = parcluster ( 'local' ) ; Example, for HPC2N Asking for 1 hour walltime. >> c = parcluster ( 'kebnekaise' ); >> c . AdditionalProperties . AccountName = 'hpc2n2025-062' ; >> c . AdditionalProperties . WallTime = '01:00:00' ; >> c . saveProfile Example, for PDC Asking for 1 hour. Starting from personal computer. bbrydsoe@enterprise:~$ ssh -X dardel.pdc.kth.se Last login: Thu Mar 20 17 :02:49 2025 from enterprise.hpc2n.umu.se 2025 -03-14 at 15 :39 [ dardel ] System maintenance done , Dardel is running jobs since a few hours. -- == Welcome to Dardel! == -- bbrydsoe@login1:~> bbrydsoe@login1:~> salloc -c 24 -t 1 :00:00 -A naiss2025-22-934 -p shared salloc: Pending job allocation 9050479 salloc: job 9050479 queued and waiting for resources salloc: job 9050479 has been allocated resources salloc: Granted job allocation 9050479 salloc: Waiting for resource configuration salloc: Nodes nid002585 are ready for job bbrydsoe@login1:~> ssh nid002585 bbrydsoe@nid002585:~> ml PDCOLD/23.12 matlab/r2024a-ps bbrydsoe@nid002585:~> matlab -nodisplay -nodesktop -nosplash < M A T L A B ( R ) > Copyright 1984 -2024 The MathWorks, Inc. R2024a Update 3 ( 24 .1.0.2603908 ) 64 -bit ( glnxa64 ) May 2 , 2024 To get started, type doc. For product information, visit www.mathworks.com. >> c = parcluster ; >> Challenge 2. Set MATLAB job settings. Fill in the job settings on one of: HPC2N: CLUSTER=kebnekaise UPPMAX: no CLUSTER, as said above - i.e. just c=parcluster; LUNARC: CLUSTER=cosmos R2023b NSC: CLUSTER=tetralith C3SE: c=parcluster('local'); NO OTHER JOB SETTINGS! Here you start an interactive session first, as shown above! PDC: no CLUSTER , as said above - i.e. just c=parcluster; NO OTHER JOB SETTINGS! Here you instead start an interactive session first! Remember, the project-id is the compute allocation number given for your choice of cluster at the top of this webpage. Since we are just doing a short test, you can use 15 min instead of 1 hour. Also remember the c.AdditionalProperties.ProcsPerNode=20 if you are on UPPMAX. Test that the settings were added by viewing c.AdditionalProperties (not PDC or C3SE). Running a job from within the MATLAB terminal interface \u00b6 When starting a simple MATLAB program inside MATLAB on the terminal, by default, it will use your cluster profile which you just created and saved above. If you have a script file called myScript.m , the code to run it as a batch job with the default settings is: >> job = batch ( 'myScript' ); The batch command does not block MATLAB so you can continue working while computations take place. If you want to block MATLAB until the job finishes, use the wait function on the job object, like this: wait ( job ); By default, MATLAB saves the Command Window output from the batch job to the diary of the job. To retrieve it, use the diary function: diary ( job ) After the job finishes, fetch the results by using the load function: load ( job , 'x' ); or using the .fetchOutputs attribute: job . fetchOutputs {:} Tip Other useful job monitoring commands: If you need the Job id, run squeue --me on the command line. To get the MATLAB jobid do id=job.ID within MATLAB. To see if the job is running, inside MATLAB, do job.State Serial \u00b6 After starting MATLAB, you can get a handle to the cluster (remember, on Rackham/Pelle and Dardel, just use c=parcluster; ) like this: >> c = parcluster ( 'CLUSTER' ) For example, given that myfcn is a command or serial MATLAB program. N is the number of output arguments from the evaluated function x1, x2, x3,\u2026 are the input arguments then starting a batch job with these inputs would look like: j = c . batch (@ myfcn , N , { x1 , x2 , x3 , ... }) This job handle j then allows us to: Query the job status with j.State Fetch the result, if the job is finished, using j.fetchOutputs{:} , and Delete the job when it is not needed anymore using j.delete If you are running a lot of jobs, or if you want to quit MATLAB and restart it at a later time, you can retrieve the list of jobs and fetch the outputs of a specific job by index. For example: Get the list of jobs: jobs = c . Jobs Retrieve the output of the second job: j2 = jobs ( 2 ) output = j2 . fetchOutputs {:} Type-along. After doing the job settings further up, let us try running an example. We will use the example add2.m which adds two numbers. This example uses 1 and 2, but you can pick any numbers you want. You can find the add2.m script in the exercises/matlab directory or you can download it from here . Create a parcluster ( c=parcluster; or c=parcluster('CLUSTER'); or c=parcluster('local'=; Set up the job with job = c.batch(@add2, 1, {1,2}) Check if it has finished with job.State When it has finished, retrieve the result with job.fetchOutputs{:} Parallel \u00b6 Running parallel batch jobs are quite similar to running serial jobs. We just need to specify a MATLAB Pool to use and of course MATLAB code that is parallelized. This is easiest illustrated with an example. To make a pool of workers, and to give input etc., the basic syntax is: >> job = c . batch (@ SCRIPT , # output , { input1 , input2 , input3 , ... }, 'pool', #workers); Example: Running a simple MATLAB script, parallel-example.m, giving the input \u201c16\u201d, creating 4 workers, expecting 1 output. The name of the job handle is up to you (here it is j ). >> j = c . batch (@ parallel_example , 1 , { 16 }, 'pool' , 4 ); Let us try running this on Kebnekaise, including checking state and then getting output: >> j = c . batch (@ parallel_example , 1 , { 16 }, 'pool' , 4 ); additionalSubmitArgs = '--ntasks=5 --cpus-per-task=1 -A hpc2n2025-062 -t 01:00:00' >> j . State ans = 'running' >> j . State ans = 'finished' >> j . fetchOutputs {:} ans = 9.3387 >> Challenge 3. Try the above example. This exercise assumes you did the previous ones on this page; loading MATLAB, doing the configCluster.sh, adding the job settings. It should work on all the clusters. You can download parallel_example.m here . parpool \u00b6 On the clusters where this works, you can start a parpool and then (for instance) run a parallel code inside MATLAB. Parpool Example, for PDC: As shown earlier, first start an interactive session, login to the compute node you got, then load matlab and start it. Then create a parpool of the size (at most) that you asked for in number of cores. >> p = parpool ( 24 ) Starting parallel pool (parpool) using the 'Processes' profile ... connected to 24 workers. p = Pool with properties: Connected : true NumWorkers : 24 Cluster : local AttachedFiles : {} IdleTimeout : 30 minute ( s ) ( 30 minutes remaining ) SpmdEnabled : true >> parallel_example ans = 8.9287 There is more information about batch jobs here on Mathworks . Batch scripts containing MATLAB code \u00b6 Here we will discuss creating a batch script to run MATLAB in - Serial - Parallel While we can submit batch jobs (or even batch jobs of batch jobs) from inside MATLAB (and that may be the most common way of using the batch system with MATLAB), it is also possible to create a batch submit script and use that to run MATLAB. The difference here is that when the batch script has been submitted, you cannot make changes to your job. It is not interactive. That is also an advantage - you can submit the job, log out, and then come back later and see the results. Warning parpool can only be used on UPPMAX, Cosmos, Dardel, Kebnekaise, and Alvis. Serial batch jobs \u00b6 Here is an example of a serial batch job formatted for each of the 6 facilities covered. UPPMAX HPC2N LUNARC NSC PDC C3SE #!/bin/bash # Change to your actual project number later #SBATCH -A uppmax2025-2-360 # Asking for 1 core #SBATCH -n 1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider MATLAB module add MATLAB/2024a # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Change to your actual project number later #SBATCH -A hpc2n2025-151 # Asking for 1 core #SBATCH -n 1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add MATLAB/2023a.Update4 # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Change to your actual project number later #SBATCH -A lu2025-7-94 # Asking for 1 core #SBATCH -n 1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add matlab/2023b # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Change to your actual project number later #SBATCH -A naiss2025-22-934 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-core=1 # Asking for 15 min (change as you want) #SBATCH -t 00:15:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 module load MATLAB/2024a-hpc1-bdist # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -singleCompThread -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Change to your actual project number later #SBATCH -A naiss2025-22-934 #SBATCH -n 1 # Asking for 15 min (change as you want) #SBATCH -t 00:15:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out #SBATCH -p main # Clean the environment module purge > /dev/null 2 > & 1 module load PDCOLD/23.12 matlab/r2024a-ps # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -singleCompThread -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A NAISS2025-22-934 #SBATCH -t 00:05:00 #SBATCH -p alvis #SBATCH -N 1 --gpus-per-node=T4:1 ml purge > /dev/null 2 > & 1 module load MATLAB/2024b time matlab -singleCompThread -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" You can download monte_carlo_pi.m here here or find it under matlab in the exercises directory. You submit it with sbatch <batchscript.sh> Where <batchscript.sh> is the name you gave your batchscript. You can find ones for each of the clusters in the exercises -> matlab directory, named monte_carlo_pi_<cluster>.sh . Challenge 4: Try run the serial batch script. Submit it, then check that it is running with squeue --me . Check the output in the matlab_JOBID.out (and the error in the matlab_JOBID.err file). Parallel batch script \u00b6 This is an example batch script for parallel MATLAB. Adjust to your preferred facility by cross-referencing the tabs above. #!/bin/bash # Change to your actual project number #SBATCH -A XXXX-YY-ZZZ #SBATCH --ntasks-per-node=<how many tasks> #SBATCH --nodes <how many nodes> # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add MATLAB/<version> # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r parallel-matlab-script.m Inside the MATLAB code, the number of CPU-cores ( NumWorkers in MATLAB terminology) can be specified when creating the parallel pool, for example, with 8 processes: >> poolobj = parpool ( 'local' , 8 ); Challenge 5. Write a batch script. Try making a batch script to run the parallel_example.m in the example from inside MATLAB above. You can use the above batch script as template. Solutions UPPMAX HPC2N LUNARC NSC PDC C3SE #!/bin/bash # Change to your actual project number #SBATCH -A uppmax2025-2-360 # Remember, there are 4 workers and 1 master! #SBATCH --ntasks=5 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=5 #SBATCH --ntasks-per-core=1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider MATLAB module add MATLAB/2024a # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Change to your actual project number #SBATCH -A hpc2n2025-151 # Remember, there are 4 workers and 1 master! #SBATCH --ntasks=5 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=5 #SBATCH --ntasks-per-core=1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add MATLAB/2023a.Update4 # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Change to your actual project number #SBATCH -A lu2025-7-94 # Remember, there are 4 workers and 1 master! #SBATCH --ntasks=5 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=5 #SBATCH --ntasks-per-core=1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add matlab/2023b # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Change to your actual project number #SBATCH -A naiss2025-22-934 # Remember, there are 4 workers and 1 master! #SBATCH --ntasks=5 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-core=1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add MATLAB/2024a-hpc1-bdist # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Change to your actual project number #SBATCH -A naiss2025-22-934 # Remember, there are 4 workers and 1 master! #SBATCH -p shared #SBATCH -n 5 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add PDCOLD/23.12 matlab/r2024a-ps # Executing a parallel matlab program matlab -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A NAISS2025-22-934 #SBATCH -t 00:05:00 #SBATCH -p alvis #You always need to ask for GPUs on Alvis! And you should not use it for anything but GPU jobs! #SBATCH -N 1 --gpus-per-node=T4:1 ml purge > /dev/null 2 > & 1 module load MATLAB/2024b time matlab -singleCompThread -nojvm -nodisplay -r \"parallel_example(16)\" GPU code Using MATLAB with GPUs will be covered in the Introduction to GPUs section on the fourth day (\u201cAdvanced material\u201d). Summary The SLURM scheduler handles allocations to the calculation/compute nodes Batch jobs run without interaction with user A batch script consists of a part with SLURM parameters describing the allocation and a second part describing the actual work within the job, for instance one or several MATLAB scripts. You can run MATLAB as a batch job through a batch script or from inside MATLAB (shell or GUI) Remember to include possible input arguments to the MATLAB script in the batch script. You need to configure MATLAB before submitting batch jobs (except on Dardel).","title":"MATLAB terminal and Slurm"},{"location":"matlab/slurmMatlab/#slurm-job-scheduler-and-matlab-in-terminal","text":"Objectives Understand and use the Slurm scheduler Configure the cluster Start (MATLAB) batch jobs from the command line Try example Important Compute allocations in this workshop Pelle/Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934 Storage space for this workshop Rackham: /proj/r-matlab-julia-pelle Kebnekaise: /proj/nobackup/fall-courses Tetralith: /proj/courses-fall-2025/users/ Dardel: /cfs/klemming/projects/snic/courses-fall-2025 Alvis: /mimer/NOBACKUP/groups/courses-fall-2025/ Warning Any longer, resource-intensive, or parallel jobs must be run through a batch script . The batch system used at UPPMAX, HPC2N, LUNARC, NSC, and PDC is called SLURM. The same is the case at most of the Swedish HPC centres. SLURM is an Open Source job scheduler, which provides three key functions: - Keeps track of available system resources - Enforces local system resource usage and job scheduling policies - Manages a job queue, distributing work across resources according to policies Users ask for compute resources via the sbatch command (as those who attended yesterday\u2019s R session will already know). To run a batch job, you must create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script). Guides and documentation at: https://docs.hpc2n.umu.se/documentation/batchsystem/intro/ https://docs.uppmax.uu.se/cluster_guides/slurm/ https://lunarc-documentation.readthedocs.io/en/latest/manual/submitting_jobs/manual_basic_job/ https://www.nsc.liu.se/support/batch-jobs/introduction/ https://support.pdc.kth.se/doc/run_jobs/job_scheduling/ https://www.c3se.chalmers.se/documentation/submitting_jobs/running_jobs/ MATLAB is well integrated with SLURM, so there are several ways to run these jobs: Using the job scheduler ( batch command) in MATLAB Desktop/graphical interface (This is the Recommended Use). Starting a parpool with a predefined cluster (This allows for more interactivity). Writing a batch script as for any other software and submitting the job with the sbatch command from SLURM (This could be particularly useful if you want to run long jobs and you don\u2019t need to modify the code in the meantime). In the following sections we will extend these concepts.","title":"Slurm job scheduler and MATLAB in terminal"},{"location":"matlab/slurmMatlab/#useful-commands-to-the-batch-system","text":"Before going into MATLAB specifics for batch jobs, we should look briefly at some useful commands: Submit job: sbatch <jobscript.sh> Get list of your jobs: squeue --me Check on a specific job: scontrol show job <job-id> Delete a specific job: scancel <job-id> Useful info about a job: sacct -l -j <job-id> | less -S Url to a page with info about the job (Kebnekaise only): job-usage <job-id>","title":"Useful commands to the batch system"},{"location":"matlab/slurmMatlab/#first-time-configuration","text":"In order to be able to submit jobs to the SLURM queue, you usually need to configure MATLAB the first time you load any particular version. This configuration step will provide a set of default specifications for batch and parallel jobs called a cluster profile . Configuration only needs to be run once per version of MATLAB on each cluster. These specifications can be changed or added to at runtime, and it is possible to have more than one profile for a single release. The instructions for each facility are linked below: HPC2N: Configure MATLAB UPPMAX: MATLAB configuration LUNARC: Configuration at the command line NSC: MATLAB (scroll down to \u201cBefore submitting your first job\u201d) . PDC: no need to configure C3SE: no need to configure Warning On Dardel you need to either have your own Mathworks account, or contact them ahead of time and ask for access to run MATLAB there! Do NOT run configCluster(.sh) on Dardel! For most clusters, configuration can be done at the regular terminal using a shell script called configCluster.sh . At most clusters, it is possible to run configCluster (without \u201c.sh\u201d) on the MATLAB command line; for NSC, this is the only way to run the configuration program. The tabs below demonstrate the preferred method for each cluster where configuration is required the first time you use a given version of MATLAB. UPPMAX HPC2N LUNARC NSC configCluster.sh <project-id> On Pelle, configCluster does not work properly inside the MATLAB GUI. configCluster.sh Example: running configCluster.sh at HPC2N configCluster.sh <project-id> Choose \u201ccosmos\u201d when prompted. Example: [ bbrydsoe@cosmos3 ~ ] $ configCluster.sh lu2024-7-68 salloc: Granted job allocation 927531 salloc: Waiting for resource configuration salloc: Nodes cn011 are ready for job < M A T L A B ( R ) > Copyright 1984 -2023 The MathWorks, Inc. R2023b Update 7 ( 23 .2.0.2515942 ) 64 -bit ( glnxa64 ) January 30 , 2024 To get started, type doc. For product information, visit www.mathworks.com. ip = \"10.21.0.11\" [ 1 ] aurora [ 2 ] cosmos 2 Select a cluster [ 1 -2 ] : >>Complete. Default cluster profile set to \"cosmos R2023b\" . Must set AccountName and WallTime before submitting jobs to COSMOS. E.g. >> c = parcluster ; >> c.AdditionalProperties.AccountName = 'account-name' ; >> % 5 hour walltime >> c.AdditionalProperties.WallTime = '05:00:00' ; >> c.saveProfile MATLAB is configured for multi-node parallelism. salloc: Relinquishing job allocation 927531 salloc: Job allocation 927531 has been revoked. [ bbrydsoe@cosmos3 ~ ] $ module load MATLAB/2024a-hpc1-bdist matlab -nodisplay -nodesktop -nosplash -softwareopengl >> configCluster Choose \u201ctetralith\u201d when prompted. Challenge 1. Log in and Configure MATLAB. Log into your chosen HPC cluster if you have not already. Load the newest version of MATLAB (find with ml spider MATLAB ). Note that on Dardel it has a prerequisite which you must load first, and that only the matlab/r2024a-ps (prerequisite PDCOLD/23.12) allows access from the shell/terminal without you having to give your own Mathworks credentials. Depending on cluster, now do NSC: Run configCluster inside MATLAB on the terminal (start with matlab -singleCompThread -nodisplay -nosplash -nodesktop ). PDC: You do not do configCluster C3SE: You do not do configCluster HPC2N. Run configCluster.sh on the terminal UPPMAX: Run configCluster.sh <project-id> on the terminal LUNARC. Run ``configCluster.sh on the terminl","title":"First time configuration"},{"location":"matlab/slurmMatlab/#matlab-terminal-interface","text":"Here we will discuss: Starting MATLAB on the command line Job settings c.parcluster c.AdditionalProperties. c.batch Starting a job from within MATLAB This section will show you how to use MATLAB completely from the shell/terminal without having to open the GUI. This can be useful if you only have a regular SSH connection or otherwise need to run something fast and lightweight instead of having to open the GUI. This is an extra advantage when you have a poor network connection.","title":"MATLAB terminal interface"},{"location":"matlab/slurmMatlab/#starting-matlab","text":"To start MATLAB on the command line, without running the GUI, load the MATLAB version and do matlab -singleCompThread -nodisplay -nosplash -nodesktop This starts MATLAB. Not all of the flags shown are needed at every center, but it does no harm to include them. Danger On the login-nodes MATLAB must be started with the option -singleCompThread to prevent MATLAB from starting an arbitrary number of threads, potentially occupying the whole node.","title":"Starting MATLAB"},{"location":"matlab/slurmMatlab/#working-in-matlab","text":"Of course, we can work in MATLAB like this in exactly the same way as in the GUI, as the example below shows $ matlab -singleCompThread -nodisplay -nosplash -nodesktop Opening log file: /home/b/bbrydsoe/java.log.43927 < M A T L A B ( R ) > Copyright 1984 -2023 The MathWorks, Inc. R2023a Update 4 ( 9 .14.0.2306882 ) 64 -bit ( glnxa64 ) June 19 , 2023 To get started, type doc. For product information, visit www.mathworks.com. >> a = [ 1 2 3 ; 4 5 6 ; 7 8 9 ] ; >> b = [ 7 5 6 ; 2 0 8 ; 5 7 1 ] ; >> c = a + b c = 8 7 9 6 5 14 12 15 10 >> d = a - b d = -6 -3 -3 2 5 -2 2 1 8 >> e = c + d ; >> e e = 2 4 6 8 10 12 14 16 18 >>","title":"Working in MATLAB"},{"location":"matlab/slurmMatlab/#running-matlab-code-with-batch","text":"Now going to look at running in batch on the compute nodes.","title":"Running MATLAB code with batch"},{"location":"matlab/slurmMatlab/#job-settings-at-the-command-line","text":"If you want to run a MATLAB program on the cluster with batch, you have to set some things for the job. Start MATLAB and input the following: NSC, HPC2N, LUNARC, UPPMAX >> c = parcluster ( 'CLUSTER' ); >> c . AdditionalProperties . AccountName = 'PROJECT-ID' ; >> c . AdditionalProperties . WallTime = 'HHH1:MM:SS' ; >> c . saveProfile To list the content of your profile, enter c.AdditionalProperties at the prompt. Important On UPPMAX and PDC you should use c=parcluster; instead of c=parcluster('CLUSTER') . On UPPMAX you also need to add c.AdditionalProperties.ProcsPerNode=20; . C3SE The parallel toolbox does not allow multi-node jobs. You should run an interactive job on a compute node, so do this first: srun --account = naiss2025-22-934 --gpus-per-node = T4:1 --time = 01 :00:00 --pty /bin/bash Now start Matlab on the compute node: matlab -singleCompThread -nodisplay -nosplash -nodesktop And do the example (we already set account and walltime when allocating the compute node): >> c = parcluster ( 'local' ); Additional instructions specific to Dardel The process at PDC (Dardel) is more involved than at other facilities: At PDC, you do NOT set any AdditionalProperties . You instead work in an interactive session. To start an interactive session at PDC \u2026 \u2026on a full node: use salloc -N 1 -t 00:30:00 -A naiss2025-22-934 -p main \u2026on a subset of the cores on a node (here 24): use salloc -c 24 -t 1:00:00 -A naiss2025-22-262 -p shared When the job is allocated, start an SSH connection to the compute node. If you need the GUI on Dardel, you need to start both the SSH connection to the Dardel login node and to the compute node with ssh -X (i.e. use ssh -X <node-allocated-to-you> ). Then, you can load MATLAB and start it (on shell) as usual with ml PDCOLD/23.12 matlab/r2024a-ps matlab -nodisplay -nodesktop -nosplash Example, for C3SE Asking for one hour. Starting from the login node. $ srun --account = naiss2025-22-934 --gpus-per-node = T4:1 --time = 01 :00:00 --pty /bin/bash Then on the compute node: $ ml MATLAB/2024b $ matlab -singleCompThread -nodisplay -nosplash -nodesktop Opening log file: /cephyr/users/brydso/Alvis/java.log.65485 < M A T L A B ( R ) > Copyright 1984 -2024 The MathWorks, Inc. R2024b ( 24 .2.0.2712019 ) 64 -bit ( glnxa64 ) August 22 , 2024 To get started, type doc. For product information, visit www.mathworks.com. >> c = parcluster ( 'local' ) ; Example, for HPC2N Asking for 1 hour walltime. >> c = parcluster ( 'kebnekaise' ); >> c . AdditionalProperties . AccountName = 'hpc2n2025-062' ; >> c . AdditionalProperties . WallTime = '01:00:00' ; >> c . saveProfile Example, for PDC Asking for 1 hour. Starting from personal computer. bbrydsoe@enterprise:~$ ssh -X dardel.pdc.kth.se Last login: Thu Mar 20 17 :02:49 2025 from enterprise.hpc2n.umu.se 2025 -03-14 at 15 :39 [ dardel ] System maintenance done , Dardel is running jobs since a few hours. -- == Welcome to Dardel! == -- bbrydsoe@login1:~> bbrydsoe@login1:~> salloc -c 24 -t 1 :00:00 -A naiss2025-22-934 -p shared salloc: Pending job allocation 9050479 salloc: job 9050479 queued and waiting for resources salloc: job 9050479 has been allocated resources salloc: Granted job allocation 9050479 salloc: Waiting for resource configuration salloc: Nodes nid002585 are ready for job bbrydsoe@login1:~> ssh nid002585 bbrydsoe@nid002585:~> ml PDCOLD/23.12 matlab/r2024a-ps bbrydsoe@nid002585:~> matlab -nodisplay -nodesktop -nosplash < M A T L A B ( R ) > Copyright 1984 -2024 The MathWorks, Inc. R2024a Update 3 ( 24 .1.0.2603908 ) 64 -bit ( glnxa64 ) May 2 , 2024 To get started, type doc. For product information, visit www.mathworks.com. >> c = parcluster ; >> Challenge 2. Set MATLAB job settings. Fill in the job settings on one of: HPC2N: CLUSTER=kebnekaise UPPMAX: no CLUSTER, as said above - i.e. just c=parcluster; LUNARC: CLUSTER=cosmos R2023b NSC: CLUSTER=tetralith C3SE: c=parcluster('local'); NO OTHER JOB SETTINGS! Here you start an interactive session first, as shown above! PDC: no CLUSTER , as said above - i.e. just c=parcluster; NO OTHER JOB SETTINGS! Here you instead start an interactive session first! Remember, the project-id is the compute allocation number given for your choice of cluster at the top of this webpage. Since we are just doing a short test, you can use 15 min instead of 1 hour. Also remember the c.AdditionalProperties.ProcsPerNode=20 if you are on UPPMAX. Test that the settings were added by viewing c.AdditionalProperties (not PDC or C3SE).","title":"Job settings at the command line"},{"location":"matlab/slurmMatlab/#running-a-job-from-within-the-matlab-terminal-interface","text":"When starting a simple MATLAB program inside MATLAB on the terminal, by default, it will use your cluster profile which you just created and saved above. If you have a script file called myScript.m , the code to run it as a batch job with the default settings is: >> job = batch ( 'myScript' ); The batch command does not block MATLAB so you can continue working while computations take place. If you want to block MATLAB until the job finishes, use the wait function on the job object, like this: wait ( job ); By default, MATLAB saves the Command Window output from the batch job to the diary of the job. To retrieve it, use the diary function: diary ( job ) After the job finishes, fetch the results by using the load function: load ( job , 'x' ); or using the .fetchOutputs attribute: job . fetchOutputs {:} Tip Other useful job monitoring commands: If you need the Job id, run squeue --me on the command line. To get the MATLAB jobid do id=job.ID within MATLAB. To see if the job is running, inside MATLAB, do job.State","title":"Running a job from within the MATLAB terminal interface"},{"location":"matlab/slurmMatlab/#batch-scripts-containing-matlab-code","text":"Here we will discuss creating a batch script to run MATLAB in - Serial - Parallel While we can submit batch jobs (or even batch jobs of batch jobs) from inside MATLAB (and that may be the most common way of using the batch system with MATLAB), it is also possible to create a batch submit script and use that to run MATLAB. The difference here is that when the batch script has been submitted, you cannot make changes to your job. It is not interactive. That is also an advantage - you can submit the job, log out, and then come back later and see the results. Warning parpool can only be used on UPPMAX, Cosmos, Dardel, Kebnekaise, and Alvis.","title":"Batch scripts containing MATLAB code"},{"location":"matlab/slurmMatlab/#serial-batch-jobs","text":"Here is an example of a serial batch job formatted for each of the 6 facilities covered. UPPMAX HPC2N LUNARC NSC PDC C3SE #!/bin/bash # Change to your actual project number later #SBATCH -A uppmax2025-2-360 # Asking for 1 core #SBATCH -n 1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider MATLAB module add MATLAB/2024a # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Change to your actual project number later #SBATCH -A hpc2n2025-151 # Asking for 1 core #SBATCH -n 1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add MATLAB/2023a.Update4 # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Change to your actual project number later #SBATCH -A lu2025-7-94 # Asking for 1 core #SBATCH -n 1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add matlab/2023b # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Change to your actual project number later #SBATCH -A naiss2025-22-934 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-core=1 # Asking for 15 min (change as you want) #SBATCH -t 00:15:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 module load MATLAB/2024a-hpc1-bdist # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -singleCompThread -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Change to your actual project number later #SBATCH -A naiss2025-22-934 #SBATCH -n 1 # Asking for 15 min (change as you want) #SBATCH -t 00:15:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out #SBATCH -p main # Clean the environment module purge > /dev/null 2 > & 1 module load PDCOLD/23.12 matlab/r2024a-ps # Executing the matlab program monte_carlo_pi.m for the value n=100000 # (n is number of steps - see program). # The command 'time' is timing the execution time matlab -singleCompThread -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A NAISS2025-22-934 #SBATCH -t 00:05:00 #SBATCH -p alvis #SBATCH -N 1 --gpus-per-node=T4:1 ml purge > /dev/null 2 > & 1 module load MATLAB/2024b time matlab -singleCompThread -nojvm -nodisplay -r \"monte_carlo_pi(100000)\" You can download monte_carlo_pi.m here here or find it under matlab in the exercises directory. You submit it with sbatch <batchscript.sh> Where <batchscript.sh> is the name you gave your batchscript. You can find ones for each of the clusters in the exercises -> matlab directory, named monte_carlo_pi_<cluster>.sh . Challenge 4: Try run the serial batch script. Submit it, then check that it is running with squeue --me . Check the output in the matlab_JOBID.out (and the error in the matlab_JOBID.err file).","title":"Serial batch jobs"},{"location":"matlab/slurmMatlab/#parallel-batch-script","text":"This is an example batch script for parallel MATLAB. Adjust to your preferred facility by cross-referencing the tabs above. #!/bin/bash # Change to your actual project number #SBATCH -A XXXX-YY-ZZZ #SBATCH --ntasks-per-node=<how many tasks> #SBATCH --nodes <how many nodes> # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add MATLAB/<version> # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r parallel-matlab-script.m Inside the MATLAB code, the number of CPU-cores ( NumWorkers in MATLAB terminology) can be specified when creating the parallel pool, for example, with 8 processes: >> poolobj = parpool ( 'local' , 8 ); Challenge 5. Write a batch script. Try making a batch script to run the parallel_example.m in the example from inside MATLAB above. You can use the above batch script as template. Solutions UPPMAX HPC2N LUNARC NSC PDC C3SE #!/bin/bash # Change to your actual project number #SBATCH -A uppmax2025-2-360 # Remember, there are 4 workers and 1 master! #SBATCH --ntasks=5 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=5 #SBATCH --ntasks-per-core=1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider MATLAB module add MATLAB/2024a # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Change to your actual project number #SBATCH -A hpc2n2025-151 # Remember, there are 4 workers and 1 master! #SBATCH --ntasks=5 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=5 #SBATCH --ntasks-per-core=1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add MATLAB/2023a.Update4 # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Change to your actual project number #SBATCH -A lu2025-7-94 # Remember, there are 4 workers and 1 master! #SBATCH --ntasks=5 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=5 #SBATCH --ntasks-per-core=1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add matlab/2023b # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Change to your actual project number #SBATCH -A naiss2025-22-934 # Remember, there are 4 workers and 1 master! #SBATCH --ntasks=5 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-core=1 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add MATLAB/2024a-hpc1-bdist # Executing a parallel matlab program srun matlab -nojvm -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Change to your actual project number #SBATCH -A naiss2025-22-934 # Remember, there are 4 workers and 1 master! #SBATCH -p shared #SBATCH -n 5 # Asking for 30 min (change as you want) #SBATCH -t 00:30:00 #SBATCH --error=matlab_%J.err #SBATCH --output=matlab_%J.out # Clean the environment module purge > /dev/null 2 > & 1 # Change depending on resource and MATLAB version # to find out available versions: module spider matlab module add PDCOLD/23.12 matlab/r2024a-ps # Executing a parallel matlab program matlab -nodisplay -nodesktop -nosplash -r \"parallel_example(16)\" #!/bin/bash # Remember to change this to your own project ID after the course! #SBATCH -A NAISS2025-22-934 #SBATCH -t 00:05:00 #SBATCH -p alvis #You always need to ask for GPUs on Alvis! And you should not use it for anything but GPU jobs! #SBATCH -N 1 --gpus-per-node=T4:1 ml purge > /dev/null 2 > & 1 module load MATLAB/2024b time matlab -singleCompThread -nojvm -nodisplay -r \"parallel_example(16)\" GPU code Using MATLAB with GPUs will be covered in the Introduction to GPUs section on the fourth day (\u201cAdvanced material\u201d). Summary The SLURM scheduler handles allocations to the calculation/compute nodes Batch jobs run without interaction with user A batch script consists of a part with SLURM parameters describing the allocation and a second part describing the actual work within the job, for instance one or several MATLAB scripts. You can run MATLAB as a batch job through a batch script or from inside MATLAB (shell or GUI) Remember to include possible input arguments to the MATLAB script in the batch script. You need to configure MATLAB before submitting batch jobs (except on Dardel).","title":"Parallel batch script"},{"location":"r/batch/","text":"Running R in batch mode \u00b6 Questions What is a batch job? How to write a batch script and submit a batch job? Objectives Short introduction to SLURM scheduler Show structure of a batch script Examples to try Compute allocations in this workshop Pelle/Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934 Storage space for this workshop Pelle/Rackham: /proj/r-matlab-julia-pelle Kebnekaise: /proj/nobackup/fall-courses Tetralith: /proj/courses-fall-2025/users/ Dardel: /cfs/klemming/projects/snic/courses-fall-2025 Alvis: /mimer/NOBACKUP/groups/courses-fall-2025/ Overview of the UPPMAX systems \u00b6 Overview of the HPC2N system \u00b6 Overview of the LUNARC system \u00b6 Overview of the NSC system \u00b6 Any longer, resource-intensive, or parallel jobs must be run through a batch script . The batch system used at UPPMAX, HPC2N, LUNARC, NSC, and PDC (and most other HPC centres in Sweden) is called Slurm. Slurm is an Open Source job scheduler, which provides three key functions Keeps track of available system resources Enforces local system resource usage and job scheduling policies Manages a job queue, distributing work across resources according to policies In order to run a batch job, you need to create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script). Guides and documentation at: https://docs.hpc2n.umu.se/documentation/batchsystem/intro/ and https://docs.uppmax.uu.se/cluster_guides/slurm/ and https://lunarc-documentation.readthedocs.io/en/latest/manual/manual_intro/ and https://www.nsc.liu.se/support/batch-jobs/introduction/ and https://support.pdc.kth.se/doc/support-docs/run_jobs/job_scheduling/ Workflow \u00b6 Write a batch script Inside the batch script you need to load the modules you need (R and any prerequisites) If you are using any own-installed packages, make sure R_LIBS_USER is set (export R_LIBS_USER=/path/to/my/R-packages) Ask for resources depending on if it is a parallel job or a serial job, if you need GPUs or not, etc. Give the command(s) to your R script Submit batch script with sbatch <my-batch-script-for-R.sh> Common file extensions for batch scripts are .sh or .batch , but they are not necessary. You can choose any name that makes sense to you. Useful commands to the batch system \u00b6 Submit job: sbatch <jobscript.sh> Get list of your jobs: squeue --me OR squeue -u <username> Check on a specific job: scontrol show job <job-id> Delete a specific job: scancel <job-id> Useful info about a job: sacct -l -j <job-id> | less -S Url to a page with info about the job (Kebnekaise only): job-usage <job-id> .. keypoints:: The Slurm scheduler handles allocations to the calculation nodes Interactive sessions was presented in the previous presentation Batch jobs runs without interaction with the user A batch script consists of a part with Slurm parameters describing the allocation and a second part describing the actual work within the job, for instance one or several R scripts. Remember to include possible input arguments to the R script in the batch script. Warning: Modules on Dardel If you are using Dardel, then note that there are 13 pre-loaded modules when you login, most of which are related to the machine being a Cray. If you do module purge there, they will all be removed together with the application software modules you wanted to purge. This may cause problems. List of modules that are pre-loaded (March 2025) and which will be removed with module purge : craype-x86-rome libfabric/1.20.1 craype-network-ofi perftools-base/23.12.0 xpmem/2.8.2-1.0_3.9__g84a27a5.shasta cce/17.0.0 craype/2.7.30 cray-dsmml/0.2.2 cray-mpich/8.1.28 cray-libsci/23.12.5 PrgEnv-cray/8.5.0 snic-env/1.0.0 You may have to reload all of these if you do module purge . The easiest solution is this: Immediately after logging in, and before loading any modules (assuming you have not added any to .bashrc do module save preload then, when you have done a module purge to remove some application software modules you have loaded (like R and prerequisites) and want to load a different version perhaps, do module restore preload That will restore the preloaded modules. Example R batch scripts \u00b6 Serial code \u00b6 Type-along Short serial batch example for running the code hello.R UPPMAX HPC2N LUNARC NSC PDC C3SE hello.R Short serial example script for Pelle. Loading R/4.4.2 #!/bin/bash -l #SBATCH -A uppmax2025-2-360 # Course project id. Change to your own project ID after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.2 module load R/4.4.2-gfbf-2024a # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Short serial example for running on Kebnekaise. Loading R/4.4.1 and prerequisites #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.1 and prerequisites module load GCC/13.2.0 R/4.4.1 # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Short serial example for running on Cosmos. Loading R/4.2.1 and prerequisites #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.1.2 and prerequisites module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Short serial example for running on Tetralith. Loading R/4.2.2 #!/bin/bash #SBATCH -A naiss2025-22-934 #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.2.2 module load R/4.2.2-hpc1-gcc-11.3.0-bare # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Short serial example for running on Dardel. Loading R/4.4.1 #!/bin/bash -l #SBATCH -A naiss2025-22-934 #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH -p main # Load any modules you need, here R/4.4.1 module load PDC/23.12 R/4.4.1-cpeGNU-23.12 # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Alvis is only for running GPU code. R example code message <- \"Hello World!\" print ( message ) Send the script to the batch: $ sbatch <batch script> Parallel code \u00b6 foreach and doParallel \u00b6 Type-along Short parallel example, using foreach and doParallel UPPMAX HPC2N LUNARC NSC PDC C3SE parallel_foreach.R Short parallel example (Since we are using packages \u201cforeach\u201d and \u201cdoParallel\u201d, you need to use module R/4.4.2-gfbf-2024a and the R-bundle-CRAN/2024.11-foss-2024a module. #!/bin/bash -l #SBATCH -A uppmax2025-2-360 #SBATCH -t 00:10:00 #SBATCH -N 1 #SBATCH -c 4 ml purge > /dev/null 2 > & 1 ml R/4.4.2-gfbf-2024a R-bundle-CRAN/2024.11-foss-2024a # Batch script to submit the R program parallel_foreach.R R -q --slave -f parallel_foreach.R Short parallel example (using packages \u201cforeach\u201d and \u201cdoParallel\u201d which are included in the R module) for running on Kebnekaise. Loading R/4.4.1 and its prerequisites, as well as R-bundle-CRAN/2024.06 and extra prerequisites for that. #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #SBATCH -t 00:10:00 #SBATCH -N 1 #SBATCH -c 4 ml purge > /dev/null 2 > & 1 ml GCC/13.2.0 R/4.4.1 OpenMPI/4.1.6 R-bundle-CRAN/2024.06 # Batch script to submit the R program parallel_foreach.R R -q --slave -f parallel_foreach.R Short parallel example (using packages \u201cforeach\u201d and \u201cdoParallel\u201d which are included in the R module) for running on Cosmos. Loading R/4.2.1 and its prerequisites. #!/bin/bash # A batch script for running the R program parallel_foreach.R #SBATCH -A lu2025-2-94 # Change to your own project ID #SBATCH -t 00:10:00 #SBATCH -N 1 #SBATCH -c 4 ml purge > /dev/null 2 > & 1 ml GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 # Batch script to submit the R program parallel_foreach.R R -q --slave -f parallel_foreach.R Short parallel example (using packages \u201cforeach\u201d and \u201cdoParallel\u201d which you at Tetralith need to install first) for running on Tetralith. Loading R/4.2.2. Installing foreach and doParallel (with R module R/4.2.2-hpc1-gcc-11.3.0-bare loaded but not inside R): R --quiet --no-save --no-restore -e \"install.packages('foreach', repos='http://ftp.acc.umu.se/mirror/CRAN/')\" and R --quiet --no-save --no-restore -e \"install.packages('doParallel', repos='http://ftp.acc.umu.se/mirror/CRAN/')\" #!/bin/bash # A batch script for running the R program parallel_foreach.R #SBATCH -A naiss2025-22-934 #SBATCH -t 00:10:00 #SBATCH -N 1 #SBATCH -c 4 ml purge > /dev/null 2 > & 1 ml R/4.2.2-hpc1-gcc-11.3.0-bare # Batch script to submit the R program parallel_foreach.R R -q --slave -f parallel_foreach.R Short parallel example (using packages \u201cforeach\u201d and \u201cdoParallel\u201d which are included in the R module) for running on Dardel. Loading R/4.4.1. #!/bin/bash -l # A batch script for running the R program parallel_foreach.R #SBATCH -A naiss2025-22-934 #SBATCH -t 00:10:00 #SBATCH -N 1 #SBATCH -c 4 #SBATCH -p main # If you do ml purge you also need to restore the preloaded modules which you should have saved # when you logged in. Otherwise uncomment the two following lines. #ml purge > /dev/null 2>&1 #ml restore preload module load PDC/23.12 module load R/4.4.1-cpeGNU-23.12 # Batch script to submit the R program parallel_foreach.R R -q --slave -f parallel_foreach.R Alvis is only for running GPU code on. This R script uses packages \u201cforeach\u201d and \u201cdoParallel\u201d. library ( parallel ) library ( foreach ) library ( doParallel ) # Function for calculating PI with no values calcpi <- function ( no ) { y <- runif ( no ) x <- runif ( no ) z <- sqrt ( x ^ 2 + y ^ 2 ) length ( which ( z <= 1 )) * 4 / length ( z ) } # Detect the number of cores no_cores <- detectCores () - 1 # Loop to max number of cores for ( n in 1 : no_cores ) { # print how many cores we are using print ( n ) # Set start time start_time <- Sys.time () # Create a cluster nproc <- makeCluster ( n ) registerDoParallel ( nproc ) # Create a vector 1000 length with 100 randomizations input <- rep ( 100 , 1000 ) # Use foreach on n cores registerDoParallel ( nproc ) res <- foreach ( i = input , .combine = '+' ) %dopar% calcpi ( i ) # Print the mean of the results print ( res / length ( input )) # Stop the cluster stopCluster ( nproc ) # print end time print ( Sys.time () - start_time ) } Send the script to the batch: $ sbatch <batch script> Rmpi \u00b6 Type-along Short parallel example using package \u201cRmpi\u201d (\u201cpbdMPI on Dardel\u201d) UPPMAX HPC2N LUNARC NSC PDC Rmpi.R pbdMPI.R Short parallel example (using package \u201cRmpi\u201d, so we need to load both the module R/4.4.2-gfbf-2024a and the module R-bundle-CRAN/2024.11-foss-2024a. A suitable openmpi module, OpenMPI/5.0.3-GCC-13.3.0, is loaded with these.) #!/bin/bash -l #SBATCH -A uppmax2025-2-360 #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 8 export OMPI_MCA_mpi_warn_on_fork = 0 export OMPI_MCA_btl_openib_allow_ib = 1 ml purge > /dev/null 2 > & 1 ml R/4.4.2-gfbf-2024a ml OpenMPI/5.0.3-GCC-13.3.0 R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 mpirun -np 1 R CMD BATCH --no-save --no-restore Rmpi.R output.out Short parallel example (using packages \u201cRmpi\u201d). Loading R/4.4.1 and its prerequisites, as well as R-bundle-CRAN/2024.06 and its prerequisites. #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 8 export OMPI_MCA_mpi_warn_on_fork = 0 ml purge > /dev/null 2 > & 1 ml GCC/13.2.0 R/4.4.1 ml OpenMPI/4.1.6 R-bundle-CRAN/2024.06 mpirun -np 1 Rscript Rmpi.R Short parallel example (using packages \u201cRmpi\u201d). Loading R/4.2.1 and its prerequisites. #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID # Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH -n 8 export OMPI_MCA_mpi_warn_on_fork = 0 ml purge > /dev/null 2 > & 1 ml GCC/11.3.0 OpenMPI/4.1.4 ml R/4.2.1 mpirun -np 1 R CMD BATCH --no-save --no-restore Rmpi.R output.out Short parallel example (using packages \u201cpbdMPI as \u201cRmpi\u201d does not work correctly on NSC). Loading R/4.2.2. Note: for NSC you first need to install \u201cpdbMPI\u201d ( module load R/4.2.2-hpc1-gcc-11.3.0-bare , start R , install.packages('pbdMPI') , pick CRAN mirror (Denmark, Finland, Sweden or other closeby)) #!/bin/bash #SBATCH -A naiss2025-22-934 # Asking for 15 min. #SBATCH -t 00:15:00 #SBATCH -n 8 #SBATCH --exclusive ml purge > /dev/null 2 > & 1 ml R/4.2.2-hpc1-gcc-11.3.0-bare srun --mpi = pmix Rscript pbdMPI.R Short parallel example (using packages \u201cpbdMPI\u201d). Loading R/4.4.1. Note: for PDC you first need to install \u201cpbdMPI\u201d (\u201cRmpi\u201d does not work). You can find the tarball in /cfs/klemming/projects/supr/courses-fall-2025/pbdMPI_0.5-4.tar.gz . Copy it to your own subdirectory under the project directory and then do: module load PDC/24.11 R/4.4.2-cpeGNU-24.11 R CMD INSTALL pbdMPI_0.5-4.tar.gz --configure-args=\" --with-mpi-include=/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/include --with-mpi-libpath=/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib --with-mpi-type=MPICH2\" --no-test-load #!/bin/bash -l #SBATCH -A naiss2025-22-934 # Asking for 10 min. #SBATCH -t 00:10:00 #SBATCH --nodes 2 #SBATCH --ntasks-per-node=8 #SBATCH -p main #SBATCH --output=pbdMPI-test_%J.out # If you do ml purge you also need to restore the preloaded modules which you should have saved # when you logged in. Otherwise leave the two following lines outcommented. #ml purge > /dev/null 2>&1 #ml restore preload ml PDC/24.11 ml R/4.4.2-cpeGNU-24.11 srun -n 4 Rscript pbdMPI.R This R script uses package \u201cRmpi\u201d. # Load the R MPI package if it is not already loaded. if ( ! is.loaded ( \"mpi_initialize\" )) { library ( \"Rmpi\" ) } print ( mpi.universe.size ()) ns <- mpi.universe.size () - 1 mpi.spawn.Rslaves ( nslaves = ns ) # # In case R exits unexpectedly, have it automatically clean up # resources taken up by Rmpi (slaves, memory, etc...) .Last <- function (){ if ( is.loaded ( \"mpi_initialize\" )){ if ( mpi.comm.size ( 1 ) > 0 ){ print ( \"Please use mpi.close.Rslaves() to close slaves.\" ) mpi.close.Rslaves () } print ( \"Please use mpi.quit() to quit R\" ) .Call ( \"mpi_finalize\" ) } } # Tell all slaves to return a message identifying themselves mpi.remote.exec ( paste ( \"I am\" , mpi.comm.rank (), \"of\" , mpi.comm.size (), system ( \"hostname\" , intern = T ))) # Test computations x <- 5 x <- mpi.remote.exec ( rnorm , x ) length ( x ) x # Tell all slaves to close down, and exit the program mpi.close.Rslaves () mpi.quit () This R script uses package \u201cpbdMPI\u201d. library ( pbdMPI ) ns <- comm.size () # Tell all R sessions to return a message identifying themselves id <- comm.rank () ns <- comm.size () host <- system ( \"hostname\" , intern = TRUE ) comm.cat ( \"I am\" , id , \"on\" , host , \"of\" , ns , \"\\n\" , all.rank = TRUE ) # Test computations x <- 5 x <- rnorm ( x ) comm.print ( length ( x )) comm.print ( x , all.rank = TRUE ) finalize () Send the script to the batch system: $ sbatch <batch script> Using GPUs in a batch job \u00b6 There are generally either not GPUs on the login nodes or they cannot be accessed for computations. To use them you need to either launch an interactive job or submit a batch job. UPPMAX only \u00b6 Rackham/Snowy or Pelle Rackham\u2019s compute nodes do not have GPUs. You need to use Snowy for that. The new cluster Pelle has GPUs. On Rackham, you need to use this batch command (for x being the number of cards, 1 or 2): #SBATCH -M snowy #SBATCH --gres=gpu:x On Pelle, you need to use this batch command for L40s GPUs (up to 10 GPU cards) #SBATCH -p gpu #SBATCH --gpus:l40s:<number of GPUs> or for H100 GPUs (up to 2 GPU cards) #SBATCH -p gpu #SBATCH --gpus=h100:<number of GPUs> HPC2N \u00b6 Kebnekaise\u2019s GPU nodes are considered a separate resource, and the regular compute nodes do not have GPUs. Kebnekaise has a great many different types of GPUs: V100 (2 cards/node) A40 (8 cards/node) A6000 (2 cards/node) L40s (2 or 6 cards/node) A100 (2 cards/node) H100 (4 cards/node) MI100 (2 cards/node) To access them, you need to use this to the batch system: #SBATCH --gpus=x where x is the number of GPU cards you want. Above are given how many are on each type, so you can ask for up to that number. In addition, you need to add this to the batch system: #SBATCH -C <type> where type is v100 a40 a6000 l40s a100 h100 mi100 For more information, see HPC2N\u2019s guide to the different parts of the batch system: https://docs.hpc2n.umu.se/documentation/batchsystem/resources/ LUNARC \u00b6 LUNARC has Nvidia A100 GPUs and Nvidia A40 GPUs, but the latter ones are reserved for interactive graphics work on the on-demand system, and Slurm jobs should not be submitted to them. Thus in order to use the A100 GPUs on Cosmos, add this to your batch script: A100 GPUs on AMD nodes: #SBATCH -p gpua100 #SBATCH --gres=gpu:1 These nodes are configured as exclusive access and will not be shared between users. User projects will be charged for the entire node (48 cores). A job on a node will also have access to all memory on the node. A100 GPUs on Intel nodes: #SBATCH -p gpua100i #SBATCH --gres=gpu:<number> where is 1 or 2 (Two of the nodes have 1 GPU and two have 2 GPUs). NSC \u00b6 Tetralith has Nvidia T4 GPUs. In order to access them, add this to your batch script or interactive job: #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 PDC \u00b6 Dardel has AMD Instinct\u2122 MI250X GPU chips. In order to access them, add this to your batch script or interactive job: #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu C3SE \u00b6 Alvis is meant for GPU jobs. There is no node-sharing on multi-node jobs (\u2013exclusive is automatic). NOTE: Requesting -N 1 does not mean 1 full node #SBATCH -p alvis #SBATCH -N <nodes> #SBATCH --gpus-per-node=<type>:x where <type> is one of V100 T4 A100 and x is number of GPU cards 1-4 for V100 1-8 for T4 1-4 for A100 Note For more about how to use GPUs at the centres, see the [\u201cIntroduction to GPUs\u201d](../../advanced/gpus] section. Exercises \u00b6 Challenge: Serial batch script for R Run the serial batch script from further up on the page, but for the add2.R code. Remember the arguments. This is not for Alvis since that resource is only for GPU code. Solution for UPPMAX Serial script on Rackham #!/bin/bash -l #SBATCH -A uppmax2025-2-360 # Change to your own after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for R/4.1.1 module load R/4.1.1 # Run your R script Rscript add2.R 2 3 Same for Pelle, except you should use R/4.4.2-gfbf-2024a Solution for HPC2N Serial script on Kebnekaise #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for R/4.1.2 module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 # Run your R script Rscript add2.R 2 3 Solution for LUNARC Serial script on Cosmos #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for R/4.2.1 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 # Run your R script Rscript add2.R 2 3 Solution for NSC Serial script on Tetralith #!/bin/bash #SBATCH -A naiss2025-22-934 #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for R/4.2.2 module load R/4.2.2-hpc1-gcc-11.3.0-bare # Run your R script Rscript add2.R 2 3 Solution for PDC Serial script on Dardel #!/bin/bash #SBATCH -A naiss2025-22-934 #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH -p main # Load any modules you need, here for R/4.4.1 module load PDC/23.12 R/4.4.1-cpeGNU-23.12 # Run your R script Rscript add2.R 2 3 Challenge: Parallel job run Try making a batch script for running the parallel example with \u201cforeach\u201d from further up on the page.","title":"Batch"},{"location":"r/batch/#running-r-in-batch-mode","text":"Questions What is a batch job? How to write a batch script and submit a batch job? Objectives Short introduction to SLURM scheduler Show structure of a batch script Examples to try Compute allocations in this workshop Pelle/Rackham: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934 Storage space for this workshop Pelle/Rackham: /proj/r-matlab-julia-pelle Kebnekaise: /proj/nobackup/fall-courses Tetralith: /proj/courses-fall-2025/users/ Dardel: /cfs/klemming/projects/snic/courses-fall-2025 Alvis: /mimer/NOBACKUP/groups/courses-fall-2025/","title":"Running R in batch mode"},{"location":"r/batch/#overview-of-the-uppmax-systems","text":"","title":"Overview of the UPPMAX systems"},{"location":"r/batch/#overview-of-the-hpc2n-system","text":"","title":"Overview of the HPC2N system"},{"location":"r/batch/#overview-of-the-lunarc-system","text":"","title":"Overview of the LUNARC system"},{"location":"r/batch/#overview-of-the-nsc-system","text":"Any longer, resource-intensive, or parallel jobs must be run through a batch script . The batch system used at UPPMAX, HPC2N, LUNARC, NSC, and PDC (and most other HPC centres in Sweden) is called Slurm. Slurm is an Open Source job scheduler, which provides three key functions Keeps track of available system resources Enforces local system resource usage and job scheduling policies Manages a job queue, distributing work across resources according to policies In order to run a batch job, you need to create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script). Guides and documentation at: https://docs.hpc2n.umu.se/documentation/batchsystem/intro/ and https://docs.uppmax.uu.se/cluster_guides/slurm/ and https://lunarc-documentation.readthedocs.io/en/latest/manual/manual_intro/ and https://www.nsc.liu.se/support/batch-jobs/introduction/ and https://support.pdc.kth.se/doc/support-docs/run_jobs/job_scheduling/","title":"Overview of the NSC system"},{"location":"r/batch/#workflow","text":"Write a batch script Inside the batch script you need to load the modules you need (R and any prerequisites) If you are using any own-installed packages, make sure R_LIBS_USER is set (export R_LIBS_USER=/path/to/my/R-packages) Ask for resources depending on if it is a parallel job or a serial job, if you need GPUs or not, etc. Give the command(s) to your R script Submit batch script with sbatch <my-batch-script-for-R.sh> Common file extensions for batch scripts are .sh or .batch , but they are not necessary. You can choose any name that makes sense to you.","title":"Workflow"},{"location":"r/batch/#useful-commands-to-the-batch-system","text":"Submit job: sbatch <jobscript.sh> Get list of your jobs: squeue --me OR squeue -u <username> Check on a specific job: scontrol show job <job-id> Delete a specific job: scancel <job-id> Useful info about a job: sacct -l -j <job-id> | less -S Url to a page with info about the job (Kebnekaise only): job-usage <job-id> .. keypoints:: The Slurm scheduler handles allocations to the calculation nodes Interactive sessions was presented in the previous presentation Batch jobs runs without interaction with the user A batch script consists of a part with Slurm parameters describing the allocation and a second part describing the actual work within the job, for instance one or several R scripts. Remember to include possible input arguments to the R script in the batch script. Warning: Modules on Dardel If you are using Dardel, then note that there are 13 pre-loaded modules when you login, most of which are related to the machine being a Cray. If you do module purge there, they will all be removed together with the application software modules you wanted to purge. This may cause problems. List of modules that are pre-loaded (March 2025) and which will be removed with module purge : craype-x86-rome libfabric/1.20.1 craype-network-ofi perftools-base/23.12.0 xpmem/2.8.2-1.0_3.9__g84a27a5.shasta cce/17.0.0 craype/2.7.30 cray-dsmml/0.2.2 cray-mpich/8.1.28 cray-libsci/23.12.5 PrgEnv-cray/8.5.0 snic-env/1.0.0 You may have to reload all of these if you do module purge . The easiest solution is this: Immediately after logging in, and before loading any modules (assuming you have not added any to .bashrc do module save preload then, when you have done a module purge to remove some application software modules you have loaded (like R and prerequisites) and want to load a different version perhaps, do module restore preload That will restore the preloaded modules.","title":"Useful commands to the batch system"},{"location":"r/batch/#example-r-batch-scripts","text":"","title":"Example R batch scripts"},{"location":"r/batch/#serial-code","text":"Type-along Short serial batch example for running the code hello.R UPPMAX HPC2N LUNARC NSC PDC C3SE hello.R Short serial example script for Pelle. Loading R/4.4.2 #!/bin/bash -l #SBATCH -A uppmax2025-2-360 # Course project id. Change to your own project ID after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.2 module load R/4.4.2-gfbf-2024a # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Short serial example for running on Kebnekaise. Loading R/4.4.1 and prerequisites #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.4.1 and prerequisites module load GCC/13.2.0 R/4.4.1 # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Short serial example for running on Cosmos. Loading R/4.2.1 and prerequisites #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.1.2 and prerequisites module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Short serial example for running on Tetralith. Loading R/4.2.2 #!/bin/bash #SBATCH -A naiss2025-22-934 #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here R/4.2.2 module load R/4.2.2-hpc1-gcc-11.3.0-bare # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Short serial example for running on Dardel. Loading R/4.4.1 #!/bin/bash -l #SBATCH -A naiss2025-22-934 #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH -p main # Load any modules you need, here R/4.4.1 module load PDC/23.12 R/4.4.1-cpeGNU-23.12 # Run your R script (here 'hello.R') R --no-save --quiet < hello.R Alvis is only for running GPU code. R example code message <- \"Hello World!\" print ( message ) Send the script to the batch: $ sbatch <batch script>","title":"Serial code"},{"location":"r/batch/#parallel-code","text":"","title":"Parallel code"},{"location":"r/batch/#using-gpus-in-a-batch-job","text":"There are generally either not GPUs on the login nodes or they cannot be accessed for computations. To use them you need to either launch an interactive job or submit a batch job.","title":"Using GPUs in a batch job"},{"location":"r/batch/#exercises","text":"Challenge: Serial batch script for R Run the serial batch script from further up on the page, but for the add2.R code. Remember the arguments. This is not for Alvis since that resource is only for GPU code. Solution for UPPMAX Serial script on Rackham #!/bin/bash -l #SBATCH -A uppmax2025-2-360 # Change to your own after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for R/4.1.1 module load R/4.1.1 # Run your R script Rscript add2.R 2 3 Same for Pelle, except you should use R/4.4.2-gfbf-2024a Solution for HPC2N Serial script on Kebnekaise #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for R/4.1.2 module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 # Run your R script Rscript add2.R 2 3 Solution for LUNARC Serial script on Cosmos #!/bin/bash #SBATCH -A lu2025-2-94 # Change to your own project ID #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for R/4.2.1 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 # Run your R script Rscript add2.R 2 3 Solution for NSC Serial script on Tetralith #!/bin/bash #SBATCH -A naiss2025-22-934 #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for R/4.2.2 module load R/4.2.2-hpc1-gcc-11.3.0-bare # Run your R script Rscript add2.R 2 3 Solution for PDC Serial script on Dardel #!/bin/bash #SBATCH -A naiss2025-22-934 #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH -p main # Load any modules you need, here for R/4.4.1 module load PDC/23.12 R/4.4.1-cpeGNU-23.12 # Run your R script Rscript add2.R 2 3 Challenge: Parallel job run Try making a batch script for running the parallel example with \u201cforeach\u201d from further up on the page.","title":"Exercises"},{"location":"r/evaluation/","text":"Evaluation \u00b6 The evaluation form for the R part can be found here . It takes into account that one may need to leave early too. This is the page for evaluating the current iteration of the course. Where can I find the results of earlier evaluations? At the \u2018Evaluations\u2019 page . Evaluation questions \u00b6 Evaluation questions form Why do you evaluate under lesson hours? Because we value your time: your free time should be your free time. We think the time lost teaching is worth it to improve our teaching. For teachers: what is in that form? Question 1: Overall, how would you rate today\u2019s training event? A grade from 1 to (and including) 10 Question 2: What do you think about the pace of teaching overall? [Free text] Question 3: Confidences I can find the module to be able to run R I can load the module to be able to run R I can run the R interpreter I can run the R command to get the list of installed R packages I can run an R script from the command-line I can find out if an R package is already installed I can load the pre-installed R packages I can install an R package from CRAN I can submit a job to the scheduler to run an R script with regular code I can submit a job to the scheduler to run an R script that uses parallel code I can start an interactive session I can verify I am on the login node yes/no I can start an interactive session with multiple cores I can start RStudio Coding scheme: Column text Description NA I did not attend that session 0 I have no confidence I can do this 1 I have low confidence I can do this 2 I have some confidence I can do this 3 I have good confidence I can do this 4 I can absolutely do this! Question 4: Would you recommend this course to someone else? Multiple choice: Yes, no, not sure Question 5: Which future training topics would you like to be provided by the training host(s)? [Free text] Question 6: Do you have any additional comments? [Free text]","title":"Evaluation"},{"location":"r/evaluation/#evaluation","text":"The evaluation form for the R part can be found here . It takes into account that one may need to leave early too. This is the page for evaluating the current iteration of the course. Where can I find the results of earlier evaluations? At the \u2018Evaluations\u2019 page .","title":"Evaluation"},{"location":"r/evaluation/#evaluation-questions","text":"Evaluation questions form Why do you evaluate under lesson hours? Because we value your time: your free time should be your free time. We think the time lost teaching is worth it to improve our teaching. For teachers: what is in that form? Question 1: Overall, how would you rate today\u2019s training event? A grade from 1 to (and including) 10 Question 2: What do you think about the pace of teaching overall? [Free text] Question 3: Confidences I can find the module to be able to run R I can load the module to be able to run R I can run the R interpreter I can run the R command to get the list of installed R packages I can run an R script from the command-line I can find out if an R package is already installed I can load the pre-installed R packages I can install an R package from CRAN I can submit a job to the scheduler to run an R script with regular code I can submit a job to the scheduler to run an R script that uses parallel code I can start an interactive session I can verify I am on the login node yes/no I can start an interactive session with multiple cores I can start RStudio Coding scheme: Column text Description NA I did not attend that session 0 I have no confidence I can do this 1 I have low confidence I can do this 2 I have some confidence I can do this 3 I have good confidence I can do this 4 I can absolutely do this! Question 4: Would you recommend this course to someone else? Multiple choice: Yes, no, not sure Question 5: Which future training topics would you like to be provided by the training host(s)? [Free text] Question 6: Do you have any additional comments? [Free text]","title":"Evaluation questions"},{"location":"r/interactive/","text":"Interactive work on the compute nodes \u00b6 Note It is possible to run R directly on the login (including ThinLinc) nodes. should only be done for short and small jobs otherwise the login node becomes slow for all users. If you want to work interactively with your code or data, you should start an interactive session . this includes Open OnDemand at those centres that have it If you rather will run a script which won\u2019t use any interactive user input while running , you should start a batch job , see previous session. Questions How do I proceed to work interactively on a compute node Objectives Show how to reach the calculation nodes on UPPMAX and HPC2N Test some commands on the calculation nodes Compute allocations in this workshop Rackham/Pelle: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934 Overview of the UPPMAX systems \u00b6 Overview of the HPC2N system \u00b6 Overview of the LUNARC system \u00b6 Overview of the NSC system \u00b6 General \u00b6 In order to run interactively, you need to have compute nodes allocated to run on, and this is done through the batch system. Warning (HPC2N) Do note that it (salloc) is not real interactivity as you probably mean it, as you will have to run it as a R script instead of by starting R and giving commands inside it. The reason for this is that you are not actually logged into the compute node and only sees the output of the commands you run. You also need to preface with srun or it will run on the login node. The way to get real interactivity at HPC2N is to use the Open OnDemand desktop (https://portal.hpc2n.umu.se) R \u201cinteractively\u201d on the compute nodes \u00b6 To run interactively, you need to allocate resources on the cluster first. You can use the command salloc / interactive to allow interactive use of resources allocated to your job. When the resources are allocated, if you are using HPC2N and salloc you need to preface commands with srun in order to run on the allocated nodes instead of the login node. First, you make a request for resources with interactive / salloc , like this: UPPMAX (interactive) HPC2N (salloc) LUNARC (interactive) NSC (interactive) PDC (salloc + ssh) C3SE $ interactive -n <tasks> --time = HHH:MM:SS -A uppmax2025-2-360 $ salloc -n <tasks> --time = HHH:MM:SS -A hpc2n2024-025 interactive -A lu2025-2-94 -t HHH:MM:SS interactive -A naiss2025-22-934 salloc --time = HHH:MM:SS -A naiss2025-22-934 -p [ partition ] Where [partition] is main or gpu. After your resources have been allocated, you can login to the compute node with ssh <node> and have real interactivity. srun -A NAISS2025-22-934 -p alvis --gpus-per-node = T4:1 --pty bash Your request enters the job queue just like any other job, and interactive/salloc will tell you that it is waiting for the requested resources. When salloc tells you that your job has been allocated resources, you can interactively run programs on those resources with srun (or login to the compute node with ssh on PDC). The commands you run with srun will then be executed on the resources your job has been allocated. If you do not preface with srun the command is run on the login node! You can now run R scripts on the allocated resources directly instead of waiting for your batch job to return a result. This is an advantage if you want to test your R script or perhaps figure out which parameters are best. Warning Let us use ThinLinc or Open OnDemand (where available) UPPMAX HPC2N LUNARC NSC (Tetralith) PDC C3SE (Alvis) ThinLinc app: <user>@rackham-gui.uppmax.uu.se ThinLinc in web browser: https://rackham-gui.uppmax.uu.se This requires 2FA! ThinLinc (Pelle): <user>@pelle-gui.uppmax.uu.se ThinLinc (Pelle) in web browser: https://pelle-gui.uppmax.uu.se This requires 2FA! ThinLinc: kebnekaise-tl.hpc2n.umu.se From web browser: https://kebnekaise-tl.hpc2n.umu.se:300/ OpenOnDemand: https://portal.hpc2n.umu.se (click https://docs.hpc2n.umu.se/tutorials/connections/#interactive__apps__-__rstudio__server ) ThinLinc: cosmos-dt.lunarc.lu.se Note that you need to setup TFA (PocketPass) to use LUNARC! You can start Gfx (Open OnDemand) from inside ThinLinc! ThinLinc: tetralith.pdc.kth.se 2FA required! https://www.nsc.liu.se/support/2fa/ ThinLinc: dardel-vnc.nsc.kth.se SSH keys or kerberos needed! https://support.pdc.kth.se/doc/support/?section=/doc/support-docs/basics/quickstart OpenOnDemand: https://alvis.c3se.chalmers.se https://www.c3se.chalmers.se/documentation/connecting/remote_graphics/ Note that Alvis is accessible via SUNET networks (i.e. most Swedish university networks). If you are not on one of those networks you need to use a VPN - preferrably your own Swedish university VPN. If this is not possible, contact support@chalmers.se and ask to be added to the Chalmers\u2019s eduVPN. Using terminal NSC HPC2N UPPMAX LUNARC PDC C3SE SSH: ssh <user>@tetralith.nsc.liu.se Note that you need to setup TFA to use NSC! SSH: ssh <user>@kebnekaise.hpc2n.umu.se Rackham SSH: ssh <user>@rackham.uppmax.uu.se Note that you may have to setup <a href=\u201dhttps://docs.uppmax.uu.se/getting_started/get_uppmax_2fa/\u201d * Pelle SSH: ssh <user>@pelle.uppmax.uu.se Note that you may have to setup TFA for Uppmax when using either of the ThinLinc connections. SSH: ssh <user>@cosmos.lunarc.lu.se Note that you need to setup TFA (PocketPass) to use LUNARC! SSH: ssh <user>@dardel.pdc.kth.se Note that you need to setup SSH keys or kerberos in order to login to PDC! SSH: ssh <user>@alvis1.c3se.chalmers.se or ssh <user>@alvis2.c3se.chalmers.se Note that Alvis is accessible via SUNET networks (i.e. most Swedish university networks). If you are not on one of those networks you need to use a VPN - preferrably your own Swedish university VPN. If this is not possible, contact support@chalmers.se and ask to be added to the Chalmers\u2019s eduVPN. In any case: Remember to have X11 installed! On Mac install XQuartz On Windows Use MobaXterm or install XMING and use with Putty or PowerShell Example \u00b6 Requesting 4 cores for 10 minutes, then running R UPPMAX [ bjornc@pelle2 ~ ] $ interactive -A uppmax2025-2-360 -n 4 -t 10 :00 This is a temporary version of interactive-script for Pelle Most interactive-script functionality is removed salloc: Pending job allocation 82050 salloc: job 82050 queued and waiting for resources salloc: job 82050 has been allocated resources salloc: Granted job allocation 82050 salloc: Waiting for resource configuration salloc: Nodes p102 are ready for job [ bjornc@p102 ~ ] $ Let us check that we actually run on the compute node: [ bjornc@p102 ~ ] $ srun hostname p102.uppmax.uu.se p102.uppmax.uu.se p102.uppmax.uu.se p102.uppmax.uu.se We are! Notice that we got a response from all four cores we have allocated. HPC2N [ ~ ] $ salloc -n 4 --time = 00 :30:00 -A hpc2n2024-025 salloc: Pending job allocation 20174806 salloc: job 20174806 queued and waiting for resources salloc: job 20174806 has been allocated resources salloc: Granted job allocation 20174806 salloc: Waiting for resource configuration salloc: Nodes b-cn0241 are ready for job [ ~ ] $ module load GCC/12.2.0 OpenMPI/4.1.4 R/4.2.2 [ ~ ] $ Let us check that we actually run on the compute node: [ ~ ] $ srun hostname b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se We are. Notice that we got a response from all four cores we have allocated. Running a script Warning You need to reload all modules you used on the login node!!! The script Adding two numbers from user input ( serial_sum.R ) You will find it in the exercise directory exercises/r/ so go there with cd . Otherwise, use your favourite editor and add the text below and save as serial_sum.R . # This program will add two numbers that are provided by the user args = commandArgs ( trailingOnly = TRUE ) res = as.numeric ( args [ 1 ]) + as.numeric ( args [ 2 ]) print ( paste ( \"The sum of the two numbers is\" , res )) Running the script Note that the commands are the same for both HPC2N and UPPMAX! Running a R script in the allocation we made further up. Notice that since we asked for 4 cores, the script is run 4 times, since it is a serial script $ srun Rscript serial_sum.R 3 4 [ 1 ] \"The sum of the two numbers is 7\" [ 1 ] \"The sum of the two numbers is 7\" [ 1 ] \"The sum of the two numbers is 7\" [ 1 ] \"The sum of the two numbers is 7\" Without the srun command, R won\u2019t understand that it can use several cores. Therefore the program is run only once. $ Rscript serial_sum.R 3 4 [ 1 ] \"The sum of the two numbers is 7\" Running R with workers First start R and check available workers with future . Create a R script called `script-workers.R`` with the following content: library ( future ) availableWorkers () availableCores () Execute the code with srun -n 1 -c 4 Rscript script-workers.R Exit \u00b6 When you have finished using the allocation, either wait for it to end, or close it with exit Don\u2019t do it now! We shall test RStudio first in the next session! UPPMAX HPC2N [ bjornc@p102 ~ ] $ exit exit [ screen is terminating ] Connection to p102 closed. [ bjornc@pelle2 ~ ] $ [ ~ ] $ exit exit salloc: Relinquishing job allocation 20174806 salloc: Job allocation 20174806 has been revoked. [ ~ ] $ Keypoints Start an interactive session on a calculation node by a SLURM allocation At HPC2N: salloc \u2026 At UPPMAX: interactive \u2026 Follow the same procedure as usual by loading the R module and possible prerequisites.","title":"Interactive work on the compute nodes"},{"location":"r/interactive/#interactive-work-on-the-compute-nodes","text":"Note It is possible to run R directly on the login (including ThinLinc) nodes. should only be done for short and small jobs otherwise the login node becomes slow for all users. If you want to work interactively with your code or data, you should start an interactive session . this includes Open OnDemand at those centres that have it If you rather will run a script which won\u2019t use any interactive user input while running , you should start a batch job , see previous session. Questions How do I proceed to work interactively on a compute node Objectives Show how to reach the calculation nodes on UPPMAX and HPC2N Test some commands on the calculation nodes Compute allocations in this workshop Rackham/Pelle: uppmax2025-2-360 Kebnekaise: hpc2n2025-151 Cosmos: lu2025-2-94 Tetralith: naiss2025-22-934 Dardel: naiss2025-22-934 Alvis: naiss2025-22-934","title":"Interactive work on the compute nodes"},{"location":"r/interactive/#overview-of-the-uppmax-systems","text":"","title":"Overview of the UPPMAX systems"},{"location":"r/interactive/#overview-of-the-hpc2n-system","text":"","title":"Overview of the HPC2N system"},{"location":"r/interactive/#overview-of-the-lunarc-system","text":"","title":"Overview of the LUNARC system"},{"location":"r/interactive/#overview-of-the-nsc-system","text":"","title":"Overview of the NSC system"},{"location":"r/interactive/#general","text":"In order to run interactively, you need to have compute nodes allocated to run on, and this is done through the batch system. Warning (HPC2N) Do note that it (salloc) is not real interactivity as you probably mean it, as you will have to run it as a R script instead of by starting R and giving commands inside it. The reason for this is that you are not actually logged into the compute node and only sees the output of the commands you run. You also need to preface with srun or it will run on the login node. The way to get real interactivity at HPC2N is to use the Open OnDemand desktop (https://portal.hpc2n.umu.se)","title":"General"},{"location":"r/interactive/#r-interactively-on-the-compute-nodes","text":"To run interactively, you need to allocate resources on the cluster first. You can use the command salloc / interactive to allow interactive use of resources allocated to your job. When the resources are allocated, if you are using HPC2N and salloc you need to preface commands with srun in order to run on the allocated nodes instead of the login node. First, you make a request for resources with interactive / salloc , like this: UPPMAX (interactive) HPC2N (salloc) LUNARC (interactive) NSC (interactive) PDC (salloc + ssh) C3SE $ interactive -n <tasks> --time = HHH:MM:SS -A uppmax2025-2-360 $ salloc -n <tasks> --time = HHH:MM:SS -A hpc2n2024-025 interactive -A lu2025-2-94 -t HHH:MM:SS interactive -A naiss2025-22-934 salloc --time = HHH:MM:SS -A naiss2025-22-934 -p [ partition ] Where [partition] is main or gpu. After your resources have been allocated, you can login to the compute node with ssh <node> and have real interactivity. srun -A NAISS2025-22-934 -p alvis --gpus-per-node = T4:1 --pty bash Your request enters the job queue just like any other job, and interactive/salloc will tell you that it is waiting for the requested resources. When salloc tells you that your job has been allocated resources, you can interactively run programs on those resources with srun (or login to the compute node with ssh on PDC). The commands you run with srun will then be executed on the resources your job has been allocated. If you do not preface with srun the command is run on the login node! You can now run R scripts on the allocated resources directly instead of waiting for your batch job to return a result. This is an advantage if you want to test your R script or perhaps figure out which parameters are best. Warning Let us use ThinLinc or Open OnDemand (where available) UPPMAX HPC2N LUNARC NSC (Tetralith) PDC C3SE (Alvis) ThinLinc app: <user>@rackham-gui.uppmax.uu.se ThinLinc in web browser: https://rackham-gui.uppmax.uu.se This requires 2FA! ThinLinc (Pelle): <user>@pelle-gui.uppmax.uu.se ThinLinc (Pelle) in web browser: https://pelle-gui.uppmax.uu.se This requires 2FA! ThinLinc: kebnekaise-tl.hpc2n.umu.se From web browser: https://kebnekaise-tl.hpc2n.umu.se:300/ OpenOnDemand: https://portal.hpc2n.umu.se (click https://docs.hpc2n.umu.se/tutorials/connections/#interactive__apps__-__rstudio__server ) ThinLinc: cosmos-dt.lunarc.lu.se Note that you need to setup TFA (PocketPass) to use LUNARC! You can start Gfx (Open OnDemand) from inside ThinLinc! ThinLinc: tetralith.pdc.kth.se 2FA required! https://www.nsc.liu.se/support/2fa/ ThinLinc: dardel-vnc.nsc.kth.se SSH keys or kerberos needed! https://support.pdc.kth.se/doc/support/?section=/doc/support-docs/basics/quickstart OpenOnDemand: https://alvis.c3se.chalmers.se https://www.c3se.chalmers.se/documentation/connecting/remote_graphics/ Note that Alvis is accessible via SUNET networks (i.e. most Swedish university networks). If you are not on one of those networks you need to use a VPN - preferrably your own Swedish university VPN. If this is not possible, contact support@chalmers.se and ask to be added to the Chalmers\u2019s eduVPN. Using terminal NSC HPC2N UPPMAX LUNARC PDC C3SE SSH: ssh <user>@tetralith.nsc.liu.se Note that you need to setup TFA to use NSC! SSH: ssh <user>@kebnekaise.hpc2n.umu.se Rackham SSH: ssh <user>@rackham.uppmax.uu.se Note that you may have to setup <a href=\u201dhttps://docs.uppmax.uu.se/getting_started/get_uppmax_2fa/\u201d * Pelle SSH: ssh <user>@pelle.uppmax.uu.se Note that you may have to setup TFA for Uppmax when using either of the ThinLinc connections. SSH: ssh <user>@cosmos.lunarc.lu.se Note that you need to setup TFA (PocketPass) to use LUNARC! SSH: ssh <user>@dardel.pdc.kth.se Note that you need to setup SSH keys or kerberos in order to login to PDC! SSH: ssh <user>@alvis1.c3se.chalmers.se or ssh <user>@alvis2.c3se.chalmers.se Note that Alvis is accessible via SUNET networks (i.e. most Swedish university networks). If you are not on one of those networks you need to use a VPN - preferrably your own Swedish university VPN. If this is not possible, contact support@chalmers.se and ask to be added to the Chalmers\u2019s eduVPN. In any case: Remember to have X11 installed! On Mac install XQuartz On Windows Use MobaXterm or install XMING and use with Putty or PowerShell","title":"R &ldquo;interactively&rdquo; on the compute nodes"},{"location":"r/interactive/#example","text":"Requesting 4 cores for 10 minutes, then running R UPPMAX [ bjornc@pelle2 ~ ] $ interactive -A uppmax2025-2-360 -n 4 -t 10 :00 This is a temporary version of interactive-script for Pelle Most interactive-script functionality is removed salloc: Pending job allocation 82050 salloc: job 82050 queued and waiting for resources salloc: job 82050 has been allocated resources salloc: Granted job allocation 82050 salloc: Waiting for resource configuration salloc: Nodes p102 are ready for job [ bjornc@p102 ~ ] $ Let us check that we actually run on the compute node: [ bjornc@p102 ~ ] $ srun hostname p102.uppmax.uu.se p102.uppmax.uu.se p102.uppmax.uu.se p102.uppmax.uu.se We are! Notice that we got a response from all four cores we have allocated. HPC2N [ ~ ] $ salloc -n 4 --time = 00 :30:00 -A hpc2n2024-025 salloc: Pending job allocation 20174806 salloc: job 20174806 queued and waiting for resources salloc: job 20174806 has been allocated resources salloc: Granted job allocation 20174806 salloc: Waiting for resource configuration salloc: Nodes b-cn0241 are ready for job [ ~ ] $ module load GCC/12.2.0 OpenMPI/4.1.4 R/4.2.2 [ ~ ] $ Let us check that we actually run on the compute node: [ ~ ] $ srun hostname b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se b-cn0241.hpc2n.umu.se We are. Notice that we got a response from all four cores we have allocated. Running a script Warning You need to reload all modules you used on the login node!!! The script Adding two numbers from user input ( serial_sum.R ) You will find it in the exercise directory exercises/r/ so go there with cd . Otherwise, use your favourite editor and add the text below and save as serial_sum.R . # This program will add two numbers that are provided by the user args = commandArgs ( trailingOnly = TRUE ) res = as.numeric ( args [ 1 ]) + as.numeric ( args [ 2 ]) print ( paste ( \"The sum of the two numbers is\" , res )) Running the script Note that the commands are the same for both HPC2N and UPPMAX! Running a R script in the allocation we made further up. Notice that since we asked for 4 cores, the script is run 4 times, since it is a serial script $ srun Rscript serial_sum.R 3 4 [ 1 ] \"The sum of the two numbers is 7\" [ 1 ] \"The sum of the two numbers is 7\" [ 1 ] \"The sum of the two numbers is 7\" [ 1 ] \"The sum of the two numbers is 7\" Without the srun command, R won\u2019t understand that it can use several cores. Therefore the program is run only once. $ Rscript serial_sum.R 3 4 [ 1 ] \"The sum of the two numbers is 7\" Running R with workers First start R and check available workers with future . Create a R script called `script-workers.R`` with the following content: library ( future ) availableWorkers () availableCores () Execute the code with srun -n 1 -c 4 Rscript script-workers.R","title":"Example"},{"location":"r/interactive/#exit","text":"When you have finished using the allocation, either wait for it to end, or close it with exit Don\u2019t do it now! We shall test RStudio first in the next session! UPPMAX HPC2N [ bjornc@p102 ~ ] $ exit exit [ screen is terminating ] Connection to p102 closed. [ bjornc@pelle2 ~ ] $ [ ~ ] $ exit exit salloc: Relinquishing job allocation 20174806 salloc: Job allocation 20174806 has been revoked. [ ~ ] $ Keypoints Start an interactive session on a calculation node by a SLURM allocation At HPC2N: salloc \u2026 At UPPMAX: interactive \u2026 Follow the same procedure as usual by loading the R module and possible prerequisites.","title":"Exit"},{"location":"r/load_run/","text":"Load and run R \u00b6 Learning outcomes Practice using the documentation of your HPC cluster Load an R module Start the R interpreter Run an R script Download and extract the exercise files (optional) Find the different R modules (optional) See the list of installed R packages For teachers Teaching goals are: Learners have practiced using the documentation of their HPC clusters Learners have loaded the module to be able to run R Learners have run the R interpreter Learners have run an R script from the command-line Learners have downloaded and extracted the exercise files (optional) Learners have found the different R modules (optional) Learners have seen the list of installed R packages Prior: What is \u2018HPC\u2019 in \u2018HPC cluster\u2019? What is \u2018cluster\u2019 in \u2018HPC cluster\u2019? What is a software module? What is a script? What are features of using an HPC cluster? What problem would arise if users can install their own software? Introduction \u00b6 You want to run R on an HPC cluster. For this, you\u2019ll need to read the documentation of your HPC cluster. In this session, we will use the documentation of your HPC cluster to start R. Only do lightweight things! Only do lightweight things! We are still on the login node, which is shared with many other users. This means, that if we do heavy calculations, all the other users are affected. How to do heavy calculations will be shown in this course later. Exercises \u00b6 Prefer this session as video? HPC cluster Login method YouTube video COSMOS Local ThinLinc client YouTube video Dardel Local ThinLinc client YouTube video Kebnekaise Local ThinLinc client YouTube video Pelle SSH YouTube video Rackham Local ThinLinc client YouTube video Tetralith Local ThinLinc client YouTube video HPC cluster name Main breakout room Alvis Room 1 Bianca Room 2 COSMOS Room 3 Dardel Room 4 Kebnekaise Room 5 LUMI Room 6 Pelle Room 7 Rackham Room 8 Tetralith Room 9 Exercise 1: start the R interpreter \u00b6 Find the user documentation of your HPC cluster Answer HPC cluster User documentation for that HPC cluster Alvis Alvis user documentation Bianca Bianca user documentation COSMOS COSMOS user documentation Dardel Dardel user documentation Kebnekaise Kebnekaise user documentation LUMI LUMI user documentation Pelle Pelle user documentation Rackham Rackham user documentation Tetralith Tetralith user documentation Within the documentation of your HPC cluster, search for the documentation about R Answer HPC cluster R user documentation for that HPC cluster Bianca R user documentation COSMOS R user documentation Dardel R user documentation Kebnekaise R user documentation Pelle R user documentation Rackham R user documentation Tetralith R user documentation From a terminal on your HPC cluster, load the module(s) for R, of the recommend version as shown below HPC center R module COSMOS R/4.2.1 Dardel R/4.1.1 Kebnekaise R/4.1.2 Pelle R/4.5.1-gfbf-2024a Rackham R/4.1.1 Tetralith R/4.2.2 Answer HPC cluster How to load the module(s) for R Bianca module load R/4.1.1 COSMOS module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 Dardel module load PDC/23.12 R/4.1.1 Kebnekaise module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 Pelle module load 4.5.1-gfbf-2024a Rackham module load R/4.1.1 Tetralith module load R/4.2.2-hpc1-gcc-11.3.0-bare From the terminal, start the R interpreter Answer HPC cluster How to start the R interpreter Alvis R Bianca R COSMOS R Dardel R Kebnekaise R LUMI R Pelle R Rackham R Tetralith R From the R interpreter, run the R code message(\"Hello\") to verify if this makes the R interpreter show the text \u2018Hello\u2019 From the R interpreter, run the R code quit() to quit the R interpreter and go back to the terminal Exercise 2: run an R script \u00b6 To run an R script, we\u2019ll download one, after which we\u2019ll run it: From the terminal, run wget https://raw.githubusercontent.com/UPPMAX/R-python-julia-HPC/main/exercises/r/hello.R From the terminal, run Rscript hello.R Exercise 3: download and extract the tarball with exercises \u00b6 See how to download and use the tarball how to download and extract the tarball with exercises. Exercise X1: find and use installed R packages \u00b6 From the R interpreter, check which packages are installed, using installed.packages() Answer HPC cluster Answer Alvis installed.packages() Bianca installed.packages() COSMOS installed.packages() Dardel installed.packages() Kebnekaise installed.packages() LUMI installed.packages() Pelle installed.packages() Rackham installed.packages() Tetralith installed.packages() From the R interpreter, load the parallel package, using library(parallel) Answer HPC cluster Answer Alvis library(parallel) Bianca library(parallel) COSMOS library(parallel) Dardel library(parallel) Kebnekaise library(parallel) LUMI library(parallel) Pelle library(parallel) Rackham library(parallel) Tetralith library(parallel) Exercise X2: search for other R versions \u00b6 Use the module system to find which versions of R are provided by your cluster\u2019s module system. Answer HPC cluster How to search for the R modules Alvis module spider R Bianca module spider R COSMOS module spider R Dardel module spider R Kebnekaise module spider R Pelle module spider R Rackham module spider R Tetralith module spider R","title":"Load and run"},{"location":"r/load_run/#load-and-run-r","text":"Learning outcomes Practice using the documentation of your HPC cluster Load an R module Start the R interpreter Run an R script Download and extract the exercise files (optional) Find the different R modules (optional) See the list of installed R packages For teachers Teaching goals are: Learners have practiced using the documentation of their HPC clusters Learners have loaded the module to be able to run R Learners have run the R interpreter Learners have run an R script from the command-line Learners have downloaded and extracted the exercise files (optional) Learners have found the different R modules (optional) Learners have seen the list of installed R packages Prior: What is \u2018HPC\u2019 in \u2018HPC cluster\u2019? What is \u2018cluster\u2019 in \u2018HPC cluster\u2019? What is a software module? What is a script? What are features of using an HPC cluster? What problem would arise if users can install their own software?","title":"Load and run R"},{"location":"r/load_run/#introduction","text":"You want to run R on an HPC cluster. For this, you\u2019ll need to read the documentation of your HPC cluster. In this session, we will use the documentation of your HPC cluster to start R. Only do lightweight things! Only do lightweight things! We are still on the login node, which is shared with many other users. This means, that if we do heavy calculations, all the other users are affected. How to do heavy calculations will be shown in this course later.","title":"Introduction"},{"location":"r/load_run/#exercises","text":"Prefer this session as video? HPC cluster Login method YouTube video COSMOS Local ThinLinc client YouTube video Dardel Local ThinLinc client YouTube video Kebnekaise Local ThinLinc client YouTube video Pelle SSH YouTube video Rackham Local ThinLinc client YouTube video Tetralith Local ThinLinc client YouTube video HPC cluster name Main breakout room Alvis Room 1 Bianca Room 2 COSMOS Room 3 Dardel Room 4 Kebnekaise Room 5 LUMI Room 6 Pelle Room 7 Rackham Room 8 Tetralith Room 9","title":"Exercises"},{"location":"r/load_run/#exercise-1-start-the-r-interpreter","text":"Find the user documentation of your HPC cluster Answer HPC cluster User documentation for that HPC cluster Alvis Alvis user documentation Bianca Bianca user documentation COSMOS COSMOS user documentation Dardel Dardel user documentation Kebnekaise Kebnekaise user documentation LUMI LUMI user documentation Pelle Pelle user documentation Rackham Rackham user documentation Tetralith Tetralith user documentation Within the documentation of your HPC cluster, search for the documentation about R Answer HPC cluster R user documentation for that HPC cluster Bianca R user documentation COSMOS R user documentation Dardel R user documentation Kebnekaise R user documentation Pelle R user documentation Rackham R user documentation Tetralith R user documentation From a terminal on your HPC cluster, load the module(s) for R, of the recommend version as shown below HPC center R module COSMOS R/4.2.1 Dardel R/4.1.1 Kebnekaise R/4.1.2 Pelle R/4.5.1-gfbf-2024a Rackham R/4.1.1 Tetralith R/4.2.2 Answer HPC cluster How to load the module(s) for R Bianca module load R/4.1.1 COSMOS module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 Dardel module load PDC/23.12 R/4.1.1 Kebnekaise module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 Pelle module load 4.5.1-gfbf-2024a Rackham module load R/4.1.1 Tetralith module load R/4.2.2-hpc1-gcc-11.3.0-bare From the terminal, start the R interpreter Answer HPC cluster How to start the R interpreter Alvis R Bianca R COSMOS R Dardel R Kebnekaise R LUMI R Pelle R Rackham R Tetralith R From the R interpreter, run the R code message(\"Hello\") to verify if this makes the R interpreter show the text \u2018Hello\u2019 From the R interpreter, run the R code quit() to quit the R interpreter and go back to the terminal","title":"Exercise 1: start the R interpreter"},{"location":"r/load_run/#exercise-2-run-an-r-script","text":"To run an R script, we\u2019ll download one, after which we\u2019ll run it: From the terminal, run wget https://raw.githubusercontent.com/UPPMAX/R-python-julia-HPC/main/exercises/r/hello.R From the terminal, run Rscript hello.R","title":"Exercise 2: run an R script"},{"location":"r/load_run/#exercise-3-download-and-extract-the-tarball-with-exercises","text":"See how to download and use the tarball how to download and extract the tarball with exercises.","title":"Exercise 3: download and extract the tarball with exercises"},{"location":"r/load_run/#exercise-x1-find-and-use-installed-r-packages","text":"From the R interpreter, check which packages are installed, using installed.packages() Answer HPC cluster Answer Alvis installed.packages() Bianca installed.packages() COSMOS installed.packages() Dardel installed.packages() Kebnekaise installed.packages() LUMI installed.packages() Pelle installed.packages() Rackham installed.packages() Tetralith installed.packages() From the R interpreter, load the parallel package, using library(parallel) Answer HPC cluster Answer Alvis library(parallel) Bianca library(parallel) COSMOS library(parallel) Dardel library(parallel) Kebnekaise library(parallel) LUMI library(parallel) Pelle library(parallel) Rackham library(parallel) Tetralith library(parallel)","title":"Exercise X1: find and use installed R packages"},{"location":"r/load_run/#exercise-x2-search-for-other-r-versions","text":"Use the module system to find which versions of R are provided by your cluster\u2019s module system. Answer HPC cluster How to search for the R modules Alvis module spider R Bianca module spider R COSMOS module spider R Dardel module spider R Kebnekaise module spider R Pelle module spider R Rackham module spider R Tetralith module spider R","title":"Exercise X2: search for other R versions"},{"location":"r/morepackages/","text":"More about R packages \u00b6 This page contains some more advanced information about R packages. R packages: A short Primer \u00b6 What is a package, really? \u00b6 An R package is essentially a contained folder and file structure containing R code (and possibly C/C++ or other code) and other files relevant for the package e.g. documentation(vignettes), licensing and configuration files. Let us look at a very simple example $ git clone https://github.com/MatPiq/R_example.git $ cd R_example $ tree . \u251c\u2500\u2500 DESCRIPTION \u251c\u2500\u2500 NAMESPACE \u251c\u2500\u2500 R \u2502 \u2514\u2500\u2500 hello.R \u251c\u2500\u2500 man \u2502 \u2514\u2500\u2500 hello.Rd \u2514\u2500\u2500 r_example.Rproj Installing tree as non-root on Linux Ubuntu If you are on a Linux Ubuntu system where tree is not installed, and you do not have root permissions, you can do this to install it in your own area 1) Create a directory (in your home folder) to install in: mkdir ~/mytree Change to that directory: cd ~/mytree Now download tree: apt download tree Unpack the files: dpkg-deb -xv ./*deb ./ You can use tree like this now, giving the full path: ~/mytree/usr/bin/tree Note : if you want to be able to use it with the command \u201ctree\u201d you could set an alias in your ~/.bashrc file and then source it: echo 'alias tree=\"$HOME/mytree/usr/bin/tree\"' >> ~/.bashrc source ~/.bashrc Package states \u00b6 An R packages can exist in five possible states Source: \u201csource code\u201d or \u201csource files\u201d. Development form. Bundled: The source code compressed into a single file, usually tar.gz and sometimes referred to as \u201csource tarballs\u201d. Files in .Rbuildignore are excluded. Binary: A compressed and pre-compiled version of a bundle built for a specific architecture. Usually how the package is provided by CRAN. Much faster than having to compile yourself and no need for dev/build tools. Installed: A decompressed binary package located in a package library (more on this later). In-memory: When the installed package has been loaded from the library into memory, using require(pkg) or library(pkg) . Source: https://r-pkgs.org/structure.html and https://nbisweden.github.io/RaukR-2021/rpackages_Sebastian/presentation/rpackages_Sebastian.html Finding out if an R package is installed \u00b6 There are many different ways to check if the package you are after is already installed - chances are it is! The simplest way is probably to simply try loading the package from within R library(package-name) Another option would be to create a dataframe of all the installed packages ip <- as.data.frame ( installed.packages ()[, c ( 1 , 3 : 4 )]) rownames ( ip ) <- NULL ip <- ip [ is.na ( ip $ Priority ), 1 : 2 , drop = FALSE ] print ( ip , row.names = FALSE ) However, this might not be so helpful unless you do additional filtering. Another simple option is to grep the library directory. For example, both when loading R_packages at UPPMAX and R-bundle-Bioconductor at HPC2N the environment variable R_LIBS_SITE will be set to the path of the package library. UPPMAX HPC2N Load R_packages $ ml R_packages/4.1.1 Then grep for some package $ ls -l $R_LIBS_SITE | grep glmnet dr-xr-sr-x 9 douglas sw 4096 Sep 6 2021 EBglmnet dr-xr-sr-x 11 douglas sw 4096 Nov 11 2021 glmnet dr-xr-sr-x 8 douglas sw 4096 Sep 7 2021 glmnetcr dr-xr-sr-x 7 douglas sw 4096 Sep 7 2021 glmnetUtils Load R-bundle-Bioconductor $ ml GCC/11.2.0 OpenMPI/4.1.1 R-bundle-Bioconductor/3.14-R-4.1.2 Check the R_LIBS_SITE environment variable $ echo $R_LIBS_SITE /hpc2n/eb/software/R-bundle-Bioconductor/3.14-foss-2021b-R-4.1.2:/hpc2n/eb/software/arrow-R/6.0.0.2-foss-2021b-R-4.1.2 Then grep for some package in the BioConductor package library $ ls -l /hpc2n/eb/software/R-bundle-Bioconductor/3.14-foss-2021b-R-4.1.2 | grep RNA drwxr-xr-x 9 easybuild easybuild 4096 Dec 30 2021 DeconRNASeq/ drwxr-xr-x 7 easybuild easybuild 4096 Dec 30 2021 RNASeqPower/ Installing your own packages \u00b6 Sometimes you will need R packages that are not already installed. The solution to this is to install your own packages. These packages will usually come from CRAN (https://cran.r-project.org/) - the Comprehensive R Archive Network, or sometimes from other places, like GitHub or R-Forge Here we will look at installing R packages with automatic download and with manual download. It is also possible to install from inside Rstudio. Setup \u00b6 We need to create a place for the own-installed packages to be and to tell R where to find them. The initial setup only needs to be done once, but separate package directories need to be created for each R version used. R reads the $HOME/.Renviron file to setup its environment. It should be created by R on first run, or you can create it with the command: touch $HOME/.Renviron NOTE : In this example we are going to assume you have chosen to place the R packages in a directory under your home directory, but in general it might be good to use the project storage for space reasons. As mentioned, you will need separate ones for each R version. If you have not yet installed any packages to R yourself, the environment file should be empty and you can update it like this: $ echo R_LIBS_USER = \" $HOME /R-packages-%V\" > ~/.Renviron Warning If it is not empty , you can edit $HOME/.Renviron with your favorite editor so that R_LIBS_USER contains the path to your chosen directory for own-installed R packages. It should look something like this when you are done: $ R_LIBS_USER = \"/home/u/user/R-packages-%V\" NOTE Replace /home/u/user with the value of $HOME . Run echo $HOME to see its value. NOTE The %V should be written as-is, it\u2019s substituted at runtime with the active R version. For each version of R you are using, create a directory matching the pattern used in .Renviron to store your packages in. This example is shown for R version 4.1.1: $ mkdir -p $HOME /R-packages-4.1.1 Automatical download and install from CRAN \u00b6 Note You find a list of packages in CRAN (https://cran.r-project.org/) and a list of repos here: https://cran.r-project.org/mirrors.html Please choose a location close to you when picking a repo. From command line From inside R $ R --quiet --no-save --no-restore -e \"install.packages('<r-package>', repos='<repo>')\" install.packages ( '<r-package>' , repos = '<repo>' ) In either case, the dependencies of the package will be downloaded and installed as well. Example \u00b6 In this example, we will install the R package stringr and use the repository http://ftp.acc.umu.se/mirror/CRAN/ Note : You need to load R (and any prerequisites, and possibly R-bundle-Bioconductor if you need packages from that) before installing packages. From command line From inside R $ R --quiet --no-save --no-restore -e \"install.packages('stringr', repos='http://ftp.acc.umu.se/mirror/CRAN/')\" install.packages ( 'stringr' , repos = 'http://ftp.acc.umu.se/mirror/CRAN/' ) There are other ways to install R packages, including from GitHub. Automatic download and install from GitHub \u00b6 If you want to install a package that is not on CRAN, but which do have a GitHub page, then there is an automatic way of installing, but you need to handle prerequsites yourself by installing those first. It can also be that the package is not in as finished a state as those on CRAN, so be careful. To install packages from GitHub directly, from inside R, you first need to install the devtools package. Note that you only need to install this once . This is how you install a package from GitHub, inside R: install.packages ( \"devtools\" ) # ONLY ONCE devtools :: install_github ( \"DeveloperName/package\" ) Example \u00b6 In this example we want to install the package quantstrat . It is not on CRAN, so let\u2019s get it from the GitHub page for the project: https://github.com/braverock/quantstrat We also need to install devtools so we can install packages from GitHub. In addition, quantstrat has some prerequisites, some on CRAN, some on GitHub, so we need to install those as well. install.packages ( \"devtools\" ) # ONLY ONCE install.packages ( \"FinancialInstrument\" ) install.packages ( \"PerformanceAnalytics\" ) devtools :: install_github ( \"braverock/blotter\" ) devtools :: install_github ( \"braverock/quantstrat\" ) Manual download and install \u00b6 If the package is not on CRAN or you want the development version, or you for other reason want to install a package you downloaded, then this is how to install from the command line: $ R CMD INSTALL -l <path-to-R-package>/R-package.tar.gz NOTE that if you install a package this way, you need to handle any dependencies yourself. Note Places to look for R packages CRAN (https://cran.r-project.org/) R-Forge (https://r-forge.r-project.org/) Project\u2019s own GitHub page etc. Keypoints You can check for installed packages from inside R with installed.packages() from BASH shell with the - ml help R/<version> at UPPMAX - ml spider R/<version> at HPC2N Installation of R packages can be done either from within R or from the command line (BASH shell) CRAN is the recommended place to look for R-packages, but many packages can be found on GitHub and if you want the development version of a package you likely need to get it from GitHub or other place outside CRAN. You would then either download and install manually or install with something like devtools, from within R. Install own packages on Bianca \u00b6 If an R package is not not available on Bianca already (like Conda repositories) you may have to use the wharf to install the library/package Typical workflow Install on Rackham Transfer to Wharf Move package to local Bianca R package path Test your installation Demo and exercise from our Bianca course: Installing R packages on Bianca","title":"More about R packages"},{"location":"r/morepackages/#more-about-r-packages","text":"This page contains some more advanced information about R packages.","title":"More about R packages"},{"location":"r/morepackages/#r-packages-a-short-primer","text":"","title":"R packages: A short Primer"},{"location":"r/morepackages/#what-is-a-package-really","text":"An R package is essentially a contained folder and file structure containing R code (and possibly C/C++ or other code) and other files relevant for the package e.g. documentation(vignettes), licensing and configuration files. Let us look at a very simple example $ git clone https://github.com/MatPiq/R_example.git $ cd R_example $ tree . \u251c\u2500\u2500 DESCRIPTION \u251c\u2500\u2500 NAMESPACE \u251c\u2500\u2500 R \u2502 \u2514\u2500\u2500 hello.R \u251c\u2500\u2500 man \u2502 \u2514\u2500\u2500 hello.Rd \u2514\u2500\u2500 r_example.Rproj Installing tree as non-root on Linux Ubuntu If you are on a Linux Ubuntu system where tree is not installed, and you do not have root permissions, you can do this to install it in your own area 1) Create a directory (in your home folder) to install in: mkdir ~/mytree Change to that directory: cd ~/mytree Now download tree: apt download tree Unpack the files: dpkg-deb -xv ./*deb ./ You can use tree like this now, giving the full path: ~/mytree/usr/bin/tree Note : if you want to be able to use it with the command \u201ctree\u201d you could set an alias in your ~/.bashrc file and then source it: echo 'alias tree=\"$HOME/mytree/usr/bin/tree\"' >> ~/.bashrc source ~/.bashrc","title":"What is a package, really?"},{"location":"r/morepackages/#package-states","text":"An R packages can exist in five possible states Source: \u201csource code\u201d or \u201csource files\u201d. Development form. Bundled: The source code compressed into a single file, usually tar.gz and sometimes referred to as \u201csource tarballs\u201d. Files in .Rbuildignore are excluded. Binary: A compressed and pre-compiled version of a bundle built for a specific architecture. Usually how the package is provided by CRAN. Much faster than having to compile yourself and no need for dev/build tools. Installed: A decompressed binary package located in a package library (more on this later). In-memory: When the installed package has been loaded from the library into memory, using require(pkg) or library(pkg) . Source: https://r-pkgs.org/structure.html and https://nbisweden.github.io/RaukR-2021/rpackages_Sebastian/presentation/rpackages_Sebastian.html","title":"Package states"},{"location":"r/morepackages/#finding-out-if-an-r-package-is-installed","text":"There are many different ways to check if the package you are after is already installed - chances are it is! The simplest way is probably to simply try loading the package from within R library(package-name) Another option would be to create a dataframe of all the installed packages ip <- as.data.frame ( installed.packages ()[, c ( 1 , 3 : 4 )]) rownames ( ip ) <- NULL ip <- ip [ is.na ( ip $ Priority ), 1 : 2 , drop = FALSE ] print ( ip , row.names = FALSE ) However, this might not be so helpful unless you do additional filtering. Another simple option is to grep the library directory. For example, both when loading R_packages at UPPMAX and R-bundle-Bioconductor at HPC2N the environment variable R_LIBS_SITE will be set to the path of the package library. UPPMAX HPC2N Load R_packages $ ml R_packages/4.1.1 Then grep for some package $ ls -l $R_LIBS_SITE | grep glmnet dr-xr-sr-x 9 douglas sw 4096 Sep 6 2021 EBglmnet dr-xr-sr-x 11 douglas sw 4096 Nov 11 2021 glmnet dr-xr-sr-x 8 douglas sw 4096 Sep 7 2021 glmnetcr dr-xr-sr-x 7 douglas sw 4096 Sep 7 2021 glmnetUtils Load R-bundle-Bioconductor $ ml GCC/11.2.0 OpenMPI/4.1.1 R-bundle-Bioconductor/3.14-R-4.1.2 Check the R_LIBS_SITE environment variable $ echo $R_LIBS_SITE /hpc2n/eb/software/R-bundle-Bioconductor/3.14-foss-2021b-R-4.1.2:/hpc2n/eb/software/arrow-R/6.0.0.2-foss-2021b-R-4.1.2 Then grep for some package in the BioConductor package library $ ls -l /hpc2n/eb/software/R-bundle-Bioconductor/3.14-foss-2021b-R-4.1.2 | grep RNA drwxr-xr-x 9 easybuild easybuild 4096 Dec 30 2021 DeconRNASeq/ drwxr-xr-x 7 easybuild easybuild 4096 Dec 30 2021 RNASeqPower/","title":"Finding out if an R package is installed"},{"location":"r/morepackages/#installing-your-own-packages","text":"Sometimes you will need R packages that are not already installed. The solution to this is to install your own packages. These packages will usually come from CRAN (https://cran.r-project.org/) - the Comprehensive R Archive Network, or sometimes from other places, like GitHub or R-Forge Here we will look at installing R packages with automatic download and with manual download. It is also possible to install from inside Rstudio.","title":"Installing your own packages"},{"location":"r/morepackages/#setup","text":"We need to create a place for the own-installed packages to be and to tell R where to find them. The initial setup only needs to be done once, but separate package directories need to be created for each R version used. R reads the $HOME/.Renviron file to setup its environment. It should be created by R on first run, or you can create it with the command: touch $HOME/.Renviron NOTE : In this example we are going to assume you have chosen to place the R packages in a directory under your home directory, but in general it might be good to use the project storage for space reasons. As mentioned, you will need separate ones for each R version. If you have not yet installed any packages to R yourself, the environment file should be empty and you can update it like this: $ echo R_LIBS_USER = \" $HOME /R-packages-%V\" > ~/.Renviron Warning If it is not empty , you can edit $HOME/.Renviron with your favorite editor so that R_LIBS_USER contains the path to your chosen directory for own-installed R packages. It should look something like this when you are done: $ R_LIBS_USER = \"/home/u/user/R-packages-%V\" NOTE Replace /home/u/user with the value of $HOME . Run echo $HOME to see its value. NOTE The %V should be written as-is, it\u2019s substituted at runtime with the active R version. For each version of R you are using, create a directory matching the pattern used in .Renviron to store your packages in. This example is shown for R version 4.1.1: $ mkdir -p $HOME /R-packages-4.1.1","title":"Setup"},{"location":"r/morepackages/#automatical-download-and-install-from-cran","text":"Note You find a list of packages in CRAN (https://cran.r-project.org/) and a list of repos here: https://cran.r-project.org/mirrors.html Please choose a location close to you when picking a repo. From command line From inside R $ R --quiet --no-save --no-restore -e \"install.packages('<r-package>', repos='<repo>')\" install.packages ( '<r-package>' , repos = '<repo>' ) In either case, the dependencies of the package will be downloaded and installed as well.","title":"Automatical download and install from CRAN"},{"location":"r/morepackages/#automatic-download-and-install-from-github","text":"If you want to install a package that is not on CRAN, but which do have a GitHub page, then there is an automatic way of installing, but you need to handle prerequsites yourself by installing those first. It can also be that the package is not in as finished a state as those on CRAN, so be careful. To install packages from GitHub directly, from inside R, you first need to install the devtools package. Note that you only need to install this once . This is how you install a package from GitHub, inside R: install.packages ( \"devtools\" ) # ONLY ONCE devtools :: install_github ( \"DeveloperName/package\" )","title":"Automatic download and install from GitHub"},{"location":"r/morepackages/#example_1","text":"In this example we want to install the package quantstrat . It is not on CRAN, so let\u2019s get it from the GitHub page for the project: https://github.com/braverock/quantstrat We also need to install devtools so we can install packages from GitHub. In addition, quantstrat has some prerequisites, some on CRAN, some on GitHub, so we need to install those as well. install.packages ( \"devtools\" ) # ONLY ONCE install.packages ( \"FinancialInstrument\" ) install.packages ( \"PerformanceAnalytics\" ) devtools :: install_github ( \"braverock/blotter\" ) devtools :: install_github ( \"braverock/quantstrat\" )","title":"Example"},{"location":"r/morepackages/#manual-download-and-install","text":"If the package is not on CRAN or you want the development version, or you for other reason want to install a package you downloaded, then this is how to install from the command line: $ R CMD INSTALL -l <path-to-R-package>/R-package.tar.gz NOTE that if you install a package this way, you need to handle any dependencies yourself. Note Places to look for R packages CRAN (https://cran.r-project.org/) R-Forge (https://r-forge.r-project.org/) Project\u2019s own GitHub page etc. Keypoints You can check for installed packages from inside R with installed.packages() from BASH shell with the - ml help R/<version> at UPPMAX - ml spider R/<version> at HPC2N Installation of R packages can be done either from within R or from the command line (BASH shell) CRAN is the recommended place to look for R-packages, but many packages can be found on GitHub and if you want the development version of a package you likely need to get it from GitHub or other place outside CRAN. You would then either download and install manually or install with something like devtools, from within R.","title":"Manual download and install"},{"location":"r/morepackages/#install-own-packages-on-bianca","text":"If an R package is not not available on Bianca already (like Conda repositories) you may have to use the wharf to install the library/package Typical workflow Install on Rackham Transfer to Wharf Move package to local Bianca R package path Test your installation Demo and exercise from our Bianca course: Installing R packages on Bianca","title":"Install own packages on Bianca"},{"location":"r/packages/","text":"Packages \u00b6 R packages R packages is the main way of extending the functionality of R and broadens the use of R to almost infinity! Instead of writing code yourself there may be others that have done the same! Many scientific tools are distributed as R packages . This makes it possible to just run a script from the prompt. You can define files to be analysed and use arguments to define exactly what to do. For more details about packages and in particular developing your own, see: R packages Questions What is an R package? How do I find which packages and versions are available? What to do if I need other packages? Are there differences between HPC2N, UPPMAX, LUNARC, NSC, and PDC? Objectives Check if an R package is installed Load and use R packages Install R packages yourself Package libraries \u00b6 In R, a library is a directory containing installed packages, sort of like a library for books. Unfortunately, in the R world, you will frequently encounter confused usage of the words \u201clibrary\u201d and \u201cpackage\u201d. It\u2019s common for someone to refer to dplyr, for example, as a library when it is actually a package (Wickham & Hadley, 2023). We might want to know where the R interpreter will be searching for packages, i.e. where the libraries are located (could be several). The easiest way to check is probably starting the interpreter and running the libPaths() function. NSC PDC UPPMAX HPC2N LUNARC C3SE Load R , e.g. version 4.2.2 and start the Interpreter $ ml R/4.2.2-hpc1-gcc-11.3.0-bare $ R Then check find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/software/sse2/tetralith_el9/manual/R/4.2.2/g11/hpc1/lib64/R/library\" Load R , e.g. version 4.4.1 and start the Interpreter $ ml PDC/23.12 R/4.4.1-cpeGNU-23.12 $ R Then check find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/cfs/klemming/home/b/bbrydsoe/.R/23.12/4.4.1/library\" [ 2 ] \"/cfs/klemming/pdc/software/dardel/23.12/eb/software/R/4.4.1-cpeGNU-23.12/lib64/R/library\" > Load R , e.g. version 4.4.2 and start the Interpreter $ ml R/4.4.2-gfbf-2024a $ R Then check find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/sw/arch/eb/software/R/4.4.2-gfbf-2024a/lib64/R/library\" Load R , e.g. version R/4.4.1 and start the Interpreter $ ml GCC/13.2.0 R/4.4.1 $ R Then check find the path of the library using the libPaths() function. > [ 1 ] \"/cvmfs/ebsw.hpc2n.umu.se/amd64_ubuntu2004_bdw/software/R/4.4.1-gfbf-2023b/lib/R/library\" Load R , e.g. version 4.2.1 and start the Interpreter $ ml GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 $ R Then check find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/home/bbrydsoe/R-packages-4.2.1\" [ 2 ] \"/sw/easybuild_milan/software/R/4.2.1-foss-2022a/lib64/R/library\" Load R , e.g. version 4.2.1 and start the Interpreter $ ml R/4.2.1-foss-2022a $ R Then check to find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/apps/Arch/software/R/4.2.1-foss-2022a/lib64/R/library\" > Warning: Modules on Dardel If you are using Dardel, then note that there are 13 pre-loaded modules when you login, most of which are related to the machine being a Cray. If you do module purge there, they will all be removed together with the application software modules you wanted to purge. This may cause problems. List of modules that are pre-loaded (March 2025) and which will be removed with module purge : craype-x86-rome libfabric/1.20.1 craype-network-ofi perftools-base/23.12.0 xpmem/2.8.2-1.0_3.9__g84a27a5.shasta cce/17.0.0 craype/2.7.30 cray-dsmml/0.2.2 cray-mpich/8.1.28 cray-libsci/23.12.5 PrgEnv-cray/8.5.0 snic-env/1.0.0 You may have to reload all of these if you do module purge . The easiest solution is this: Immediately after logging in, and before loading any modules (assuming you have not added any to .bashrc do module save preload then, when you have done a module purge to remove some application software modules you have loaded (like R and prerequisites) and want to load a different version perhaps, do module restore preload That will restore the preloaded modules. Preinstalled package libraries \u00b6 UPPMAX, HPC2N, LUNARC, NSC, and PDC all offer larger or smaller amounts of preinstalled packages. HPC2N On HPC2N it depends on the version, but for R/4.4.1 only a smaller number of these (around 110 packages) come with the R module, but additional ones are in the R-bundle-Bioconductor . Most of the packages are in the module R-bundle-CRAN and a few more in R-bundle-CRAN-extra . For older versions of R, most packages come with the R module (around 750 packages). Older versions of R also usually have OpenMPI as prerequisite. Use module spider <module>/<version> to check for prerequisites, as usual. UPPMAX On UPPMAX the module R contains only a smaller number of packages (110), but almost all packages in the CRAN and BioConductor repositories are in the R-bundle-CRAN respectively R-bundle-Bioconductor modules. LUNARC On LUNARC most of the R packages come with the R module. NSC On NSC, a small number of the R packages come with the R module. You will have to install the rest yourself. PDC On PDC about 250 packages come with the R module. C3SE On Alvis at C3SE, around 1340 packages are installed with R/4.2.1 - but much fewer with R/4.3.3. There are many different ways to check if the package you are after is already installed - chances are it is! The simplest way is probably to simply try loading the package from within R (you can also get a list of all packages with installed.packages() but that can be overwhelming): library ( package - name ) To learn about other ways, see the page \u201cMore about R packages\u201d under \u201cExtra reading\u201d in the left menu. Exercise Start R (remember to load a module + prerequisites if you have not already). Check if the packages pillar and caret are installed, as shown above. UPPMAX (Pelle) HPC2N LUNARC NSC PDC C3SE Solution If you want, you can try loading the libraries inside R without loading either the R-bundle-CRAN or R-bundle-Bioconductor modules and see that almost nothing is installed. $ module load R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 # You'll get R/4.4.2-gfbf-2024a on the fly! $ R R version 4 .4.2 ( 2024 -10-31 ) -- \"Pile of Leaves\" Copyright ( C ) 2024 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Loading required package: ggplot2 Loading required package: lattice > Solution $ module load GCC/13.2.0 R/4.4.1 $ module load OpenMPI/4.1.6 R-bundle-CRAN/2024.06 $ R R version 4 .4.1 ( 2024 -06-14 ) -- \"Race for Your Life\" Copyright ( C ) 2024 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Loading required package: ggplot2 Loading required package: lattice > Solution $ module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 $ R R version 4 .2.1 ( 2022 -06-23 ) -- \"Funny-Looking Kid\" Copyright ( C ) 2022 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Loading required package: ggplot2 Loading required package: lattice > Solution $ ml R/4.2.2-hpc1-gcc-11.3.0-bare $ R R version 4 .2.2 ( 2022 -10-31 ) -- \"Innocent and Trusting\" Copyright ( C ) 2022 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) Error in library ( pillar ) : there is no package called \u2018pillar\u2019 > library ( caret ) Error in library ( caret ) : there is no package called \u2018caret\u2019 > Solution $ ml PDC/23.12 R/4.4.1-cpeGNU-23.12 $ R R version 4 .4.1 ( 2024 -06-14 ) -- \"Race for Your Life\" Copyright ( C ) 2024 The R Foundation for Statistical Computing Platform: x86_64-suse-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Error in library ( caret ) : there is no package called \u2018caret\u2019 > Solution $ ml R/4.2.1-foss-2022a $ R Copyright ( C ) 2022 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Loading required package: ggplot2 Loading required package: lattice > Installing your own packages \u00b6 Sometimes you will need R packages that are not already installed. The solution to this is to install your own packages. These packages will usually come from CRAN (https://cran.r-project.org/) - the Comprehensive R Archive Network, or sometimes from other places, like GitHub or R-Forge Here we will look at installing R packages with automatic download and with manual download. It is also possible to install from inside Rstudio. Setup \u00b6 We need to create a place for the own-installed packages to be and to tell R where to find them. The initial setup only needs to be done once, but separate package directories need to be created for each R version used. R reads the $HOME/.Renviron file to setup its environment. It should be created by R on first run, or you can create it with the command: touch $HOME/.Renviron NOTE : In this example we are going to assume you have chosen to place the R packages in a directory under your home directory, but in general it might be good to use the project storage for space reasons. As mentioned, you will need separate ones for each R version. If you have not yet installed any packages to R yourself, the environment file should be empty and you can update it like this: $ echo R_LIBS_USER = \" $HOME /R-packages-%V\" > ~/.Renviron Warning If it is not empty , you can edit $HOME/.Renviron with your favorite editor so that R_LIBS_USER contains the path to your chosen directory for own-installed R packages. It should look something like this when you are done: $ R_LIBS_USER = \"/home/u/user/R-packages-%V\" NOTE Replace /home/u/user with the value of $HOME . Run echo $HOME to see its value. NOTE The %V should be written as-is, it\u2019s substituted at runtime with the active R version. NOTE At Dardel (PDC) the path is /cfs/klemming/home/u/user but you can see it with echo $HOME . For each version of R you are using, create a directory matching the pattern used in .Renviron to store your packages in. This example is shown for R version 4.1.1 (change to version suitable for your centre): $ mkdir -p $HOME /R-packages-4.1.1 Note If you will be installing many R packages yourself, it is a good idea to place them in your project storage instead, as they can take up a lot of space and your home directory is not very large. In that case you would instead do the setup as: $ echo R_LIBS_USER = \"<path-to-your-space-on-proj-storage>/R-packages-%V\" > ~/.Renviron And then create directories for each R version you use there, like this, for R/4.1.1. DO NOT JUST DO IT FOR R/4.1.1! Change to the version you will be using, depending on centre! See under Package libraries . mkdir -p <path-to-your-space-on-proj-storage>/R-packages-4.1.1 Automatical download and install from CRAN \u00b6 Note You find a list of packages in CRAN (https://cran.r-project.org/) and a list of repos here: https://cran.r-project.org/mirrors.html Please choose a location close to you when picking a repo. From command line From inside R $ R --quiet --no-save --no-restore -e \"install.packages('<r-package>', repos='<repo>')\" install.packages ( '<r-package>' , repos = '<repo>' ) In either case, the dependencies of the package will be downloaded and installed as well. Example \u00b6 In this example, we will install the R package nuggets and use the repository http://ftp.acc.umu.se/mirror/CRAN/ Note : You need to load R (and any prerequisites, and possibly R-bundle-Bioconductor if you need packages from that) before installing packages. From command line From inside R $ R --quiet --no-save --no-restore -e \"install.packages('nuggets', repos='http://ftp.acc.umu.se/mirror/CRAN/')\" install.packages ( 'nuggets' , repos = 'http://ftp.acc.umu.se/mirror/CRAN/' ) Solution for installing nuggets 1) UPPMAX (Pelle): module load R/4.4.2-gfbf-2024a UPPMAX (Rackham): module load R_packages/4.1.1 HPC2N: module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 LUNARC: module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 NSC: module load R/4.2.2-hpc1-gcc-11.3.0-bare PDC: module load PDC/23.12 R/4.4.1-cpeGNU-23.12 C3SE: module load R/4.2.1-foss-2022a 2) echo R_LIBS_USER=\"$HOME/R-packages-%V\" > ~/.Renviron OR (option if UPPMAX, HPC2N, NSC, PDC, or C3SE) UPPMAX: echo R_LIBS_USER=\"/proj/r-matlab-julia-pelle/<yourdir>/R-packages-%V\" > ~/.Renviron HPC2N: echo R_LIBS_USER=\"/proj/nobackup/fall-courses/<yourdir>/R-packages-%V\" > ~/.Renviron NSC: echo R_LIBS_USER=\"/proj/courses-fall-2025/<yourdir>/R-packages-%V\" > ~/.Renviron PDC: echo R_LIBS_USER=\"/cfs/klemming/projects/supr/courses-fall-2025/<yourdir>/R-packages-%V\" > ~/.Renviron C3SE: echo R_LIBS_USER=/mimer/NOBACKUP/groups/courses-fall-2025/<yourdir>/R-packages-%V\" > ~/.Renviron 3) Create directory for R packages: LUNARC: mkdir -p $HOME/R-packages-4.2.1 UPPMAX (Pelle): mkdir -p $HOME/R-packages-4.4.2 OR mkdir -p /proj/r-matlab-julia-pelle/<yourdir>/R-packages-4.4.2 UPPMAX (Rackham): mkdir -p $HOME/R-packages-4.1.1 OR mkdir -p /proj/r-matlab-julia-pelle/<yourdir>/R-packages-4.1.1 HPC2N: mkdir -p $HOME/R-packages-4.2.1 OR mkdir -p /proj/nobackup/fall-courses/<yourdir>/R-packages-4.2.1 NSC: mkdir -p $HOME/R-packages-4.2.2 OR mkdir -p /proj/courses-fall-2025/users/<yourdir>/R-packages-4.2.2 PDC: mkdir -p $HOME/R-packages-4.4.1 OR ``mkdir -p /cfs/klemming/projects/supr/courses-fall-2025/ /R-packages-4.4.1``` C3SE: mkdir -p $HOME/R-packages-4.2.1 OR mkdir -p /mimer/NOBACKUP/groups/courses-fall-2025/<yourdir>/R-packages-4.2.1 4) Either of Start R and install: install.packages('nuggets', repos='http://ftp.acc.umu.se/mirror/CRAN/') Install from command line: R --quiet --no-save --no-restore -e \"install.packages('nuggets', repos='http://ftp.acc.umu.se/mirror/CRAN/')\" For other ways to install R packages, including from GitHub or manually, look at the \u201cMore about R packages\u201d from the \u201cExtra reading\u201d section in the bottom left side of the menu. Note Places to look for R packages CRAN (https://cran.r-project.org/) R-Forge (https://r-forge.r-project.org/) Project\u2019s own GitHub page etc. Keypoints You can check for installed packages from inside R with installed.packages() from BASH shell with the ml help R/<version> at UPPMAX ml spider R/<version> at HPC2N ml spider R/<version> at LUNARC ml spider R/version> at NSC ml spider R/version> at PDC Installation of R packages can be done either from within R or from the command line (BASH shell) CRAN is the recommended place to look for R-packages, but many packages can be found on GitHub and if you want the development version of a package you likely need to get it from GitHub or other place outside CRAN. You would then either download and install manually or install with something like devtools, from within R. Extra example, NSC \u00b6 For NSC you need to install doParallel and foreach , and pbdMPI (since Rmpi does not work) yourself to do the exercises in the section about batch. You can either do that now, exactly the same way a above for nuggets or you do it later today, during the batch session. If you have the setup done, and R loaded, then: Start R Then install.packages('foreach', repos='http://ftp.acc.umu.se/mirror/CRAN/') and install.packages('doParallel', repos='http://ftp.acc.umu.se/mirror/CRAN/') and install.packages('pbdMPI', repos='http://ftp.acc.umu.se/mirror/CRAN/') Extra example, PDC \u00b6 For PDC the package Rmpi does not work correctly, so we will instead use the package pbdMPI , which you need to install yourself to do one of the exercises in the advanced section about parallelism. You can either do that now, or you can do it that day, during the parallel session. If you have the setup done, and the R module loaded, then (on the command line, not inside R): First copy the tarball: cp /cfs/klemming/projects/snic/r-matlab-julia-naiss/pbdMPI_0.5-2.tar.gz . while standing in your own directory that you are running R scripts from. Then do: R CMD INSTALL pbdMPI_0.5-2.tar.gz --configure-args=\" --with-mpi-include=/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/include --with-mpi-libpath=/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib --with-mpi-type=MPICH2\" --no-test-load Install own packages on Bianca \u00b6 If an R package is not not available on Bianca already (like Conda repositories) you may have to use the wharf to install the library/package Typical workflow Install on Rackham Transfer to Wharf Move package to local Bianca R package path Test your installation Demo and exercise from our Bianca course: Installing R packages on Bianca <https://uppmax.github.io/bianca_workshops/extra/rpackages/> _ Exercises \u00b6 Install a package with automatic download First do the setup of .Renviron and create the directory for installing R packages (Recommended load R version 4.4.1 on Pelle, 4.1.2 on Kebnekaise, 4.2.1 on LUNARC, 4.2.2 on NSC, and 4.4.1 on PDC) From the command line. Suggestion: anomalize From inside R. Suggestion: BGLR Start R and see if the library can be loaded. These are both on CRAN, and this way any dependencies will be installed as well. Remember to pick a repo that is nearby, to install from: https://cran.r-project.org/mirrors.html Solution for 4.4.2 on Pelle (change ) Solution is very similar for the other centres - just change the R version (for instance to 4.2.1 for LUNARC and 4.4.1 for HPC2N and 4.0.0 for NSC and 4.4.2 for PDC). Setup Command line Inside R Load library $ echo R_LIBS_USER = \" $HOME /R-packages-%V\" > ~/.Renviron $ mkdir -p $HOME /R-packages-4.4.2 Installing package \u201canomalize\u201d. Using the repo http://ftp.acc.umu.se/mirror/CRAN/ $ R --quiet --no-save --no-restore -e \"install.packages('anomalize', repo='http://ftp.acc.umu.se/mirror/CRAN/')\" This assumes you have already loaded the R module. If not, then do so first. Installing package \u201cBGLR\u201d. Using the repo http://ftp.acc.umu.se/mirror/CRAN/ > install.packages ( 'BGLR' , repo = 'http://ftp.acc.umu.se/mirror/CRAN/' ) $ R > library ( \"anomalize\" ) > library ( \"BGLR\" ) \u201cBGLR\u201d outputs some text/advertisment when loaded. You can ignore this.","title":"Packages"},{"location":"r/packages/#packages","text":"R packages R packages is the main way of extending the functionality of R and broadens the use of R to almost infinity! Instead of writing code yourself there may be others that have done the same! Many scientific tools are distributed as R packages . This makes it possible to just run a script from the prompt. You can define files to be analysed and use arguments to define exactly what to do. For more details about packages and in particular developing your own, see: R packages Questions What is an R package? How do I find which packages and versions are available? What to do if I need other packages? Are there differences between HPC2N, UPPMAX, LUNARC, NSC, and PDC? Objectives Check if an R package is installed Load and use R packages Install R packages yourself","title":"Packages"},{"location":"r/packages/#package-libraries","text":"In R, a library is a directory containing installed packages, sort of like a library for books. Unfortunately, in the R world, you will frequently encounter confused usage of the words \u201clibrary\u201d and \u201cpackage\u201d. It\u2019s common for someone to refer to dplyr, for example, as a library when it is actually a package (Wickham & Hadley, 2023). We might want to know where the R interpreter will be searching for packages, i.e. where the libraries are located (could be several). The easiest way to check is probably starting the interpreter and running the libPaths() function. NSC PDC UPPMAX HPC2N LUNARC C3SE Load R , e.g. version 4.2.2 and start the Interpreter $ ml R/4.2.2-hpc1-gcc-11.3.0-bare $ R Then check find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/software/sse2/tetralith_el9/manual/R/4.2.2/g11/hpc1/lib64/R/library\" Load R , e.g. version 4.4.1 and start the Interpreter $ ml PDC/23.12 R/4.4.1-cpeGNU-23.12 $ R Then check find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/cfs/klemming/home/b/bbrydsoe/.R/23.12/4.4.1/library\" [ 2 ] \"/cfs/klemming/pdc/software/dardel/23.12/eb/software/R/4.4.1-cpeGNU-23.12/lib64/R/library\" > Load R , e.g. version 4.4.2 and start the Interpreter $ ml R/4.4.2-gfbf-2024a $ R Then check find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/sw/arch/eb/software/R/4.4.2-gfbf-2024a/lib64/R/library\" Load R , e.g. version R/4.4.1 and start the Interpreter $ ml GCC/13.2.0 R/4.4.1 $ R Then check find the path of the library using the libPaths() function. > [ 1 ] \"/cvmfs/ebsw.hpc2n.umu.se/amd64_ubuntu2004_bdw/software/R/4.4.1-gfbf-2023b/lib/R/library\" Load R , e.g. version 4.2.1 and start the Interpreter $ ml GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 $ R Then check find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/home/bbrydsoe/R-packages-4.2.1\" [ 2 ] \"/sw/easybuild_milan/software/R/4.2.1-foss-2022a/lib64/R/library\" Load R , e.g. version 4.2.1 and start the Interpreter $ ml R/4.2.1-foss-2022a $ R Then check to find the path of the library using the libPaths() function. > .libPaths () [ 1 ] \"/apps/Arch/software/R/4.2.1-foss-2022a/lib64/R/library\" > Warning: Modules on Dardel If you are using Dardel, then note that there are 13 pre-loaded modules when you login, most of which are related to the machine being a Cray. If you do module purge there, they will all be removed together with the application software modules you wanted to purge. This may cause problems. List of modules that are pre-loaded (March 2025) and which will be removed with module purge : craype-x86-rome libfabric/1.20.1 craype-network-ofi perftools-base/23.12.0 xpmem/2.8.2-1.0_3.9__g84a27a5.shasta cce/17.0.0 craype/2.7.30 cray-dsmml/0.2.2 cray-mpich/8.1.28 cray-libsci/23.12.5 PrgEnv-cray/8.5.0 snic-env/1.0.0 You may have to reload all of these if you do module purge . The easiest solution is this: Immediately after logging in, and before loading any modules (assuming you have not added any to .bashrc do module save preload then, when you have done a module purge to remove some application software modules you have loaded (like R and prerequisites) and want to load a different version perhaps, do module restore preload That will restore the preloaded modules.","title":"Package libraries"},{"location":"r/packages/#preinstalled-package-libraries","text":"UPPMAX, HPC2N, LUNARC, NSC, and PDC all offer larger or smaller amounts of preinstalled packages. HPC2N On HPC2N it depends on the version, but for R/4.4.1 only a smaller number of these (around 110 packages) come with the R module, but additional ones are in the R-bundle-Bioconductor . Most of the packages are in the module R-bundle-CRAN and a few more in R-bundle-CRAN-extra . For older versions of R, most packages come with the R module (around 750 packages). Older versions of R also usually have OpenMPI as prerequisite. Use module spider <module>/<version> to check for prerequisites, as usual. UPPMAX On UPPMAX the module R contains only a smaller number of packages (110), but almost all packages in the CRAN and BioConductor repositories are in the R-bundle-CRAN respectively R-bundle-Bioconductor modules. LUNARC On LUNARC most of the R packages come with the R module. NSC On NSC, a small number of the R packages come with the R module. You will have to install the rest yourself. PDC On PDC about 250 packages come with the R module. C3SE On Alvis at C3SE, around 1340 packages are installed with R/4.2.1 - but much fewer with R/4.3.3. There are many different ways to check if the package you are after is already installed - chances are it is! The simplest way is probably to simply try loading the package from within R (you can also get a list of all packages with installed.packages() but that can be overwhelming): library ( package - name ) To learn about other ways, see the page \u201cMore about R packages\u201d under \u201cExtra reading\u201d in the left menu. Exercise Start R (remember to load a module + prerequisites if you have not already). Check if the packages pillar and caret are installed, as shown above. UPPMAX (Pelle) HPC2N LUNARC NSC PDC C3SE Solution If you want, you can try loading the libraries inside R without loading either the R-bundle-CRAN or R-bundle-Bioconductor modules and see that almost nothing is installed. $ module load R-bundle-CRAN/2024.11-foss-2024a R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2 # You'll get R/4.4.2-gfbf-2024a on the fly! $ R R version 4 .4.2 ( 2024 -10-31 ) -- \"Pile of Leaves\" Copyright ( C ) 2024 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Loading required package: ggplot2 Loading required package: lattice > Solution $ module load GCC/13.2.0 R/4.4.1 $ module load OpenMPI/4.1.6 R-bundle-CRAN/2024.06 $ R R version 4 .4.1 ( 2024 -06-14 ) -- \"Race for Your Life\" Copyright ( C ) 2024 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Loading required package: ggplot2 Loading required package: lattice > Solution $ module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 $ R R version 4 .2.1 ( 2022 -06-23 ) -- \"Funny-Looking Kid\" Copyright ( C ) 2022 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Loading required package: ggplot2 Loading required package: lattice > Solution $ ml R/4.2.2-hpc1-gcc-11.3.0-bare $ R R version 4 .2.2 ( 2022 -10-31 ) -- \"Innocent and Trusting\" Copyright ( C ) 2022 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) Error in library ( pillar ) : there is no package called \u2018pillar\u2019 > library ( caret ) Error in library ( caret ) : there is no package called \u2018caret\u2019 > Solution $ ml PDC/23.12 R/4.4.1-cpeGNU-23.12 $ R R version 4 .4.1 ( 2024 -06-14 ) -- \"Race for Your Life\" Copyright ( C ) 2024 The R Foundation for Statistical Computing Platform: x86_64-suse-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Error in library ( caret ) : there is no package called \u2018caret\u2019 > Solution $ ml R/4.2.1-foss-2022a $ R Copyright ( C ) 2022 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > library ( pillar ) > library ( caret ) Loading required package: ggplot2 Loading required package: lattice >","title":"Preinstalled package libraries"},{"location":"r/packages/#installing-your-own-packages","text":"Sometimes you will need R packages that are not already installed. The solution to this is to install your own packages. These packages will usually come from CRAN (https://cran.r-project.org/) - the Comprehensive R Archive Network, or sometimes from other places, like GitHub or R-Forge Here we will look at installing R packages with automatic download and with manual download. It is also possible to install from inside Rstudio.","title":"Installing your own packages"},{"location":"r/packages/#setup","text":"We need to create a place for the own-installed packages to be and to tell R where to find them. The initial setup only needs to be done once, but separate package directories need to be created for each R version used. R reads the $HOME/.Renviron file to setup its environment. It should be created by R on first run, or you can create it with the command: touch $HOME/.Renviron NOTE : In this example we are going to assume you have chosen to place the R packages in a directory under your home directory, but in general it might be good to use the project storage for space reasons. As mentioned, you will need separate ones for each R version. If you have not yet installed any packages to R yourself, the environment file should be empty and you can update it like this: $ echo R_LIBS_USER = \" $HOME /R-packages-%V\" > ~/.Renviron Warning If it is not empty , you can edit $HOME/.Renviron with your favorite editor so that R_LIBS_USER contains the path to your chosen directory for own-installed R packages. It should look something like this when you are done: $ R_LIBS_USER = \"/home/u/user/R-packages-%V\" NOTE Replace /home/u/user with the value of $HOME . Run echo $HOME to see its value. NOTE The %V should be written as-is, it\u2019s substituted at runtime with the active R version. NOTE At Dardel (PDC) the path is /cfs/klemming/home/u/user but you can see it with echo $HOME . For each version of R you are using, create a directory matching the pattern used in .Renviron to store your packages in. This example is shown for R version 4.1.1 (change to version suitable for your centre): $ mkdir -p $HOME /R-packages-4.1.1 Note If you will be installing many R packages yourself, it is a good idea to place them in your project storage instead, as they can take up a lot of space and your home directory is not very large. In that case you would instead do the setup as: $ echo R_LIBS_USER = \"<path-to-your-space-on-proj-storage>/R-packages-%V\" > ~/.Renviron And then create directories for each R version you use there, like this, for R/4.1.1. DO NOT JUST DO IT FOR R/4.1.1! Change to the version you will be using, depending on centre! See under Package libraries . mkdir -p <path-to-your-space-on-proj-storage>/R-packages-4.1.1","title":"Setup"},{"location":"r/packages/#automatical-download-and-install-from-cran","text":"Note You find a list of packages in CRAN (https://cran.r-project.org/) and a list of repos here: https://cran.r-project.org/mirrors.html Please choose a location close to you when picking a repo. From command line From inside R $ R --quiet --no-save --no-restore -e \"install.packages('<r-package>', repos='<repo>')\" install.packages ( '<r-package>' , repos = '<repo>' ) In either case, the dependencies of the package will be downloaded and installed as well.","title":"Automatical download and install from CRAN"},{"location":"r/packages/#install-own-packages-on-bianca","text":"If an R package is not not available on Bianca already (like Conda repositories) you may have to use the wharf to install the library/package Typical workflow Install on Rackham Transfer to Wharf Move package to local Bianca R package path Test your installation Demo and exercise from our Bianca course: Installing R packages on Bianca <https://uppmax.github.io/bianca_workshops/extra/rpackages/> _","title":"Install own packages on Bianca"},{"location":"r/packages/#exercises","text":"Install a package with automatic download First do the setup of .Renviron and create the directory for installing R packages (Recommended load R version 4.4.1 on Pelle, 4.1.2 on Kebnekaise, 4.2.1 on LUNARC, 4.2.2 on NSC, and 4.4.1 on PDC) From the command line. Suggestion: anomalize From inside R. Suggestion: BGLR Start R and see if the library can be loaded. These are both on CRAN, and this way any dependencies will be installed as well. Remember to pick a repo that is nearby, to install from: https://cran.r-project.org/mirrors.html Solution for 4.4.2 on Pelle (change ) Solution is very similar for the other centres - just change the R version (for instance to 4.2.1 for LUNARC and 4.4.1 for HPC2N and 4.0.0 for NSC and 4.4.2 for PDC). Setup Command line Inside R Load library $ echo R_LIBS_USER = \" $HOME /R-packages-%V\" > ~/.Renviron $ mkdir -p $HOME /R-packages-4.4.2 Installing package \u201canomalize\u201d. Using the repo http://ftp.acc.umu.se/mirror/CRAN/ $ R --quiet --no-save --no-restore -e \"install.packages('anomalize', repo='http://ftp.acc.umu.se/mirror/CRAN/')\" This assumes you have already loaded the R module. If not, then do so first. Installing package \u201cBGLR\u201d. Using the repo http://ftp.acc.umu.se/mirror/CRAN/ > install.packages ( 'BGLR' , repo = 'http://ftp.acc.umu.se/mirror/CRAN/' ) $ R > library ( \"anomalize\" ) > library ( \"BGLR\" ) \u201cBGLR\u201d outputs some text/advertisment when loaded. You can ignore this.","title":"Exercises"},{"location":"r/rstudio/","text":"Using RStudio \u00b6 Learning outcomes Practice using the documentation of your HPC cluster Start RStudio For teachers Teaching goals are: Learners have practiced using the documentation of their HPC clusters Learners have started RStudio Prior: What is RStudio? Why is RStudio useful? Why use RStudio? \u00b6 RStudio is the most popular program to write R code in. Software developers commonly use programs that help them write code, with extra features such as a debugger. Such a program is called an IDE ( aj-dee-ee ), short for Integrated Development Environment. Exercises \u00b6 Need a video? Here are videos that do this exercises for the different HPC clusters: HPC Cluster YouTube video Bianca YouTube video COSMOS YouTube video Dardel YouTube video Kebnekaise YouTube video Pelle YouTube video Rackham YouTube video Tetralith YouTube video How difficult will this be? This depends mostly on your HPC cluster: HPC Cluster Difficulty Reason Bianca Easy Step-by-step documentation COSMOS Trivial Step-by-step documentation, trivial procedure Dardel Trivial Step-by-step documentation, trivial procedure Kebnekaise Trivial Step-by-step documentation, trivial procedure Pelle Easy Step-by-step documentation Rackham Easy Step-by-step documentation Tetralith Easy Step-by-step documentation Exercise 1: start RStudio \u00b6 Use the documentation of your HPC cluster for help. Find the user documentation of your HPC cluster Answer HPC cluster User documentation for that HPC cluster Alvis Alvis user documentation Bianca Bianca user documentation COSMOS COSMOS user documentation Dardel Dardel user documentation Kebnekaise Kebnekaise user documentation LUMI LUMI user documentation Pelle Pelle user documentation Rackham Rackham user documentation Tetralith Tetralith user documentation For maximally 5 minutes , search for the procedure on how to start RStudio on your HPC cluster. Take a look at the answer if you cannot find it: sometimes there is no documentation Where is that documentation? HPC Cluster Documentation Alvis UPPMAX documentation Bianca UPPMAX documentation COSMOS UPPMAX documentation Dardel UPPMAX documentation Kebnekaise UPPMAX documentation LUMI UPPMAX documentation Pelle UPPMAX documentation Rackham UPPMAX documentation Tetralith UPPMAX documentation Vera UPPMAX documentation Follow that procedure to start RStudio","title":"RStudio"},{"location":"r/rstudio/#using-rstudio","text":"Learning outcomes Practice using the documentation of your HPC cluster Start RStudio For teachers Teaching goals are: Learners have practiced using the documentation of their HPC clusters Learners have started RStudio Prior: What is RStudio? Why is RStudio useful?","title":"Using RStudio"},{"location":"r/rstudio/#why-use-rstudio","text":"RStudio is the most popular program to write R code in. Software developers commonly use programs that help them write code, with extra features such as a debugger. Such a program is called an IDE ( aj-dee-ee ), short for Integrated Development Environment.","title":"Why use RStudio?"},{"location":"r/rstudio/#exercises","text":"Need a video? Here are videos that do this exercises for the different HPC clusters: HPC Cluster YouTube video Bianca YouTube video COSMOS YouTube video Dardel YouTube video Kebnekaise YouTube video Pelle YouTube video Rackham YouTube video Tetralith YouTube video How difficult will this be? This depends mostly on your HPC cluster: HPC Cluster Difficulty Reason Bianca Easy Step-by-step documentation COSMOS Trivial Step-by-step documentation, trivial procedure Dardel Trivial Step-by-step documentation, trivial procedure Kebnekaise Trivial Step-by-step documentation, trivial procedure Pelle Easy Step-by-step documentation Rackham Easy Step-by-step documentation Tetralith Easy Step-by-step documentation","title":"Exercises"},{"location":"r/rstudio/#exercise-1-start-rstudio","text":"Use the documentation of your HPC cluster for help. Find the user documentation of your HPC cluster Answer HPC cluster User documentation for that HPC cluster Alvis Alvis user documentation Bianca Bianca user documentation COSMOS COSMOS user documentation Dardel Dardel user documentation Kebnekaise Kebnekaise user documentation LUMI LUMI user documentation Pelle Pelle user documentation Rackham Rackham user documentation Tetralith Tetralith user documentation For maximally 5 minutes , search for the procedure on how to start RStudio on your HPC cluster. Take a look at the answer if you cannot find it: sometimes there is no documentation Where is that documentation? HPC Cluster Documentation Alvis UPPMAX documentation Bianca UPPMAX documentation COSMOS UPPMAX documentation Dardel UPPMAX documentation Kebnekaise UPPMAX documentation LUMI UPPMAX documentation Pelle UPPMAX documentation Rackham UPPMAX documentation Tetralith UPPMAX documentation Vera UPPMAX documentation Follow that procedure to start RStudio","title":"Exercise 1: start RStudio"},{"location":"r/schedule/","text":"R schedule \u00b6 Time Topic Teacher(s) 9:00 (optional) First login BB + RB 9:45 Break . 10:00 Introduction RB . Syllabus RB 10:20 Load modules and run RB 11:00 Break . 11:15 Packages BB 12:00 Lunch . 13:00 Batch BB 14:00 Break . 14:15 Simultaneous session: RStudio RB . Simultaneous session: RStudio On-Demand RP 14:45 Summary RB . Evaluation RB 15:00 Done . Teachers: BB : Birgitte Bryds\u00f6, RB : Rich\u00e8l Bilderbeek, RP : Rebecca Pitts","title":"Schedule"},{"location":"r/schedule/#r-schedule","text":"Time Topic Teacher(s) 9:00 (optional) First login BB + RB 9:45 Break . 10:00 Introduction RB . Syllabus RB 10:20 Load modules and run RB 11:00 Break . 11:15 Packages BB 12:00 Lunch . 13:00 Batch BB 14:00 Break . 14:15 Simultaneous session: RStudio RB . Simultaneous session: RStudio On-Demand RP 14:45 Summary RB . Evaluation RB 15:00 Done . Teachers: BB : Birgitte Bryds\u00f6, RB : Rich\u00e8l Bilderbeek, RP : Rebecca Pitts","title":"R schedule"},{"location":"r/intro/","text":"Introduction R \u00b6 Prefer this session as video? See this YouTube video to see this session as a video. Learning outcomes see a first overview of the R programming language see the overview of the course hear about \u2018the tarball with exercises\u2019 For teachers Learning outomes are: Learners have seen an overview of the course Learners have seen an first overview of the R programming language Priors: What is R? What are features of R? What are R packages? What is the R interpreter? What is RStudio? Course learning outcomes use the module system to load R use the module system to load site-installed R packages find out which versions of R and packages are installed run R scripts write a batch script for running R install R packages from CRAN see how to install other R packages yourself start batch jobs run RStudio on the NAISS clusters COSMOS, Dardel, Kebnekaise, Rackham and Tetralith. Course non-goals improve R coding skills use R on other HPC clusters transfer files (tip: see the NAISS \u2018File transfer\u2019 course ) First overview of R \u00b6 R is a programming language for statistical computing and data visualization ( Wikipedia ). graph TD subgraph r[R] r_interpreter[the R interpreter] r_packages[R packages] r_language[the R programming language] r_dev[R software development] rstudio[RStudio] interpreted_language[Interpreted] cran[CRAN] end r_language --> |has| r_dev r_language --> |is| interpreted_language r_language --> |uses| r_packages interpreted_language --> |done by| r_interpreter r_packages --> |maintained by| cran r_dev --> |commonly done in| rstudio The main general R resources are: The R homepage The official R documentation The CRAN homepage R is used in many NAISS centres: An overview of NAISS centres and their R documentation An (incomplete) overview of R courses being taught at NAISS Schedule \u00b6 graph TD subgraph login[HPC login] ssh[0.SSH] remote_desktop_website[5.Remote desktop website] remote_desktop_local_thinlinc_client[5.Remote desktop with local ThinLinc client] end subgraph scheduler[scheduler] running_batch_jobs[3.Running batch jobs] running_interactive_session[5.Running an interactive session] end login --> |allows for| scheduler graph TD subgraph r[R] r_interpreter[1.the R interpreter] r_packages[2.R packages] r_virtual_environments[2.R virtual environments] r_language[1.the R programming language] parallel_and_multithreaded_functions[A.Parallel and multithreaded functions] r_dev[5.R software development] rstudio[5.RStudio] ml[A.Machine learning] interpreted_language[1.Interpreted] cran[1.CRAN] end subgraph modules[modules] r_module[1.R module] r_packages_module[2.R_packages module] rstudio_module[5.RStudio module] end r_language --> |has| r_dev r_language --> |is| interpreted_language r_language --> |uses| r_packages interpreted_language --> |done by| r_interpreter r_packages --> |maintained by| cran r_packages --> |isolated by|r_virtual_environments r_language --> |allows| parallel_and_multithreaded_functions r_language --> |provides for| ml r_dev --> |commonly done in| rstudio r_interpreter --> |loaded by|r_module r_packages --> |loaded by|r_packages_module rstudio --> |loaded by|rstudio_module rstudio_module --> |automatically loads latest| r_packages_module r_packages_module --> |automatically loads corresponding version of| r_module Exercises used in the course \u00b6 The course uses a so-called tarball files with exercises as used in this course. See How to use the course tarball how to get and decompress it. In the \u2018Load and run R\u2019 session, there is the time to do so.","title":"Intro"},{"location":"r/intro/#introduction-r","text":"Prefer this session as video? See this YouTube video to see this session as a video. Learning outcomes see a first overview of the R programming language see the overview of the course hear about \u2018the tarball with exercises\u2019 For teachers Learning outomes are: Learners have seen an overview of the course Learners have seen an first overview of the R programming language Priors: What is R? What are features of R? What are R packages? What is the R interpreter? What is RStudio? Course learning outcomes use the module system to load R use the module system to load site-installed R packages find out which versions of R and packages are installed run R scripts write a batch script for running R install R packages from CRAN see how to install other R packages yourself start batch jobs run RStudio on the NAISS clusters COSMOS, Dardel, Kebnekaise, Rackham and Tetralith. Course non-goals improve R coding skills use R on other HPC clusters transfer files (tip: see the NAISS \u2018File transfer\u2019 course )","title":"Introduction R"},{"location":"r/intro/#first-overview-of-r","text":"R is a programming language for statistical computing and data visualization ( Wikipedia ). graph TD subgraph r[R] r_interpreter[the R interpreter] r_packages[R packages] r_language[the R programming language] r_dev[R software development] rstudio[RStudio] interpreted_language[Interpreted] cran[CRAN] end r_language --> |has| r_dev r_language --> |is| interpreted_language r_language --> |uses| r_packages interpreted_language --> |done by| r_interpreter r_packages --> |maintained by| cran r_dev --> |commonly done in| rstudio The main general R resources are: The R homepage The official R documentation The CRAN homepage R is used in many NAISS centres: An overview of NAISS centres and their R documentation An (incomplete) overview of R courses being taught at NAISS","title":"First overview of R"},{"location":"r/intro/#schedule","text":"graph TD subgraph login[HPC login] ssh[0.SSH] remote_desktop_website[5.Remote desktop website] remote_desktop_local_thinlinc_client[5.Remote desktop with local ThinLinc client] end subgraph scheduler[scheduler] running_batch_jobs[3.Running batch jobs] running_interactive_session[5.Running an interactive session] end login --> |allows for| scheduler graph TD subgraph r[R] r_interpreter[1.the R interpreter] r_packages[2.R packages] r_virtual_environments[2.R virtual environments] r_language[1.the R programming language] parallel_and_multithreaded_functions[A.Parallel and multithreaded functions] r_dev[5.R software development] rstudio[5.RStudio] ml[A.Machine learning] interpreted_language[1.Interpreted] cran[1.CRAN] end subgraph modules[modules] r_module[1.R module] r_packages_module[2.R_packages module] rstudio_module[5.RStudio module] end r_language --> |has| r_dev r_language --> |is| interpreted_language r_language --> |uses| r_packages interpreted_language --> |done by| r_interpreter r_packages --> |maintained by| cran r_packages --> |isolated by|r_virtual_environments r_language --> |allows| parallel_and_multithreaded_functions r_language --> |provides for| ml r_dev --> |commonly done in| rstudio r_interpreter --> |loaded by|r_module r_packages --> |loaded by|r_packages_module rstudio --> |loaded by|rstudio_module rstudio_module --> |automatically loads latest| r_packages_module r_packages_module --> |automatically loads corresponding version of| r_module","title":"Schedule"},{"location":"r/intro/#exercises-used-in-the-course","text":"The course uses a so-called tarball files with exercises as used in this course. See How to use the course tarball how to get and decompress it. In the \u2018Load and run R\u2019 session, there is the time to do so.","title":"Exercises used in the course"},{"location":"r/summary/","text":"Summary \u00b6 You can find the module to be able to run R: module spider R You can load the module to be able to run R: module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 module load R/4.1.1 You can run the R interpreter R You can run the R command to get the list of installed R packages installed.packages () You can run an R script from the command-line Rscript my_script.R You can find out if an R package is already installed installed.packages () library ( my_package ) You can load the pre-installed R packages module load R_packages/4.1.1 You can install an R package from CRAN install.packages ( \"my_package\" , repos = \"my_repo\" ) You can install an R package from GitHub devtools :: install_github ( \"developer_name/package_name\" ) You can manually download and install an R package echo R_LIBS_USER = \"$HOME/R-packages-%V\" > ~/ .Renviron UPPMAX-only: I can manually download and install an R package on Bianca rsync -Pa R ~/ You can submit a job to the scheduler to run an R script with regular code sbatch my_batch_script.sh #!/bin/bash #SBATCH -A my_account #SBATCH -t 00:10:00 module load R Rscript my_script.R You can submit a job to the scheduler to run an R script that uses parallel code #!/bin/bash #SBATCH -A my_account #SBATCH -t 00:10:00 #SBATCH -N 1 #SBATCH -c 4 R -q --slave -f my_parallel_script.R You can submit a job to the scheduler to run an R script that uses a GPU #SBATCH --gres=gpu:x #SBATCH -C v100 #SBATCH -p gpua100 #SBATCH --gres=gpu:1 You can find and load the R machine learning modules module load R/4.1.1 R_packages/4.1.1 module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 R-bundle-Bioconductor/3.14-R-4.1.2 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 R-bundle-Bioconductor/3.15-R-4.2.1 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 CUDA/12.1.1 You can submit a job to the scheduler to run an R script that uses machine learning sbatch my_ml_script.sh You can start an interactive session interactive -A my_project_code salloc -A my_project_code You can verify I am on the login node yes/no srun hostname You can start an interactive session with multiple cores interactive -n 4 -A my_project_code salloc -n 4 -A my_project_code You can verify my interactive session uses multiple cores srun hostname You can start RStudio module load R/4.1.1 RStudio/2023.12.1-402 rstudio Where to go next? \u00b6 For other courses: see the SCoRe overview of courses , which collects all courses from NAISS, SciLifeLab and many more providers For self study: below are free and excellent books Cover Book Audience Hands-on programming Those that have never programmed R for data science Beginners R packages Intermediate Advanced R Advanced","title":"Summary"},{"location":"r/summary/#summary","text":"You can find the module to be able to run R: module spider R You can load the module to be able to run R: module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 module load R/4.1.1 You can run the R interpreter R You can run the R command to get the list of installed R packages installed.packages () You can run an R script from the command-line Rscript my_script.R You can find out if an R package is already installed installed.packages () library ( my_package ) You can load the pre-installed R packages module load R_packages/4.1.1 You can install an R package from CRAN install.packages ( \"my_package\" , repos = \"my_repo\" ) You can install an R package from GitHub devtools :: install_github ( \"developer_name/package_name\" ) You can manually download and install an R package echo R_LIBS_USER = \"$HOME/R-packages-%V\" > ~/ .Renviron UPPMAX-only: I can manually download and install an R package on Bianca rsync -Pa R ~/ You can submit a job to the scheduler to run an R script with regular code sbatch my_batch_script.sh #!/bin/bash #SBATCH -A my_account #SBATCH -t 00:10:00 module load R Rscript my_script.R You can submit a job to the scheduler to run an R script that uses parallel code #!/bin/bash #SBATCH -A my_account #SBATCH -t 00:10:00 #SBATCH -N 1 #SBATCH -c 4 R -q --slave -f my_parallel_script.R You can submit a job to the scheduler to run an R script that uses a GPU #SBATCH --gres=gpu:x #SBATCH -C v100 #SBATCH -p gpua100 #SBATCH --gres=gpu:1 You can find and load the R machine learning modules module load R/4.1.1 R_packages/4.1.1 module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 R-bundle-Bioconductor/3.14-R-4.1.2 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 R-bundle-Bioconductor/3.15-R-4.2.1 module load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1 CUDA/12.1.1 You can submit a job to the scheduler to run an R script that uses machine learning sbatch my_ml_script.sh You can start an interactive session interactive -A my_project_code salloc -A my_project_code You can verify I am on the login node yes/no srun hostname You can start an interactive session with multiple cores interactive -n 4 -A my_project_code salloc -n 4 -A my_project_code You can verify my interactive session uses multiple cores srun hostname You can start RStudio module load R/4.1.1 RStudio/2023.12.1-402 rstudio","title":"Summary"},{"location":"r/summary/#where-to-go-next","text":"For other courses: see the SCoRe overview of courses , which collects all courses from NAISS, SciLifeLab and many more providers For self study: below are free and excellent books Cover Book Audience Hands-on programming Those that have never programmed R for data science Beginners R packages Intermediate Advanced R Advanced","title":"Where to go next?"},{"location":"reflections/","text":"Reflections \u00b6 Iteration Date Language Reflections 2 2023-10-17 Python Reflection 3 2024-03-12 Python Reflection . 2024-03-14 R Reflection 4 2024-10-22 Python Reflection . 2024-10-24 R Reflection 5 2025-03-24 R Reflection 6 2025-10-06 R Reflection . 2025-10-10 Advanced Reflection","title":"Reflections"},{"location":"reflections/#reflections","text":"Iteration Date Language Reflections 2 2023-10-17 Python Reflection 3 2024-03-12 Python Reflection . 2024-03-14 R Reflection 4 2024-10-22 Python Reflection . 2024-10-24 R Reflection 5 2025-03-24 R Reflection 6 2025-10-06 R Reflection . 2025-10-10 Advanced Reflection","title":"Reflections"},{"location":"reflections/20231017_richel/","text":"October 2023 Python Day by Richel \u00b6 Teaching day: 2023-10-17 Topic: Python Written on 2023-10-18 This was the original course schedule: Time Topic Teacher 9.00 Syllabus Birgitte 9.10 Python in general Birgitte 9.20 Load modules and run Birgitte 9.45 Break . 10:00 Packages Richel 10.45 Break . 11.00 Isolated environments Richel 11:45 Break or informal chat Richel 12.00 Lunch . 13.00 Batch Birgitte 13:20 GPU Birgitte 13.30 Kebnekaise: Jupyter Birgitte . Rackham: Jupyter Richel 13.45 Break . 14.00 Parallel and multithreaded functions Pedro . Bianca: Conda, interactive Richel 14.25 Conclusion & Q/A Birgitte 14.45 Evaluation Richel 15.00 END . What went well: [W1] Working as a team [W2] All students (that could log in) completed their exercises within time [W3] We tried out something new: a new format for the evaluation [W4] Collecting feedback during the lessons [W5] Using a shared document What could be improved: [I1] Lengths of the course parts [I2] Remember to schedule a moment for all students to help them log in [I3] Evaluation before Q&A [I4] Install Python package: actually install something in the demo [I5] Me being a good team member [I6] Tell students clearly what to do: observe or type along or do an exercise [I7] No questions in Zoom chat, use shared doc only To discuss: [D1] Allow students that have not logged in yet? [D2] Shouldn\u2019t this material be at the regular UPPMAX website? [W1] Working as a team \u00b6 I was very happy to be part of this team of Birgitte, Bj\u00f6rn and Pedro, as its members seems quite diverse: Birgitte: most knowledge of general things Bj\u00f6rn: the most all-around-nice-guy Pedro: most knowledgeable in parallel computing As Bj\u00f6rn was the only one to have sysadmin rights, he was the one to get learners to login, which he did as much as he could. Both Birgitte and Bj\u00f6rn impressed me by answering complex questions during lectures! Also, already after the lecture, me and Bj\u00f6rn discussed how to improve the course next time. We especially discussed learners that could not log in. Bj\u00f6rn stated there are good reasons to allow these learners to participate in the course, so we chose not to disallow these. Instead, we think it would be a good idea to have a moment scheduled for the learners before the course, for those that could not log in yet. [W2] All students (that could log in) completed their exercises within time \u00b6 Between 10:00-10:45, all students (that could log in) had completed their exercises at 10:15. Between 11:00-11:45, all students (that could log in) had completed their exercises at 11:30. Due to a request by Birgitte, they had to create another venv, which they then all did within a couple of minutes too. At 13:30-13:45, I\u2019d have had 15 mins to do Jupyter notebook on Rackham, which was moved to 13:45-13:55. I felt 15 mins would have been enough, 10 mins was too short. At 14:00-14:25, I and the Rackham learners did an exercises on interactive and conda. I think it was enough time. Note that creating a new conda environment takes 13 minutes, which I used to talk about Python things with the learners. [W3] We tried out something new: a new format for the evaluation \u00b6 I am happy that the team tried out a new format for the evaluation. Already at the end of the day, we discussed already the pros and cons of it and we\u2019ll try out another format directly the day after! [W4] Collecting feedback during the lessons \u00b6 During the exercises, I went through the Zoom rooms to check that all students knew what to do, had questions and/or have remarks. Also the shared document helped to collect feedback during the lesson. [W5] Using a shared document \u00b6 The shared document helped to collect questions and feedback during the lesson. [I1] Lengths of the course parts \u00b6 Topic Was scheduled Actual time used Suggested time packages 45 mins 15 mins 30 mins venv 45 mins 15 mins 30 mins Jupyter 15 mins 15 mins 15 mins conda 25 mins 25 mins 30 mins I/we could not predict the lengths of the topics. For \u2018packages\u2019 and \u2018venv\u2019, the learners had much time left. It is unsure if that is due to (1) me having too simple exercises, (2) my way of teaching compared to the previous time (but it is unclear in which way it differed) (3) the learners were quicker too learn this time. Maybe next time, we\u2019ll find out :-) [I2] Remember to schedule a moment for all students to help them log in \u00b6 There were 4 students out of 28 that could not log in yet. Although they behaved well, it took away time of other students. Bj\u00f6rn stated there are good reasons to allow these learners to participate in the course. However, we forgot to schedule a moment for the learners to log in before the course. [I3] Evaluation before Q&A \u00b6 During the Q&A, which was before the evaluation, a third of the students left, as there was only 1 person talking at the same time. Note that at the end of the day, we discussed this and we would change the order the day after. [I4] Install Python package: actually install something in the demo \u00b6 In the YouTube video, the install failed, due to package version conflicts. Make a new video where something is actually installed [I5] Me being a good team member \u00b6 I was doing so much teaching, that I only had a third of the time needed to prepare. I decided to prepare ruthlessly, ignoring email and other comms for some weeks. I knew this would be annoying to the team, yet I trusted the team would make the right decisions without me. [I6] Tell students clearly what to do: observe or type along or do an exercise \u00b6 There was this feedback given during the lesson: a student asked to another instructor: \u2018Should we watch or type along?\u2019. I agree with her that this always should be clear. I thanked her in a breakout room for this. I know I am imperfect on this too. [I7] No questions in Zoom chat, use shared doc only \u00b6 The course used two communication channels: the Zoom chat and the shared doc. This was needlessly confusing. Next time, only use the shared doc. Feedback \u00b6 Spoken evaluation 2023-10-19 \u00b6 Number of learners: 12 Feedback: 2x The course is well organized! Teachers do not distinguish enough between the different UPPMAX clusters on the first day, however, this was fixed on day 3 :-) Day 3 was slightly better than Day 1 (both were good!), with the advice: on Day 1, take more for ThinLinc and interactive Everything was quite good, especially SLURM, better than some other UPPMAX course. This was mostly due to, today, doing exercises step by step and having enough time to follow ThinLinc was of correct length today, on Day 1 indeed a little bit too short All the instructors should use the same R version, because else you need to load a different R module Google Form evaluations \u00b6 These are the results of the Google Forms evaluation. The responses are sorted alphabetically or in other ways that make sense. Note that the anonymous evaluation overlaps quite well with the anonymous Google Form. Overall, how would you rate today\u2019s training event? \u00b6 5: 1x 6: 3x 7: 5x 8: 1x 9: 2x Hmmm that is quite some variation there\u2026 Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? \u00b6 Cou[r]se materials are really nicely put together Formatted ReadTheDocs Good material I liked how easy it was to ask questions, how friendly the lecturers / organizers were and that there was time for trying it yourself. The course material is really nice to read and helpful. This is nice to hear! I liked the \u2018code along\u2019 parts, when it worked and was an appropriate pace one could actually follow. Unsure if this about mine or Birgittes parts :-| Materials That materials were extensive and provided (both in advance and during the day). The break-out session was good to let me ask the stupid questions to fellows that could help out or make me catch up. The goal of the break-out sessions has been achieved :-) The instructions and code given in the course material on GitHub Very good material, but a bit chaotic Unsure what this is about. I assume it is about my course material that was not embedded in the existing course material. I will fix this in the future. materials structure Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? \u00b6 Create one single entrance point for all the material where links to the rest are to be found. I got lost with five or more different links, folders and the information overflow as it got started. I also missed the big picture of the super computer (no previous experience), how the different systems are related, the main difference of Rackham and hp2cn/kebnekaise and so on. I felt I have too little information to make informed choices. For example, it would be easier if everybody used the same server, and only one\u2026 I agree I should harmonize the course material more. For some tasks, there was too much time allocated, for example for the pip installs at the start. I would suggest checking in with students more frequently about if they are done or not, and then continuing. Or students could be instructed to join the main room again when they are done. This way, if someone takes longer, you could see who is left in the breakout rooms and check in with them. I agree that we mistimed some sessions and we already plan to change that. The Zoom breakout room setup is intentional: we do checkup regularly with the students and we do not call the students back when all are done earlier. I feel this gives rest and clarity to the course. It looked like everything was rushed up and it was not clear what we were supposed to do in the breakout rooms After each exercise, the instructors always checked if the learners understood what to do. Most students usually knew what to do. It will be good if you give a small introduction about different uppmax servers and what are the differences at the beginning of the course. Maybe. Lectures should be given more time (or the contents reduced). Unfortunately it was at times a hard to follow. Perhaps, also, make it a bit more clear what is expected from the exercises. Maybe actually follow the material that is there. It\u2019s hard for us to do exercises and go back and check later when one of the presenters just had his own material. Also, maybe make actual parallel sessions instead of going break rooms. Too much time in break rooms in general, and then other sections got rushed. I agree, I should harmonize the materials. Maybe have two streams, since most Python programmers already know about venv, etc. We cannot. There are plans to teach other courses at two different levels, so that we split absolute beginners from the more advanced students. Much more time spent on code along and instructions, this would improve the course a lot I think! I understand and respect that for you this is very clear and it flows really easily. But for us who is watching over zoom: scrolling up and down fast while screen sharing makes it impossible to follow; as it is over zoom the instructions for the exercise part needs to be much clearer in my view, also do this - BEFORE - entering breakout rooms; additional thought about exercises is what really are the exercises, the ones stated at the end of each lecture part or the text/instructions embedded in the \u2018slides\u2019 in the instruction pages; the \u2018messages\u2019 that can be sent out to the breakout rooms are hard to notice sometimes while working on exercise, perhaps paste this in chat and/or document so one can re-read; please specify clearly how long a task \u2018should\u2019 take approx. (is the whole time in breakout rooms dedicated to the assigned task, or is the time also there for break); specify in start of each part/\u2018lecture\u2019 what the plan is maybe?; \u2018slides\u2019 can be a bit clearer I would say. Thank you, though! I understand that this is no easy feat to lecture and teach HPC over zoom. You didi a great job, but these are things I wish for at least. I like this extensive feedback! To compensate for unclear instructions (to some) for a breakout-room exercise, we teachers always visited the break-out rooms directly after the exercise started. I remember being unclear with the time on purpose (or was that another course?) and I felt it was preferred to be vague over setting a time that would possible be changed, to add to the tranquility. Comment from 2024-01-24: I am happy to see I took this feedback seriously, practiced this and forgot it was based on this! Now I always show the exercise before sending the learners to a breakout room :-) The first should be how to run a simple script. \u201cbatch mode\u201d should come before \u201cvirtual environments\u201d. Also, avoid \u201cpip\u201d at the start, take that later since modules are enough for standard tasks I like this suggestion! a little bit too much information too fast during the zoom call. It\u2019s not possible to fit in all your superb infos from the material in the course itself materials Length of teaching today was \u00b6 Too short: 2x Adequate: 9x Too long: 1x Depth of content was \u00b6 Too superficial: 2x Adequate: 10x The pace of teaching was \u00b6 Too slow: 3x Adequate: 3x Too fast: 6x Teaching aids used (e.g. slides) were well prepared \u00b6 Disagree: 1x No strong feelings: 1x Agree: 5x Agree completely: 5x Hands-on exercises and demonstrations were \u00b6 Too few: 6x Adequate: 6x Hands-on exercises and demonstrations were well prepared \u00b6 Disagree: 1x No strong feelings: 2x Agree: 8x Agree completely: 1x How would you rate the instructors overall teaching performances? \u00b6 Grade|Python (Day 1)(today)|R (Day 3) 5 |1x |1x 6 |3x |2x 7 |3x |1x 8 |4x |3x 9 |1x |2x 10 |0x |2x Avg |7.1 |7.8 Quite a variation here too, like at Day 3. However, Day 3 rated the average instructor performance 10% higher. Do you feel you achieved your desired learning outcome? \u00b6 Yes: 8x Not sure: 4x Did today\u2019s course meet your expectation? \u00b6 Yes: 8x Not sure: 4x Do you have any additional comments? \u00b6 Confusing with different setups for Ume\u00e5 and Uppsala I agree. This is unavoidable. Drop the break out rooms and having people go there for exercises. Or just make a silent room for those that don\u2019t want to discuss and rest can stay in main room. Wastes time. Assuming this was about me putting people in breakout rooms: I will not do this. I think it is very important to put people in different smaller groups, so that people can discuss and a teacher can check. NA No Overall nicely put together course Thanks! generally, the teaching speed was a bit high and then lots of waiting time in-between, but if you get lost, it\u2019s hard to catch up later when all is presented in a serial way. If the entire chain is provided (and I as participant know the path we are following), I could potentially use the breaks to catch up later and jump back in. My parts indeed needed less time. We\u2019ll adept :-) Post-course meeting of 2023-11-08 \u00b6 Suggest different lengths: Topic Was scheduled Actual time used Suggested time packages 45 mins 15 mins 30 mins venv 45 mins 15 mins 30 mins Jupyter 15 mins 15 mins 15 mins conda 25 mins 25 mins 30 mins Have a pre-course login session for those that need it No Zoom chat, only use shared document No anonymous evaluation needed if the Google Form is so good Suggest to follow this students\u2019 suggestion: \u2018The first should be how to run a simple script. \u201cbatch mode\u201d should come before \u201cvirtual environments\u201d. Also, avoid \u201cpip\u201d at the start, take that later since modules are enough for standard tasks\u2019 I will: schedule more time to prepare, due to that have the time to communicate with the team more and merge/harmonize course materials","title":"October 2023 Python Day by Richel"},{"location":"reflections/20231017_richel/#october-2023-python-day-by-richel","text":"Teaching day: 2023-10-17 Topic: Python Written on 2023-10-18 This was the original course schedule: Time Topic Teacher 9.00 Syllabus Birgitte 9.10 Python in general Birgitte 9.20 Load modules and run Birgitte 9.45 Break . 10:00 Packages Richel 10.45 Break . 11.00 Isolated environments Richel 11:45 Break or informal chat Richel 12.00 Lunch . 13.00 Batch Birgitte 13:20 GPU Birgitte 13.30 Kebnekaise: Jupyter Birgitte . Rackham: Jupyter Richel 13.45 Break . 14.00 Parallel and multithreaded functions Pedro . Bianca: Conda, interactive Richel 14.25 Conclusion & Q/A Birgitte 14.45 Evaluation Richel 15.00 END . What went well: [W1] Working as a team [W2] All students (that could log in) completed their exercises within time [W3] We tried out something new: a new format for the evaluation [W4] Collecting feedback during the lessons [W5] Using a shared document What could be improved: [I1] Lengths of the course parts [I2] Remember to schedule a moment for all students to help them log in [I3] Evaluation before Q&A [I4] Install Python package: actually install something in the demo [I5] Me being a good team member [I6] Tell students clearly what to do: observe or type along or do an exercise [I7] No questions in Zoom chat, use shared doc only To discuss: [D1] Allow students that have not logged in yet? [D2] Shouldn\u2019t this material be at the regular UPPMAX website?","title":"October 2023 Python Day by Richel"},{"location":"reflections/20231017_richel/#w1-working-as-a-team","text":"I was very happy to be part of this team of Birgitte, Bj\u00f6rn and Pedro, as its members seems quite diverse: Birgitte: most knowledge of general things Bj\u00f6rn: the most all-around-nice-guy Pedro: most knowledgeable in parallel computing As Bj\u00f6rn was the only one to have sysadmin rights, he was the one to get learners to login, which he did as much as he could. Both Birgitte and Bj\u00f6rn impressed me by answering complex questions during lectures! Also, already after the lecture, me and Bj\u00f6rn discussed how to improve the course next time. We especially discussed learners that could not log in. Bj\u00f6rn stated there are good reasons to allow these learners to participate in the course, so we chose not to disallow these. Instead, we think it would be a good idea to have a moment scheduled for the learners before the course, for those that could not log in yet.","title":"[W1] Working as a team"},{"location":"reflections/20231017_richel/#w2-all-students-that-could-log-in-completed-their-exercises-within-time","text":"Between 10:00-10:45, all students (that could log in) had completed their exercises at 10:15. Between 11:00-11:45, all students (that could log in) had completed their exercises at 11:30. Due to a request by Birgitte, they had to create another venv, which they then all did within a couple of minutes too. At 13:30-13:45, I\u2019d have had 15 mins to do Jupyter notebook on Rackham, which was moved to 13:45-13:55. I felt 15 mins would have been enough, 10 mins was too short. At 14:00-14:25, I and the Rackham learners did an exercises on interactive and conda. I think it was enough time. Note that creating a new conda environment takes 13 minutes, which I used to talk about Python things with the learners.","title":"[W2] All students (that could log in) completed their exercises within time"},{"location":"reflections/20231017_richel/#w3-we-tried-out-something-new-a-new-format-for-the-evaluation","text":"I am happy that the team tried out a new format for the evaluation. Already at the end of the day, we discussed already the pros and cons of it and we\u2019ll try out another format directly the day after!","title":"[W3] We tried out something new: a new format for the evaluation"},{"location":"reflections/20231017_richel/#w4-collecting-feedback-during-the-lessons","text":"During the exercises, I went through the Zoom rooms to check that all students knew what to do, had questions and/or have remarks. Also the shared document helped to collect feedback during the lesson.","title":"[W4] Collecting feedback during the lessons"},{"location":"reflections/20231017_richel/#w5-using-a-shared-document","text":"The shared document helped to collect questions and feedback during the lesson.","title":"[W5] Using a shared document"},{"location":"reflections/20231017_richel/#i1-lengths-of-the-course-parts","text":"Topic Was scheduled Actual time used Suggested time packages 45 mins 15 mins 30 mins venv 45 mins 15 mins 30 mins Jupyter 15 mins 15 mins 15 mins conda 25 mins 25 mins 30 mins I/we could not predict the lengths of the topics. For \u2018packages\u2019 and \u2018venv\u2019, the learners had much time left. It is unsure if that is due to (1) me having too simple exercises, (2) my way of teaching compared to the previous time (but it is unclear in which way it differed) (3) the learners were quicker too learn this time. Maybe next time, we\u2019ll find out :-)","title":"[I1] Lengths of the course parts"},{"location":"reflections/20231017_richel/#i2-remember-to-schedule-a-moment-for-all-students-to-help-them-log-in","text":"There were 4 students out of 28 that could not log in yet. Although they behaved well, it took away time of other students. Bj\u00f6rn stated there are good reasons to allow these learners to participate in the course. However, we forgot to schedule a moment for the learners to log in before the course.","title":"[I2] Remember to schedule a moment for all students to help them log in"},{"location":"reflections/20231017_richel/#i3-evaluation-before-qa","text":"During the Q&A, which was before the evaluation, a third of the students left, as there was only 1 person talking at the same time. Note that at the end of the day, we discussed this and we would change the order the day after.","title":"[I3] Evaluation before Q&amp;A"},{"location":"reflections/20231017_richel/#i4-install-python-package-actually-install-something-in-the-demo","text":"In the YouTube video, the install failed, due to package version conflicts. Make a new video where something is actually installed","title":"[I4] Install Python package: actually install something in the demo"},{"location":"reflections/20231017_richel/#i5-me-being-a-good-team-member","text":"I was doing so much teaching, that I only had a third of the time needed to prepare. I decided to prepare ruthlessly, ignoring email and other comms for some weeks. I knew this would be annoying to the team, yet I trusted the team would make the right decisions without me.","title":"[I5] Me being a good team member"},{"location":"reflections/20231017_richel/#i6-tell-students-clearly-what-to-do-observe-or-type-along-or-do-an-exercise","text":"There was this feedback given during the lesson: a student asked to another instructor: \u2018Should we watch or type along?\u2019. I agree with her that this always should be clear. I thanked her in a breakout room for this. I know I am imperfect on this too.","title":"[I6] Tell students clearly what to do: observe or type along or do an exercise"},{"location":"reflections/20231017_richel/#i7-no-questions-in-zoom-chat-use-shared-doc-only","text":"The course used two communication channels: the Zoom chat and the shared doc. This was needlessly confusing. Next time, only use the shared doc.","title":"[I7] No questions in Zoom chat, use shared doc only"},{"location":"reflections/20231017_richel/#feedback","text":"","title":"Feedback"},{"location":"reflections/20231017_richel/#spoken-evaluation-2023-10-19","text":"Number of learners: 12 Feedback: 2x The course is well organized! Teachers do not distinguish enough between the different UPPMAX clusters on the first day, however, this was fixed on day 3 :-) Day 3 was slightly better than Day 1 (both were good!), with the advice: on Day 1, take more for ThinLinc and interactive Everything was quite good, especially SLURM, better than some other UPPMAX course. This was mostly due to, today, doing exercises step by step and having enough time to follow ThinLinc was of correct length today, on Day 1 indeed a little bit too short All the instructors should use the same R version, because else you need to load a different R module","title":"Spoken evaluation 2023-10-19"},{"location":"reflections/20231017_richel/#google-form-evaluations","text":"These are the results of the Google Forms evaluation. The responses are sorted alphabetically or in other ways that make sense. Note that the anonymous evaluation overlaps quite well with the anonymous Google Form.","title":"Google Form evaluations"},{"location":"reflections/20231017_richel/#overall-how-would-you-rate-todays-training-event","text":"5: 1x 6: 3x 7: 5x 8: 1x 9: 2x Hmmm that is quite some variation there\u2026","title":"Overall, how would you rate today&rsquo;s training event?"},{"location":"reflections/20231017_richel/#todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-what-did-you-like-best","text":"Cou[r]se materials are really nicely put together Formatted ReadTheDocs Good material I liked how easy it was to ask questions, how friendly the lecturers / organizers were and that there was time for trying it yourself. The course material is really nice to read and helpful. This is nice to hear! I liked the \u2018code along\u2019 parts, when it worked and was an appropriate pace one could actually follow. Unsure if this about mine or Birgittes parts :-| Materials That materials were extensive and provided (both in advance and during the day). The break-out session was good to let me ask the stupid questions to fellows that could help out or make me catch up. The goal of the break-out sessions has been achieved :-) The instructions and code given in the course material on GitHub Very good material, but a bit chaotic Unsure what this is about. I assume it is about my course material that was not embedded in the existing course material. I will fix this in the future. materials structure","title":"Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best?"},{"location":"reflections/20231017_richel/#todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-where-should-we-improve","text":"Create one single entrance point for all the material where links to the rest are to be found. I got lost with five or more different links, folders and the information overflow as it got started. I also missed the big picture of the super computer (no previous experience), how the different systems are related, the main difference of Rackham and hp2cn/kebnekaise and so on. I felt I have too little information to make informed choices. For example, it would be easier if everybody used the same server, and only one\u2026 I agree I should harmonize the course material more. For some tasks, there was too much time allocated, for example for the pip installs at the start. I would suggest checking in with students more frequently about if they are done or not, and then continuing. Or students could be instructed to join the main room again when they are done. This way, if someone takes longer, you could see who is left in the breakout rooms and check in with them. I agree that we mistimed some sessions and we already plan to change that. The Zoom breakout room setup is intentional: we do checkup regularly with the students and we do not call the students back when all are done earlier. I feel this gives rest and clarity to the course. It looked like everything was rushed up and it was not clear what we were supposed to do in the breakout rooms After each exercise, the instructors always checked if the learners understood what to do. Most students usually knew what to do. It will be good if you give a small introduction about different uppmax servers and what are the differences at the beginning of the course. Maybe. Lectures should be given more time (or the contents reduced). Unfortunately it was at times a hard to follow. Perhaps, also, make it a bit more clear what is expected from the exercises. Maybe actually follow the material that is there. It\u2019s hard for us to do exercises and go back and check later when one of the presenters just had his own material. Also, maybe make actual parallel sessions instead of going break rooms. Too much time in break rooms in general, and then other sections got rushed. I agree, I should harmonize the materials. Maybe have two streams, since most Python programmers already know about venv, etc. We cannot. There are plans to teach other courses at two different levels, so that we split absolute beginners from the more advanced students. Much more time spent on code along and instructions, this would improve the course a lot I think! I understand and respect that for you this is very clear and it flows really easily. But for us who is watching over zoom: scrolling up and down fast while screen sharing makes it impossible to follow; as it is over zoom the instructions for the exercise part needs to be much clearer in my view, also do this - BEFORE - entering breakout rooms; additional thought about exercises is what really are the exercises, the ones stated at the end of each lecture part or the text/instructions embedded in the \u2018slides\u2019 in the instruction pages; the \u2018messages\u2019 that can be sent out to the breakout rooms are hard to notice sometimes while working on exercise, perhaps paste this in chat and/or document so one can re-read; please specify clearly how long a task \u2018should\u2019 take approx. (is the whole time in breakout rooms dedicated to the assigned task, or is the time also there for break); specify in start of each part/\u2018lecture\u2019 what the plan is maybe?; \u2018slides\u2019 can be a bit clearer I would say. Thank you, though! I understand that this is no easy feat to lecture and teach HPC over zoom. You didi a great job, but these are things I wish for at least. I like this extensive feedback! To compensate for unclear instructions (to some) for a breakout-room exercise, we teachers always visited the break-out rooms directly after the exercise started. I remember being unclear with the time on purpose (or was that another course?) and I felt it was preferred to be vague over setting a time that would possible be changed, to add to the tranquility. Comment from 2024-01-24: I am happy to see I took this feedback seriously, practiced this and forgot it was based on this! Now I always show the exercise before sending the learners to a breakout room :-) The first should be how to run a simple script. \u201cbatch mode\u201d should come before \u201cvirtual environments\u201d. Also, avoid \u201cpip\u201d at the start, take that later since modules are enough for standard tasks I like this suggestion! a little bit too much information too fast during the zoom call. It\u2019s not possible to fit in all your superb infos from the material in the course itself materials","title":"Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve?"},{"location":"reflections/20231017_richel/#length-of-teaching-today-was","text":"Too short: 2x Adequate: 9x Too long: 1x","title":"Length of teaching today was"},{"location":"reflections/20231017_richel/#depth-of-content-was","text":"Too superficial: 2x Adequate: 10x","title":"Depth of content was"},{"location":"reflections/20231017_richel/#the-pace-of-teaching-was","text":"Too slow: 3x Adequate: 3x Too fast: 6x","title":"The pace of teaching was"},{"location":"reflections/20231017_richel/#teaching-aids-used-eg-slides-were-well-prepared","text":"Disagree: 1x No strong feelings: 1x Agree: 5x Agree completely: 5x","title":"Teaching aids used (e.g. slides) were well prepared"},{"location":"reflections/20231017_richel/#hands-on-exercises-and-demonstrations-were","text":"Too few: 6x Adequate: 6x","title":"Hands-on exercises and demonstrations were"},{"location":"reflections/20231017_richel/#hands-on-exercises-and-demonstrations-were-well-prepared","text":"Disagree: 1x No strong feelings: 2x Agree: 8x Agree completely: 1x","title":"Hands-on exercises and demonstrations were well prepared"},{"location":"reflections/20231017_richel/#how-would-you-rate-the-instructors-overall-teaching-performances","text":"Grade|Python (Day 1)(today)|R (Day 3) 5 |1x |1x 6 |3x |2x 7 |3x |1x 8 |4x |3x 9 |1x |2x 10 |0x |2x Avg |7.1 |7.8 Quite a variation here too, like at Day 3. However, Day 3 rated the average instructor performance 10% higher.","title":"How would you rate the instructors overall teaching performances?"},{"location":"reflections/20231017_richel/#do-you-feel-you-achieved-your-desired-learning-outcome","text":"Yes: 8x Not sure: 4x","title":"Do you feel you achieved your desired learning outcome?"},{"location":"reflections/20231017_richel/#did-todays-course-meet-your-expectation","text":"Yes: 8x Not sure: 4x","title":"Did today&rsquo;s course meet your expectation?"},{"location":"reflections/20231017_richel/#do-you-have-any-additional-comments","text":"Confusing with different setups for Ume\u00e5 and Uppsala I agree. This is unavoidable. Drop the break out rooms and having people go there for exercises. Or just make a silent room for those that don\u2019t want to discuss and rest can stay in main room. Wastes time. Assuming this was about me putting people in breakout rooms: I will not do this. I think it is very important to put people in different smaller groups, so that people can discuss and a teacher can check. NA No Overall nicely put together course Thanks! generally, the teaching speed was a bit high and then lots of waiting time in-between, but if you get lost, it\u2019s hard to catch up later when all is presented in a serial way. If the entire chain is provided (and I as participant know the path we are following), I could potentially use the breaks to catch up later and jump back in. My parts indeed needed less time. We\u2019ll adept :-)","title":"Do you have any additional comments?"},{"location":"reflections/20231017_richel/#post-course-meeting-of-2023-11-08","text":"Suggest different lengths: Topic Was scheduled Actual time used Suggested time packages 45 mins 15 mins 30 mins venv 45 mins 15 mins 30 mins Jupyter 15 mins 15 mins 15 mins conda 25 mins 25 mins 30 mins Have a pre-course login session for those that need it No Zoom chat, only use shared document No anonymous evaluation needed if the Google Form is so good Suggest to follow this students\u2019 suggestion: \u2018The first should be how to run a simple script. \u201cbatch mode\u201d should come before \u201cvirtual environments\u201d. Also, avoid \u201cpip\u201d at the start, take that later since modules are enough for standard tasks\u2019 I will: schedule more time to prepare, due to that have the time to communicate with the team more and merge/harmonize course materials","title":"Post-course meeting of 2023-11-08"},{"location":"reflections/20240312_richel/","text":"Reflection \u00b6 Teaching day: 2023-03-12 Topic: Python Written on 2023-03-12 by Richel Schedule \u00b6 Time Topic Teacher 9.00 Syllabus Birgitte 9.10 Python in general Birgitte 9.20 Load modules and run Birgitte 9.45 Break . 10:00 Packages (45\u2013>30) Richel 10.45 Break . 11.00 Isolated environments Richel 12.00 Lunch . 13.00 Batch Birgitte 13:20 GPU Birgitte 13.30 Kebnekaise: Jupyter Birgitte . Rackham: Interactive session and Jupyter Richel 13.45 Break . 14.00 Parallel and multithreaded functions Pedro 14.25 Conclusion & Q/A Birgitte 14.45 Evaluation Richel Reflection 1 \u00b6 I prepared well for the lessons, as I remembered from the previous course iteration that my material was not blended in with the others. This iteration, my material was 100% part of the course website and I removed the presentation slides from the previous iteration. Although I did invest 10 + 2 + 16 + 2 = 30 hours, with little sleep, the day of teaching went fine. The most obvious problem during teaching was that Zoom did not allow me to speak or show video, in the main room, when breakout rooms are active. I think we dealt with this technical problem as good as we reasonably could. However, due to it, I cannot properly evaluate my own lessons. I was annoyed with being this blind as a teacher: how can I know if the learners understood, if I cannot see their exercises? Also, I did not do a half-way-during-the-exercise feedback, nor a nearly-the-end-of-the-hour feedback. And I was unable to see if the exercise lengths matched the time allocated. There were some mistakes: [Fixed] The exercises on venv failed due to the install of a package in the session before. Next time, ruthlessly delete the /home/richel/.local/lib/python3.11 folder. I\u2019ve added this to the course material I could not demo going into the remote desktop website of Rackham. Repeat this next time! I feel that is as much as I have to reflect on. I think it went quite smoothly when correcting for the technical error. Discussion on other sections \u00b6 Pre-requirements \u00b6 I feel the pre-requirements page, [URL] , is too extensive. I feel it should link to regular documentation and mostly show how to determine you fulfilled all pre-requirements. Interactive \u00b6 The session on starting an interactive session feels too fancy twice: create an interactive session with 1 node with more nodes <\u2014 feels beyond the teaching goals run 2 Python scripts, 1 of which is unsuitable for an interactive session with more nodes. I think, for 15 minutes, one can only do 1 node and no Python script, to achieve the teaching goals. Impressed by Birgitte \u00b6 I think it was impressive that Birgitte logs in into both clusters at the start. I want that too! Why ssh -Y \u00b6 Unrelated to the course, Birgitte does so. Loading Python \u00b6 If the session is about loading Python, maybe seeing module dependencies can removed. Also, don\u2019t care about python3? Also, don\u2019t care about IPython? Suggest to Arvid \u00b6 The Bianca portal is great. Could you do the same for Rackham? Not every user understands one needs to use 2FA now, and how More time for sbatch \u00b6 There was no time for an exercise. I would have enjoy to be sure that the learners have been able to submit a job and see the results. More time for GPU \u00b6 There was no time for an exercise. More time for UPPMAX interactive and Jupyter \u00b6 There was no time for an exercise. Parallel programming \u00b6 I feel that making a script suitable (with **FIX** in it) for a parallel run is at the \u2018Synthesis\u2019 level of Blooms taxonomy. I feel that some levels lower, e.g. \u2018Apply\u2019 with a step-by-step guide would be more suitable. Evaluations \u00b6 Overall, how would you rate today\u2019s training event? \u00b6 5: 2x 6: 2x 7: 4x 8: 11x 9: 3x 10: 3x Average: ((2 * 5) + (2 * 6) + (4 * 7) + (11 * 8) + (3 * 9) + (3 * 10)) / 25 = 7.8 Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? \u00b6 Here I aggregate the learners feedback, where I do put some under multiple topics. As all sentences are unique, such duplicates can be spotted. Structure: The structure is good. exercises and structure Good instructions, positive supportive and friendly environment, nice course structure, basic exercises. The format and structure was good, hands on exercises was also good. materials, exercise and structure Materials and structure were good Exercises: exercises exercises and structure Good instructions, positive supportive and friendly environment, nice course structure, basic exercises. The format and structure was good, hands on exercises was also good. materials, exercise and structure Clear exercises and instructions, availability of deepening material I like that the material was very well prepared and with this guidance I understood concepts and managed to do task that I could not do by myself Materials: good material on the web that was easy to follow The wiki of the course is really good. I will continue using it as a documentation after the course. As, there is no wiki , I will count this under \u2018materials\u2019. materials The topic covered materials, exercise and structure Materials and structure were good Clear exercises and instructions, availability of deepening material [URL to prerequisites] <- this webpage was great! materials Very good material, mostly in one location material gathered in one space info mail was +1, same for material Lectures: The lectures were very clear and the teachers were very good Teachers The lectures were very clear and the teachers were very good Good instructions, positive supportive and friendly environment, nice course structure, basic exercises. Sections: The part before lunch was very clear and easy to understand despite the technical issues I taught before lunch, so maybe this was my part? Misc: A good overview. I think I\u2019ll find the Jupyter lab and env extra useful in the future! info mail was +1, same for material Reflection 2 \u00b6 The learners again like the structure of the course and the course material. Maybe they do like the my teaching (\u2018The part before lunch\u2019) and my exercises even better (\u2018basic exercises\u2019, \u2018hands on exercises\u2019). I am happy one learner mentioned \u2018positive supportive and friendly environment\u2019! Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? \u00b6 Here I aggregate the learners feedback, where I do put some under multiple topics. As all sentences are unique, such duplicates can be spotted. Compliments \u00b6 None Very good, they were motivated and clear. Time organization and afternoon was fast or too short \u00b6 a bit more time dedicated to Jupyter The time dedicated for the interactive session/jupyter was not sufficient to try ourselves. The last part on Parallel and multi-threaded functions was a bit fast due to time which happens of course but it would be good to learn more on that. materials The after lunch section was really difficult for me to follow, I feel like the lecturers didn\u2019t stop long enough on each topic to explain it properly and I only got a partial understanding Maybe a better distribution of time between exercises would benefit. Spend less time on the beginning and give more time to run examples on the interactive and parallel parts. Because I think this is the part were we need more help. the 14:00 to 15:00 session was a bit hard to keep up with. Time per exercise can be optimised. More time for python jupyter and GPU exercises will be good for us. Time organization: morning pace was slow compared to the afternoon one. Maybe more time of coding together with the lecturer, especially for the second part of the program (more complex concepts of batch work and interactive work) The first part was a good pace for me, but the second part was more complicated and I did not have time to follow and do the exercises. Maybe the second part could be done in another session, because it is the most important part to take use the resources and take advantage of that. However, I got many tools to start my own projects. \u2018The first part\u2019 was partially my part :-) more time for exercises in afternoon, more time for the batch section, maybe don\u2019t ask random people to answer stuff I don\u2019t like that (stress - introvert) Teaching \u00b6 Teaching was quite fast. Sometime hard to follow. I personally thought Birgitte was too fast and I could not follow. I liked Richel the most. Hey, that is a fun compliment! Thanks! more time for exercises in afternoon, more time for the batch section, maybe don\u2019t ask random people to answer stuff I don\u2019t like that (stress - introvert) I will keep asking random people, because a teacher needs to monitor the progress of all learners. However, at that day, I did not think it was necessary to share why I ask random learners and that it is recommended by the literature. I would correct this two days later, let\u2019s see if that made it into the feedback. more and slower code-alongs The pace was relatively slow, which was great. as then also those how have very limited knowledge can also keep up the pace. However, in the afternoon it clearly went more faster. Maybe speed up in the morning a bit to have more time practicing actually submitting the jobs? As that was my main motivation for this, I basically skipped the very last lecture about parallelization and instead coded to make submitting batch jobs work. Materials \u00b6 There were some errors/not updated in the exercises in the material. Also can more of the material be on the course pages instead of pointing to documentation at the centers? It makes it clearer when we don\u2019t have to go difference places I understand this feedback, yet I do think it is a bad idea: a learner should get accustomed to the documentation of his/her HPC center. Other \u00b6 If prerequisite is well informed, such as little knowledge on Linus? it would have been better. Reflection 3 \u00b6 Most of the negative feedback was about the high pace in general, yet mostly in the afternoon. I feel I was not the reason for that feedback. I did get negative feedback on: course material in one place ask random people Although I agree with the feedback, I will not change this, as there are good reasons to do so. Length of teaching today was \u00b6 Too short: 8x Adequate: 16x Too long: 1x I think learner-centered teaching would solve this, i.e. by only teaching that amount of concepts that are confirmed to be understood by an exercise. Depth of content was \u00b6 Too superficial: 5x Adequate: 20x The pace of teaching was \u00b6 Too slow: i Adequate: iiiiiiiiiiiiiii Too fast: iiiiiiii I think learner-centered teaching would solve this, as -by definition- one follows the pace of the learners. Teaching aids used (e.g. slides) were well prepared \u00b6 Agree completely: 7x Agree: 15x No strong feelings: 3x Hands-on exercises and demonstrations were \u00b6 Adequate: 17x Too few: 7x I think this is a nonsense questions: either we do exercises (i.e. active teaching) or we do demonstrations (i.e. passive teaching). Only from this question, one would have idea what was too few off: only the written texts earlier indicates that there should be more exercises. Hands-on exercises and demonstrations were well prepared \u00b6 Agree completely: 7x Agree: 13x No strong feelings: 4x Disagree: 1x How would you rate the instructors overall teaching performances? \u00b6 This is a useless question, as there were 4 teachers, but here goes: 10: 4x 9: 5x 8: 4x 7: 8x 6: 3x Average: ((4 * 10) + (5 * 9) + (4 * 8) + (8 * 7) + (3 * 6)) / 24 = 8.0 Do you feel you achieved your desired learning outcome? \u00b6 Yes: 17x Not sure: 8x Did today\u2019s course meet your expectation? \u00b6 Yes: 18x Not sure: 7x Do you have any additional comments? \u00b6 Thanks \u00b6 Thanks for the course, it was very useful and I feel encouraged to start my project. Overall a great introductory course to get grasp of the basics in a positive and supportive environment with knowledgeable instructors. Thanks for your lecture and your time. Misc \u00b6 Can we have something like BYOC and you can show us how to use it to asses time and core used using library multiprocess, please? A bit too uneven speed with too low in the morning and too high in the afternoon or at least too little time in the afternoon to go through everything Agreed, I think learning-centered teaching would solve this. Some students had problems logging in to the needed systems. I also just noted that there was this log-in session in Monday when you mentioned it on tuesday. Maybe advertise that in the very beginning of the information email so that it does not go unnoticed. I had no problems logging etc, but those who had would have benefited participating on Monday Agreed, we could be more ruthless in letting our learners in, e.g. by putting the link to the Zoom on Rackham and Kebnekaise. Diversity of teachers \u00b6 I can not evaluate the performance of each part in a single evaluation form, as I felt it was very different depending on each teacher Agreed. Zoom room \u00b6 More time to exercises. and the zoom problems made it confusing. I know it\u2019s not your fault. Maybe don\u2019t use breakout rooms so much, especially when you know there are problems? I have never been in a course where it works well to go back and forth between breakout rooms. Just use them as backup if anyone need to get private help. I agree that the technical problems are annoying. The Zoom room use, i.e. of having a silent room, I now think it is a bad idea. Instead, doing Pair-Share-Pound in little groups would make it work. Exercises \u00b6 More time to exercises. and the zoom problems made it confusing. I know it\u2019s not your fault. Maybe don\u2019t use breakout rooms so much, especially when you know there are problems? I have never been in a course where it works well to go back and forth between breakout rooms. Just use them as backup if anyone need to get private help. i really need more time for exercises Reflection 4 \u00b6 I agree that the Zoom setup could be improved, next to the technical problems. I think the evaluation questions are mostly useless for me, as judged by me thinking about them. My dream evaluation would be: What should we keep doing? Which teacher(s) scheduled enough time for exercises? What should we improve? Other comments? Course-wide interpretation \u00b6 I think the general course advice would be: talk/demonstrate less, do more exercises instead.","title":"Reflection"},{"location":"reflections/20240312_richel/#reflection","text":"Teaching day: 2023-03-12 Topic: Python Written on 2023-03-12 by Richel","title":"Reflection"},{"location":"reflections/20240312_richel/#schedule","text":"Time Topic Teacher 9.00 Syllabus Birgitte 9.10 Python in general Birgitte 9.20 Load modules and run Birgitte 9.45 Break . 10:00 Packages (45\u2013>30) Richel 10.45 Break . 11.00 Isolated environments Richel 12.00 Lunch . 13.00 Batch Birgitte 13:20 GPU Birgitte 13.30 Kebnekaise: Jupyter Birgitte . Rackham: Interactive session and Jupyter Richel 13.45 Break . 14.00 Parallel and multithreaded functions Pedro 14.25 Conclusion & Q/A Birgitte 14.45 Evaluation Richel","title":"Schedule"},{"location":"reflections/20240312_richel/#reflection-1","text":"I prepared well for the lessons, as I remembered from the previous course iteration that my material was not blended in with the others. This iteration, my material was 100% part of the course website and I removed the presentation slides from the previous iteration. Although I did invest 10 + 2 + 16 + 2 = 30 hours, with little sleep, the day of teaching went fine. The most obvious problem during teaching was that Zoom did not allow me to speak or show video, in the main room, when breakout rooms are active. I think we dealt with this technical problem as good as we reasonably could. However, due to it, I cannot properly evaluate my own lessons. I was annoyed with being this blind as a teacher: how can I know if the learners understood, if I cannot see their exercises? Also, I did not do a half-way-during-the-exercise feedback, nor a nearly-the-end-of-the-hour feedback. And I was unable to see if the exercise lengths matched the time allocated. There were some mistakes: [Fixed] The exercises on venv failed due to the install of a package in the session before. Next time, ruthlessly delete the /home/richel/.local/lib/python3.11 folder. I\u2019ve added this to the course material I could not demo going into the remote desktop website of Rackham. Repeat this next time! I feel that is as much as I have to reflect on. I think it went quite smoothly when correcting for the technical error.","title":"Reflection 1"},{"location":"reflections/20240312_richel/#discussion-on-other-sections","text":"","title":"Discussion on other sections"},{"location":"reflections/20240312_richel/#pre-requirements","text":"I feel the pre-requirements page, [URL] , is too extensive. I feel it should link to regular documentation and mostly show how to determine you fulfilled all pre-requirements.","title":"Pre-requirements"},{"location":"reflections/20240312_richel/#interactive","text":"The session on starting an interactive session feels too fancy twice: create an interactive session with 1 node with more nodes <\u2014 feels beyond the teaching goals run 2 Python scripts, 1 of which is unsuitable for an interactive session with more nodes. I think, for 15 minutes, one can only do 1 node and no Python script, to achieve the teaching goals.","title":"Interactive"},{"location":"reflections/20240312_richel/#impressed-by-birgitte","text":"I think it was impressive that Birgitte logs in into both clusters at the start. I want that too!","title":"Impressed by Birgitte"},{"location":"reflections/20240312_richel/#why-ssh-y","text":"Unrelated to the course, Birgitte does so.","title":"Why ssh -Y"},{"location":"reflections/20240312_richel/#loading-python","text":"If the session is about loading Python, maybe seeing module dependencies can removed. Also, don\u2019t care about python3? Also, don\u2019t care about IPython?","title":"Loading Python"},{"location":"reflections/20240312_richel/#suggest-to-arvid","text":"The Bianca portal is great. Could you do the same for Rackham? Not every user understands one needs to use 2FA now, and how","title":"Suggest to Arvid"},{"location":"reflections/20240312_richel/#more-time-for-sbatch","text":"There was no time for an exercise. I would have enjoy to be sure that the learners have been able to submit a job and see the results.","title":"More time for sbatch"},{"location":"reflections/20240312_richel/#more-time-for-gpu","text":"There was no time for an exercise.","title":"More time for GPU"},{"location":"reflections/20240312_richel/#more-time-for-uppmax-interactive-and-jupyter","text":"There was no time for an exercise.","title":"More time for UPPMAX interactive and Jupyter"},{"location":"reflections/20240312_richel/#parallel-programming","text":"I feel that making a script suitable (with **FIX** in it) for a parallel run is at the \u2018Synthesis\u2019 level of Blooms taxonomy. I feel that some levels lower, e.g. \u2018Apply\u2019 with a step-by-step guide would be more suitable.","title":"Parallel programming"},{"location":"reflections/20240312_richel/#evaluations","text":"","title":"Evaluations"},{"location":"reflections/20240312_richel/#overall-how-would-you-rate-todays-training-event","text":"5: 2x 6: 2x 7: 4x 8: 11x 9: 3x 10: 3x Average: ((2 * 5) + (2 * 6) + (4 * 7) + (11 * 8) + (3 * 9) + (3 * 10)) / 25 = 7.8","title":"Overall, how would you rate today&rsquo;s training event?"},{"location":"reflections/20240312_richel/#todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-what-did-you-like-best","text":"Here I aggregate the learners feedback, where I do put some under multiple topics. As all sentences are unique, such duplicates can be spotted. Structure: The structure is good. exercises and structure Good instructions, positive supportive and friendly environment, nice course structure, basic exercises. The format and structure was good, hands on exercises was also good. materials, exercise and structure Materials and structure were good Exercises: exercises exercises and structure Good instructions, positive supportive and friendly environment, nice course structure, basic exercises. The format and structure was good, hands on exercises was also good. materials, exercise and structure Clear exercises and instructions, availability of deepening material I like that the material was very well prepared and with this guidance I understood concepts and managed to do task that I could not do by myself Materials: good material on the web that was easy to follow The wiki of the course is really good. I will continue using it as a documentation after the course. As, there is no wiki , I will count this under \u2018materials\u2019. materials The topic covered materials, exercise and structure Materials and structure were good Clear exercises and instructions, availability of deepening material [URL to prerequisites] <- this webpage was great! materials Very good material, mostly in one location material gathered in one space info mail was +1, same for material Lectures: The lectures were very clear and the teachers were very good Teachers The lectures were very clear and the teachers were very good Good instructions, positive supportive and friendly environment, nice course structure, basic exercises. Sections: The part before lunch was very clear and easy to understand despite the technical issues I taught before lunch, so maybe this was my part? Misc: A good overview. I think I\u2019ll find the Jupyter lab and env extra useful in the future! info mail was +1, same for material","title":"Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best?"},{"location":"reflections/20240312_richel/#reflection-2","text":"The learners again like the structure of the course and the course material. Maybe they do like the my teaching (\u2018The part before lunch\u2019) and my exercises even better (\u2018basic exercises\u2019, \u2018hands on exercises\u2019). I am happy one learner mentioned \u2018positive supportive and friendly environment\u2019!","title":"Reflection 2"},{"location":"reflections/20240312_richel/#todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-where-should-we-improve","text":"Here I aggregate the learners feedback, where I do put some under multiple topics. As all sentences are unique, such duplicates can be spotted.","title":"Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve?"},{"location":"reflections/20240312_richel/#compliments","text":"None Very good, they were motivated and clear.","title":"Compliments"},{"location":"reflections/20240312_richel/#time-organization-and-afternoon-was-fast-or-too-short","text":"a bit more time dedicated to Jupyter The time dedicated for the interactive session/jupyter was not sufficient to try ourselves. The last part on Parallel and multi-threaded functions was a bit fast due to time which happens of course but it would be good to learn more on that. materials The after lunch section was really difficult for me to follow, I feel like the lecturers didn\u2019t stop long enough on each topic to explain it properly and I only got a partial understanding Maybe a better distribution of time between exercises would benefit. Spend less time on the beginning and give more time to run examples on the interactive and parallel parts. Because I think this is the part were we need more help. the 14:00 to 15:00 session was a bit hard to keep up with. Time per exercise can be optimised. More time for python jupyter and GPU exercises will be good for us. Time organization: morning pace was slow compared to the afternoon one. Maybe more time of coding together with the lecturer, especially for the second part of the program (more complex concepts of batch work and interactive work) The first part was a good pace for me, but the second part was more complicated and I did not have time to follow and do the exercises. Maybe the second part could be done in another session, because it is the most important part to take use the resources and take advantage of that. However, I got many tools to start my own projects. \u2018The first part\u2019 was partially my part :-) more time for exercises in afternoon, more time for the batch section, maybe don\u2019t ask random people to answer stuff I don\u2019t like that (stress - introvert)","title":"Time organization and afternoon was fast or too short"},{"location":"reflections/20240312_richel/#teaching","text":"Teaching was quite fast. Sometime hard to follow. I personally thought Birgitte was too fast and I could not follow. I liked Richel the most. Hey, that is a fun compliment! Thanks! more time for exercises in afternoon, more time for the batch section, maybe don\u2019t ask random people to answer stuff I don\u2019t like that (stress - introvert) I will keep asking random people, because a teacher needs to monitor the progress of all learners. However, at that day, I did not think it was necessary to share why I ask random learners and that it is recommended by the literature. I would correct this two days later, let\u2019s see if that made it into the feedback. more and slower code-alongs The pace was relatively slow, which was great. as then also those how have very limited knowledge can also keep up the pace. However, in the afternoon it clearly went more faster. Maybe speed up in the morning a bit to have more time practicing actually submitting the jobs? As that was my main motivation for this, I basically skipped the very last lecture about parallelization and instead coded to make submitting batch jobs work.","title":"Teaching"},{"location":"reflections/20240312_richel/#materials","text":"There were some errors/not updated in the exercises in the material. Also can more of the material be on the course pages instead of pointing to documentation at the centers? It makes it clearer when we don\u2019t have to go difference places I understand this feedback, yet I do think it is a bad idea: a learner should get accustomed to the documentation of his/her HPC center.","title":"Materials"},{"location":"reflections/20240312_richel/#other","text":"If prerequisite is well informed, such as little knowledge on Linus? it would have been better.","title":"Other"},{"location":"reflections/20240312_richel/#reflection-3","text":"Most of the negative feedback was about the high pace in general, yet mostly in the afternoon. I feel I was not the reason for that feedback. I did get negative feedback on: course material in one place ask random people Although I agree with the feedback, I will not change this, as there are good reasons to do so.","title":"Reflection 3"},{"location":"reflections/20240312_richel/#length-of-teaching-today-was","text":"Too short: 8x Adequate: 16x Too long: 1x I think learner-centered teaching would solve this, i.e. by only teaching that amount of concepts that are confirmed to be understood by an exercise.","title":"Length of teaching today was"},{"location":"reflections/20240312_richel/#depth-of-content-was","text":"Too superficial: 5x Adequate: 20x","title":"Depth of content was"},{"location":"reflections/20240312_richel/#the-pace-of-teaching-was","text":"Too slow: i Adequate: iiiiiiiiiiiiiii Too fast: iiiiiiii I think learner-centered teaching would solve this, as -by definition- one follows the pace of the learners.","title":"The pace of teaching was"},{"location":"reflections/20240312_richel/#teaching-aids-used-eg-slides-were-well-prepared","text":"Agree completely: 7x Agree: 15x No strong feelings: 3x","title":"Teaching aids used (e.g. slides) were well prepared"},{"location":"reflections/20240312_richel/#hands-on-exercises-and-demonstrations-were","text":"Adequate: 17x Too few: 7x I think this is a nonsense questions: either we do exercises (i.e. active teaching) or we do demonstrations (i.e. passive teaching). Only from this question, one would have idea what was too few off: only the written texts earlier indicates that there should be more exercises.","title":"Hands-on exercises and demonstrations were"},{"location":"reflections/20240312_richel/#hands-on-exercises-and-demonstrations-were-well-prepared","text":"Agree completely: 7x Agree: 13x No strong feelings: 4x Disagree: 1x","title":"Hands-on exercises and demonstrations were well prepared"},{"location":"reflections/20240312_richel/#how-would-you-rate-the-instructors-overall-teaching-performances","text":"This is a useless question, as there were 4 teachers, but here goes: 10: 4x 9: 5x 8: 4x 7: 8x 6: 3x Average: ((4 * 10) + (5 * 9) + (4 * 8) + (8 * 7) + (3 * 6)) / 24 = 8.0","title":"How would you rate the instructors overall teaching performances?"},{"location":"reflections/20240312_richel/#do-you-feel-you-achieved-your-desired-learning-outcome","text":"Yes: 17x Not sure: 8x","title":"Do you feel you achieved your desired learning outcome?"},{"location":"reflections/20240312_richel/#did-todays-course-meet-your-expectation","text":"Yes: 18x Not sure: 7x","title":"Did today&rsquo;s course meet your expectation?"},{"location":"reflections/20240312_richel/#do-you-have-any-additional-comments","text":"","title":"Do you have any additional comments?"},{"location":"reflections/20240312_richel/#thanks","text":"Thanks for the course, it was very useful and I feel encouraged to start my project. Overall a great introductory course to get grasp of the basics in a positive and supportive environment with knowledgeable instructors. Thanks for your lecture and your time.","title":"Thanks"},{"location":"reflections/20240312_richel/#misc","text":"Can we have something like BYOC and you can show us how to use it to asses time and core used using library multiprocess, please? A bit too uneven speed with too low in the morning and too high in the afternoon or at least too little time in the afternoon to go through everything Agreed, I think learning-centered teaching would solve this. Some students had problems logging in to the needed systems. I also just noted that there was this log-in session in Monday when you mentioned it on tuesday. Maybe advertise that in the very beginning of the information email so that it does not go unnoticed. I had no problems logging etc, but those who had would have benefited participating on Monday Agreed, we could be more ruthless in letting our learners in, e.g. by putting the link to the Zoom on Rackham and Kebnekaise.","title":"Misc"},{"location":"reflections/20240312_richel/#diversity-of-teachers","text":"I can not evaluate the performance of each part in a single evaluation form, as I felt it was very different depending on each teacher Agreed.","title":"Diversity of teachers"},{"location":"reflections/20240312_richel/#zoom-room","text":"More time to exercises. and the zoom problems made it confusing. I know it\u2019s not your fault. Maybe don\u2019t use breakout rooms so much, especially when you know there are problems? I have never been in a course where it works well to go back and forth between breakout rooms. Just use them as backup if anyone need to get private help. I agree that the technical problems are annoying. The Zoom room use, i.e. of having a silent room, I now think it is a bad idea. Instead, doing Pair-Share-Pound in little groups would make it work.","title":"Zoom room"},{"location":"reflections/20240312_richel/#exercises","text":"More time to exercises. and the zoom problems made it confusing. I know it\u2019s not your fault. Maybe don\u2019t use breakout rooms so much, especially when you know there are problems? I have never been in a course where it works well to go back and forth between breakout rooms. Just use them as backup if anyone need to get private help. i really need more time for exercises","title":"Exercises"},{"location":"reflections/20240312_richel/#reflection-4","text":"I agree that the Zoom setup could be improved, next to the technical problems. I think the evaluation questions are mostly useless for me, as judged by me thinking about them. My dream evaluation would be: What should we keep doing? Which teacher(s) scheduled enough time for exercises? What should we improve? Other comments?","title":"Reflection 4"},{"location":"reflections/20240312_richel/#course-wide-interpretation","text":"I think the general course advice would be: talk/demonstrate less, do more exercises instead.","title":"Course-wide interpretation"},{"location":"reflections/20240314_richel/","text":"Reflection \u00b6 Teaching day: 2023-03-14 Topic: R Written on 2023-03-14 Time Topic Teacher 9:00 Syllabus Richel 9:10 R in general Richel 9:20 Load modules and run Richel 9:45 Break . I only taught the first hour. I talk before teaching, it feels nicer. At the start of the lecture, I shared that I will pick random people and why. Adding the \u2018why\u2019 hopefully helps learners understand why I should do that. I feel discussing teaching literature should not be done during a lesson, yet, however, it does show that I read (part of) the student evaluation. At the \u2018Prior Knowledge\u2019 part, I asked randomly and polled answers using Zoom \u2018Reactions\u2019. When discussing modules, it made me see that not everyone understood what modules are. This allowed me to backtrack and discuss what modules are. I feel I talked too long and I went to slow. Instead of going through the text, only give an overview and enough info to do the exercises. Exercises should start with \u2018read \u2026\u2019. [ ] TODO: talk less, more time for exercises Could not do \u2018Feedback\u2019 due to technical problem. Technical problem was solved after updating Zoom. There was a copy-paste mistakes in the R versions used, as there is a difference between the one on HPC2N and the one on UPPMAX. During the exercises, the learners found out, contacted me and I fixed it immediately. There were mistakes in the HPC2N answers. During the exercises, the HPC2N users found out that it did not work. This is probably because the course switched to using R 4.1.2 and it was forgotten to update the version of the prerequisite modules. During a one-on-one with a learner in a breakout room, we used module help R/4.1.2 , loaded the prerequisites of the right version and observed it worked. I immediately updated the documentation. Although there were mistakes found in my exercises, I gave the learners the most time on doing exercises, monitored their progress during this and interacted with them. Or: mistakes were found in my exercises, because the exercises were actually done with a level of interaction that allows the learners to actually share the mistakes. I wonder how many learners: have started R (I could not ask, due to technical problem) have run an R script (I could not ask, due to technical problem) I lead the evaluation. I took the learners to a Breakout room, told them why this is important, how to answer and to come back to the main room to say bye or ask questions. Things beyond my teaching \u00b6 There was a mismatch between the schedule we as teachers had in a shared HackMD document and the one on the course website. Already in a meeting, I suggested to remove the shared HackMD document one, to prevent the possibility of a mismatch, but there was resistance there. The teachers improvised. I have a hard time to summarize the learning objectives of other lessons: there is too much stuff that is \u2018nice to have\u2019 in it; it feels more like if randomly content gets added or deleted. What is the decision rule to decide what should be in and what should not for those lessons? Examples of content I remove, as it would not fit my teaching goals: describing an R package in details: this is not an R course when learning how to run an R script, running it on multiple nodes: do that in the session on parallel programs on R virtual environments: use a virtual environment, not describe what it is in detail in the parallel programs, letting learners fix code for parallelizing code: this is not a course on parallel programming. Instead, let them run code that can do calculations in parallel, on the CPU/GPU I feel that when someone is teaching, he/she should prioritize interacting with the learners over answering questions in a shared document. I feel the other team members should answer the questions in a shared document, so that the main teacher can focus on interacting with the learners. For the other session, I wonder how many learners: have created a virtual environment have run code that uses a GPU have run code that uses machine learning have installed a package from CRAN have installed a package from GitHub have logged in with the remote desktop website have logged in with a local ThinLinc client have started RStudio Evaluation \u00b6 Overall, how would you rate today\u2019s training event? \u00b6 7: 5x 9: 6x 8: 1x Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? \u00b6 materials and structure materials are great materials introduction and packages The explanation on how R in hpc environment can be used for machine learning and how to run RStudio interface on cluster materials are well structured exercises structure and material The exercises were helpful step by step lessons; classes available in youtube for later watching Hey, someone likes our YouTube videos! well thought structure and training exercises with clear and helpful lectures I liked the handling of packages, since it is very useful in R! Today\u2019s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? \u00b6 maybe more about onboarding I agree that this was advertised too late. exercises could be improved in tandem speed with students. Parallel and multithreaded functions was short and fast and would have liked to get info on background, how, why instead of just Learning how to sbatch. Would have been good to learn more about the topics. Overall organization, speaking turns, given time and supervision for exercices I would have liked to learn how the parellel code works rather than just knowing it can be sbatched I think Pedro has no time for that, we teach at the Apply level. it felt rushy at some points exercises Two things, maybe more time for parallel computing and Machine Learning I would also have enjoyed Matlab but I think knowing R and Python in HPC is enough for applying this knowledge to Matlab. more time for the code-along and training exercises, maybe separating the code-along for the different centers on the presenter\u00b4s screen that was a bit confusing from time to time. Also when giving lecture do one center at on time and then move on I would have needed a bit more time for the exercises, and maybe a bit more? I\u2019ll finish it later =) But otherwise very good. The usual: learners had too little time for exercises. As the one teaching only the first hour with most time spend on an exercise, I assume this address me. Length of teaching today was \u00b6 Adequate: 11x Too short: 2x Depth of content was \u00b6 Adequate: 12x Too superficial: 1x The pace of teaching was \u00b6 Adequate: 8x Too fast: 5x Teaching aids used (e.g. slides) were well prepared \u00b6 Agree completely: 4x Agree: 7x No strong feelings: 2x Hands-on exercises and demonstrations were \u00b6 Adequate: 7x Too few: 6x Hands-on exercises and demonstrations were well prepared \u00b6 Agree completely: 4x Agree: 6x No strong feelings: 3x How would you rate the instructors overall teaching performances? \u00b6 10: 1x 9: 4x 8: 4x 7: 2x 6: 2x Do you feel you achieved your desired learning outcome? \u00b6 Yes: 11x Not sure: 2x Did today\u2019s course meet your expectation? \u00b6 Yes: 11x Not sure: 1x No: 2x Do you have any additional comments? \u00b6 Thanks More time for hands on exercises would be appreciated. The teaching felt unidirectional as it was too fast that there was no space to ask questions and interact Can we have a specific ML course on R and python online please? It will be great to build models with you guys. Reflection after evaluation \u00b6 I think the evaluation questions are mostly useless for me, as judged by me thinking about them. My dream evaluation would be: What should we keep doing? Which teacher(s) scheduled enough time for exercises? What should we improve? Other comments?","title":"Reflection"},{"location":"reflections/20240314_richel/#reflection","text":"Teaching day: 2023-03-14 Topic: R Written on 2023-03-14 Time Topic Teacher 9:00 Syllabus Richel 9:10 R in general Richel 9:20 Load modules and run Richel 9:45 Break . I only taught the first hour. I talk before teaching, it feels nicer. At the start of the lecture, I shared that I will pick random people and why. Adding the \u2018why\u2019 hopefully helps learners understand why I should do that. I feel discussing teaching literature should not be done during a lesson, yet, however, it does show that I read (part of) the student evaluation. At the \u2018Prior Knowledge\u2019 part, I asked randomly and polled answers using Zoom \u2018Reactions\u2019. When discussing modules, it made me see that not everyone understood what modules are. This allowed me to backtrack and discuss what modules are. I feel I talked too long and I went to slow. Instead of going through the text, only give an overview and enough info to do the exercises. Exercises should start with \u2018read \u2026\u2019. [ ] TODO: talk less, more time for exercises Could not do \u2018Feedback\u2019 due to technical problem. Technical problem was solved after updating Zoom. There was a copy-paste mistakes in the R versions used, as there is a difference between the one on HPC2N and the one on UPPMAX. During the exercises, the learners found out, contacted me and I fixed it immediately. There were mistakes in the HPC2N answers. During the exercises, the HPC2N users found out that it did not work. This is probably because the course switched to using R 4.1.2 and it was forgotten to update the version of the prerequisite modules. During a one-on-one with a learner in a breakout room, we used module help R/4.1.2 , loaded the prerequisites of the right version and observed it worked. I immediately updated the documentation. Although there were mistakes found in my exercises, I gave the learners the most time on doing exercises, monitored their progress during this and interacted with them. Or: mistakes were found in my exercises, because the exercises were actually done with a level of interaction that allows the learners to actually share the mistakes. I wonder how many learners: have started R (I could not ask, due to technical problem) have run an R script (I could not ask, due to technical problem) I lead the evaluation. I took the learners to a Breakout room, told them why this is important, how to answer and to come back to the main room to say bye or ask questions.","title":"Reflection"},{"location":"reflections/20240314_richel/#things-beyond-my-teaching","text":"There was a mismatch between the schedule we as teachers had in a shared HackMD document and the one on the course website. Already in a meeting, I suggested to remove the shared HackMD document one, to prevent the possibility of a mismatch, but there was resistance there. The teachers improvised. I have a hard time to summarize the learning objectives of other lessons: there is too much stuff that is \u2018nice to have\u2019 in it; it feels more like if randomly content gets added or deleted. What is the decision rule to decide what should be in and what should not for those lessons? Examples of content I remove, as it would not fit my teaching goals: describing an R package in details: this is not an R course when learning how to run an R script, running it on multiple nodes: do that in the session on parallel programs on R virtual environments: use a virtual environment, not describe what it is in detail in the parallel programs, letting learners fix code for parallelizing code: this is not a course on parallel programming. Instead, let them run code that can do calculations in parallel, on the CPU/GPU I feel that when someone is teaching, he/she should prioritize interacting with the learners over answering questions in a shared document. I feel the other team members should answer the questions in a shared document, so that the main teacher can focus on interacting with the learners. For the other session, I wonder how many learners: have created a virtual environment have run code that uses a GPU have run code that uses machine learning have installed a package from CRAN have installed a package from GitHub have logged in with the remote desktop website have logged in with a local ThinLinc client have started RStudio","title":"Things beyond my teaching"},{"location":"reflections/20240314_richel/#evaluation","text":"","title":"Evaluation"},{"location":"reflections/20240314_richel/#overall-how-would-you-rate-todays-training-event","text":"7: 5x 9: 6x 8: 1x","title":"Overall, how would you rate today&rsquo;s training event?"},{"location":"reflections/20240314_richel/#todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-what-did-you-like-best","text":"materials and structure materials are great materials introduction and packages The explanation on how R in hpc environment can be used for machine learning and how to run RStudio interface on cluster materials are well structured exercises structure and material The exercises were helpful step by step lessons; classes available in youtube for later watching Hey, someone likes our YouTube videos! well thought structure and training exercises with clear and helpful lectures I liked the handling of packages, since it is very useful in R!","title":"Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best?"},{"location":"reflections/20240314_richel/#todays-content-and-feedback-to-the-lecturers-eg-materials-exercises-structure-where-should-we-improve","text":"maybe more about onboarding I agree that this was advertised too late. exercises could be improved in tandem speed with students. Parallel and multithreaded functions was short and fast and would have liked to get info on background, how, why instead of just Learning how to sbatch. Would have been good to learn more about the topics. Overall organization, speaking turns, given time and supervision for exercices I would have liked to learn how the parellel code works rather than just knowing it can be sbatched I think Pedro has no time for that, we teach at the Apply level. it felt rushy at some points exercises Two things, maybe more time for parallel computing and Machine Learning I would also have enjoyed Matlab but I think knowing R and Python in HPC is enough for applying this knowledge to Matlab. more time for the code-along and training exercises, maybe separating the code-along for the different centers on the presenter\u00b4s screen that was a bit confusing from time to time. Also when giving lecture do one center at on time and then move on I would have needed a bit more time for the exercises, and maybe a bit more? I\u2019ll finish it later =) But otherwise very good. The usual: learners had too little time for exercises. As the one teaching only the first hour with most time spend on an exercise, I assume this address me.","title":"Today&rsquo;s content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve?"},{"location":"reflections/20240314_richel/#length-of-teaching-today-was","text":"Adequate: 11x Too short: 2x","title":"Length of teaching today was"},{"location":"reflections/20240314_richel/#depth-of-content-was","text":"Adequate: 12x Too superficial: 1x","title":"Depth of content was"},{"location":"reflections/20240314_richel/#the-pace-of-teaching-was","text":"Adequate: 8x Too fast: 5x","title":"The pace of teaching was"},{"location":"reflections/20240314_richel/#teaching-aids-used-eg-slides-were-well-prepared","text":"Agree completely: 4x Agree: 7x No strong feelings: 2x","title":"Teaching aids used (e.g. slides) were well prepared"},{"location":"reflections/20240314_richel/#hands-on-exercises-and-demonstrations-were","text":"Adequate: 7x Too few: 6x","title":"Hands-on exercises and demonstrations were"},{"location":"reflections/20240314_richel/#hands-on-exercises-and-demonstrations-were-well-prepared","text":"Agree completely: 4x Agree: 6x No strong feelings: 3x","title":"Hands-on exercises and demonstrations were well prepared"},{"location":"reflections/20240314_richel/#how-would-you-rate-the-instructors-overall-teaching-performances","text":"10: 1x 9: 4x 8: 4x 7: 2x 6: 2x","title":"How would you rate the instructors overall teaching performances?"},{"location":"reflections/20240314_richel/#do-you-feel-you-achieved-your-desired-learning-outcome","text":"Yes: 11x Not sure: 2x","title":"Do you feel you achieved your desired learning outcome?"},{"location":"reflections/20240314_richel/#did-todays-course-meet-your-expectation","text":"Yes: 11x Not sure: 1x No: 2x","title":"Did today&rsquo;s course meet your expectation?"},{"location":"reflections/20240314_richel/#do-you-have-any-additional-comments","text":"Thanks More time for hands on exercises would be appreciated. The teaching felt unidirectional as it was too fast that there was no space to ask questions and interact Can we have a specific ML course on R and python online please? It will be great to build models with you guys.","title":"Do you have any additional comments?"},{"location":"reflections/20240314_richel/#reflection-after-evaluation","text":"I think the evaluation questions are mostly useless for me, as judged by me thinking about them. My dream evaluation would be: What should we keep doing? Which teacher(s) scheduled enough time for exercises? What should we improve? Other comments?","title":"Reflection after evaluation"},{"location":"reflections/20241022_richel/","text":"Reflection \u00b6 Teaching day: 2024-10-22 Topic: Python Written on 2024-10-22 Schedule \u00b6 Time Topic Teacher(s) 9:00 (optional) First login BB + PO + RB + RP 9:45 Break . 10:00 Syllabus RP 10:10 Python in general RP 10:20 Load modules and run RP 10:50 Break . 11:05 Packages RB 11:35 Isolated environments RB 12:00 Lunch . 13:00 Batch BB 13:30 GPU BB 13:50 Break . 14:05 Simultaneous sessions BB RB RP 14:35 Break . 14:50 Parallel and multi-threaded functions PO 15:35 Summary and evaluation RB 15:50 End of the day . Reflection before teaching \u00b6 Before teaching, one thing that I think can be improved, is our decision making. As an example, there was a change decided upon regarding the schedule on 2024-10-18 (course is at 2024-10-21), after we\u2019ve already voted to accept the schedule before that. I was unconvinced that we should change the schedule, but I was a minority, so I accepted the democratic decision to change the schedule again. I hope next time we can decide on the schedule earlier and freeze it, say, at least one week before the course. Before teaching, one thing that I think can be improved too, is to use Markdown instead of RestructuredText: using RestructuredText makes everything I do harder. Taking a look at the StackOverflow developer survey of 2023 , Markdown is mentioned and admired, where RestructuredText is completely absent. I\u2019d enjoy converting all these pages to Markdown and then use good MkDocs instead Before teaching, I was wondering about how we make sure the learners log in. Currently, there is an onboarding session and a daily 9:00-9:50 optional login session. I feel we treat the learners better than reasonable: when it is in the course prerequisites that its is required to be able to log in and we have an onboarding, we should be able to assume that learners can log in. This means that when a learner joins without being able to log in, it is acceptable to only help the learner when there is time (which is close to never). One way to meet in the middle, is to have daily log in sessions at 8:00, so this will not affect the teaching. Before teaching, I saw the evaluation. Beyond the \u2018rate your confidence\u2019 questions I prepared, more questions were added. Maybe because HPC2N requires that. These are the questions in the evaluation now: Introduction to running R, Python, Julia, and Matlab in HPC, 22-25/10-2024 - DAY 1 Python Thanks for your feedback. This feedback will be published as-is at the end of the evaluation period (after 1 November 2024), if and only if there are no personal details (email, address, etc.) in the feedback. Do mention the teachers, assistants, etc by name! 1.Overall, how would you rate today's training event? - Value from 1 to and including 10 2.Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? - Open question 3.Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? - Open question 4.Training event organisation (e.g. announcement, registration, ...): \u2013 What did you like best? \u2013 Where should we improve? - Open question 5.Length of teaching today was - Adequate - Too short - Too long 6.Depth of content was - Adequate - Too superficial - Too profound 7.The pace of teaching was - Adequate - Too slow - Too fast 8.Teaching aids used (e.g. slides) were well prepared - Agree completely - Agree - No strong feelings - Disagree - Disagree completely 9.Hands-on exercises and demonstrations were - Adequate - Too few - Too many 10.Hands-on exercises and demonstrations were well prepared - Agree completely - Agree - No strong feelings - Disagree - Disagree completely 11.How would you rate the separate sessions? - Introduction - Load and run - Packages - Isolated environments - Python in batch - Interactive work - Jupyter - Parallel - Conda With answers - Poor - Fair - Good - Very good - Excellent - Did not attend Give you confidence levels of the following statements, using this scale: - 0: I don't know even what this is about ...? - 1: I have no confidence I can do this - 2: I have low confidence I can do this - 3: I have some confidence I can do this - 4: I have good confidence I can do this - 5: I absolutely can do this! Give you confidence levels of the following statements below: - I can use the module system to load a specific version of Python - I can run Python - I can use the Python interpreter - I can run IPython - I can use the IPython interpreter - I can run a Python script - I can determine the version of a Python package - I can determine that a Python package is not installed - I can load a Python package module - I can install a Python package using ``pip`` - I can work (create, activate, work, deactivate) with a ``venv`` virtual environment - I can write a bash script - I can submit a script to the job scheduler - I can write a bash script that uses GPUs - I can start an interactive session - I can check that I am in an interactive session - I can start an interactive session with multiple cores - I can check that I am in an interactive session with multiple cores - I can start Jupyter - I can start a script that uses parallel code - I can measure the effect of using more nodes for parallel code 13.Did today's course meet your expectation? - Yes - No - Not sure 14.Which future training topics would you like to be provided by the training host(s)? - Open question 15.Do you have any additional comments? - Open question Only question 12 (the confidences) and 15 (any other comments) are the ones that matter to me. Question 11 is useful to me, but needlessly personal, hurtful and I don\u2019t care about satisfaction. [ ] In meeting with teachers, discuss the evaluation form Reflection after teaching \u00b6 9:18: Around 12 learners have visited. After telling each of them that they can go if (1) they have logged in and (2) can use a text editor, around 3 hang around with their cameras off. I assume they are waiting for the lesson to start. [x] Make this more explicit in the Python and R schedule The course started at 10:01. Due to technical issues, I took over the first sessions without any preparations. I improvised in following the Mike Bell teaching model with Prior, Present, Challenge and Feedback and I think that went well enough. During this improvised session, around a third of the learners had their camera on, as per my request. I felt there was enough time to work on exercises without disturbance. When the technical issues were over, at 10:35, my colleague took over the teacher role again. In that improvised sessions, I did not talk much about IPython, as I feel it is useless and time should be spent differently, hence I predict low confidence for IPython. Additionally, a colleague helped me remind about a shared document where learners can ask questions. Also here, I spent less time on this shared document, as I think it is not worth my time to talk about a shared document I do not care about. I am happy how we, teachers, dealt with the technical problem. There was a learner that found a mistake in my teaching material, about a mismatch in versions of matplotlib. I already fixed it :-) My prepared part started at the 11:05 sharp (as per schedule). Here, a fourth of the learners had their camera on. I felt there was enough time to work on exercises without disturbance. The durations of the sessions was long enough. When polling if learners had completed the exercise by using Zoom Reactions, two thirds gave a \u2018yes\u2019 as a reaction, the rest (all without camera) having a \u2018no\u2019 as a reaction. One thing I did wrong, was a question from a LUNARC. She wanted to know if pip list --user required an additional argument, such as a username. Instead of asking for clarification first, I rushed to give an answer, that was different from her question. My bad! One possible cause for this is that I feel insecure about pip list --user : I\u2019ve never used it and I don\u2019t care, but it was in the course material. I should remove this. [x] Remove pip list --user from the course material After the break, there was a simultaneous session that I was allowed to teach, for UPPMAX users only. There 6 out of the 7 learners had their camera on. Here, all learners with a camera having achieved the teaching goal of starting Jupyter. 1 out of 7 got stuck and I lead her to also start Jupyter. I felt there was enough time to work on the exercise without disturbance. The durations of the session was long enough. I did not prepare for this session at all, as the material was good enough: I\u2019d feel at home with an UPPMAX-only group anyways and am confident I can improvise there. I did not use the course material at all, instead send the learners to the documentation page with the procedure. I think that that documentation page can be improved, by rewriting an existing page. [x] Improve https://docs.uppmax.uu.se/software/jupyter/ similar to https://docs.uppmax.uu.se/software/jupyter_on_bianca/ [x] Add video What I noticed in the simultaneous session, upon questioning the learners, that 6 out of 6 learners did not know what a login node is. From that I infer that this course is the first formal introduction to our clusters for most of our learners. For the summary and evaluation, I used question 12 of the evaluation form. I think that was good enough. I did not use a breakout room, as we did not practice that enough today. In more general terms: I am happy to have had questions asked by learners during sessions. It hints that some think I am approachable. I am happy that all my exercises have answers and a video: the Feedback is there. Also, Feedback consisted out of two demos by learners and one one-on-one demo by me. The answers I provide, however, can be improved, by adding more \u2018How does that look like?\u2019 dropdowns. I did not do it this time, as I struggled with the RST markup too much All my exercise sessions were 15 minutes. This was a good length. It was a lot of waiting and I was able to be silent too I am happy that my exercises use wget instead of a tarball. The word \u2018tarball\u2019 seemed already new for the learners, judging by their facial expression I am reasonably happy with the amount of interaction I got. I spoke with all learners that had their camera on. There were too many cameras off for my taste, maybe the recording causes this? Suggests for next course: The prerequirements should only be pre-requirements, not things that are \u2018nice to have\u2019. I suggest to remove the whole \u2018Get familiar with the Linux/Bash command line\u2019 session The login sessions should only be about login. Stating what the goal of this optional session is, would be enough. I suggest to remove anything but that. Remove IPython from session on running Python The evaluation form will be useless: in a course with 4 teachers, rating the course as a whole gives little information that is useful to a teacher. I don\u2019t care about the satisfaction of learners either: all I care about is if they have learned something, i.e. if they are confident in a topic I taught. I am interested in their free comments too. Besides that, the evaluation form feels like an administrative monster Summary of evaluation results \u00b6 There are 5 evaluation results, out of the 9 learners that stayed in the end. 1. Overall, how would you rate today's training event? - 7.4. Average Number This question gives me no useful info. 2. Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? - \"the best parts were how to find correct modules, submit jobs, and the examples for parallel codes \" - \"materials\" This question gives me no useful info. 3. Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? - \"the lecturers shouldn't say the person's names and point to specific persons to reply questions\" - \"Going through the examples more throughly.\" I will definitely keep pointing to learners, as per the literature. When I told the class I would do so, I told them to turn off the cameras if they\u2019d be uncomfortable with me doing so. I don\u2019t know why this learner kept his/her camera on if he/she is uncomfortable with this. I did explain why I think this is important (to get to know what all learners think, not just the ones that are more vocal) and I stressed multiple times that \u2018I don\u2019t know\u2019 is a fine answer too. In other courses, where I take the time to show my teaching style, this feedback is not found in the evaluations. Next time, I will show the references to the literature again. [ ] Next iteration: take the time to discuss my teaching style 4. Training event organisation (e.g. announcement, registration, ...): \u2013 What did you like best? \u2013 Where should we improve? - \"material online is excellent\" - \"well organized\" No info. 5. Length of teaching today was Adequate 3 Too short 1 Too long 1 No info. 6. Depth of content was Adequate 5 Too superficial 0 Too profound 0 No info. 7. The pace of teaching was Adequate 3 Too slow 0 Too fast 2 No info: which session? 8. Teaching aids used (e.g. slides) were well prepared Agree completely 0 Agree 4 No strong feelings 0 Disagree 1 Disagree completely 0 No info: which session? 9. Hands-on exercises and demonstrations were Adequate 3 Too few 2 Too many 0 No info: which session? 10. Hands-on exercises and demonstrations were well prepared Agree completely 3 Agree 2 No strong feelings 0 Disagree 0 Disagree completely 0 No info: which session? 11. How would you rate the separate sessions? I am happy to see that the \u2018Excellents\u2019 are all in sections that I taught (\u2018Packages\u2019, \u2018Isolated environments\u2019) or that I may have taught (i.e. these we discussed in the simultaneous session) (\u2018Interactive work\u2019, \u2018Jupyter\u2019) or that I partially taught (\u2018Load and run\u2019). The lowest rating (\u2018poor\u2019) is in none of my sessions. I do see a low \u2018fair\u2019 rating in my sessions, except for Jupyter, where there are more teachers. My theory is that the person that did not like to be addressed by name followed Jupyter in the HPC2N or LUNARC session and gave a \u2018fair\u2019 to the other sessions. 12. Give your confidence levels of the following statements I am happy to see that the IPython question is not there. Seems like I am not the only one that thinks it can be removed from the course material? The confidences that were lowest of the part I am one of the teachers are \u2018I can start an interactive session with multiple cores\u2019 and \u2018I can check that I am in an interactive session with multiple cores\u2019. I for sure did not teach this, as I felt there are more important things to do. Also: I think other things are more important. The sessions that I do care about .. I can determine the version of a Python package I can determine that a Python package is/is not installed I can load a Python (machine learning) module I can install a Python package I can work (create, activate, work, deactivate) with a venv virtual environment \u2026 did reasonably well, with 1 learner stating \u2018I have some confidence in this\u2019. I wish I knew what was the problem with that one learner. A weird response is \u2018I can export and import a virtual environment\u2019, which had same confidences as the other session, although I did not teach it! I hoped that the learners would all state \u2018I have no idea what this is about\u2019. Maybe some carry-over effect? [x] Remove question \u2018I can export and import a virtual environment\u2019 from evaluation 13.Did today's course meet your expectation? Yes 3 No 0 Not sure 2 Useless info to me. 14. Which future training topics would you like to be provided by the training host(s)? \"- xarray and dask\" Useless info to me. 15. Do you have any additional comments? - A sheet summarizing the most important parts with key code snippets to do the items in 12)\" Fun! This learner suggests that I actually should have worked on a proper summary page, with -indeed- the key points. Will do so next time! Thanks! [ ] Base summary on evaluation form Analyzing the amount of learners \u00b6 Analyzing the amount of learners present during the day (data: 20241022_counts.csv ) \u2026 I see that there have been 19 learners max (at around 10:35), where there were 10 at the start of lunch. This happened even though these sessions are evaluated positively. Maybe this hints that we cannot convince the learners why these things are important, but that is just a guess\u2026","title":"Reflection"},{"location":"reflections/20241022_richel/#reflection","text":"Teaching day: 2024-10-22 Topic: Python Written on 2024-10-22","title":"Reflection"},{"location":"reflections/20241022_richel/#schedule","text":"Time Topic Teacher(s) 9:00 (optional) First login BB + PO + RB + RP 9:45 Break . 10:00 Syllabus RP 10:10 Python in general RP 10:20 Load modules and run RP 10:50 Break . 11:05 Packages RB 11:35 Isolated environments RB 12:00 Lunch . 13:00 Batch BB 13:30 GPU BB 13:50 Break . 14:05 Simultaneous sessions BB RB RP 14:35 Break . 14:50 Parallel and multi-threaded functions PO 15:35 Summary and evaluation RB 15:50 End of the day .","title":"Schedule"},{"location":"reflections/20241022_richel/#reflection-before-teaching","text":"Before teaching, one thing that I think can be improved, is our decision making. As an example, there was a change decided upon regarding the schedule on 2024-10-18 (course is at 2024-10-21), after we\u2019ve already voted to accept the schedule before that. I was unconvinced that we should change the schedule, but I was a minority, so I accepted the democratic decision to change the schedule again. I hope next time we can decide on the schedule earlier and freeze it, say, at least one week before the course. Before teaching, one thing that I think can be improved too, is to use Markdown instead of RestructuredText: using RestructuredText makes everything I do harder. Taking a look at the StackOverflow developer survey of 2023 , Markdown is mentioned and admired, where RestructuredText is completely absent. I\u2019d enjoy converting all these pages to Markdown and then use good MkDocs instead Before teaching, I was wondering about how we make sure the learners log in. Currently, there is an onboarding session and a daily 9:00-9:50 optional login session. I feel we treat the learners better than reasonable: when it is in the course prerequisites that its is required to be able to log in and we have an onboarding, we should be able to assume that learners can log in. This means that when a learner joins without being able to log in, it is acceptable to only help the learner when there is time (which is close to never). One way to meet in the middle, is to have daily log in sessions at 8:00, so this will not affect the teaching. Before teaching, I saw the evaluation. Beyond the \u2018rate your confidence\u2019 questions I prepared, more questions were added. Maybe because HPC2N requires that. These are the questions in the evaluation now: Introduction to running R, Python, Julia, and Matlab in HPC, 22-25/10-2024 - DAY 1 Python Thanks for your feedback. This feedback will be published as-is at the end of the evaluation period (after 1 November 2024), if and only if there are no personal details (email, address, etc.) in the feedback. Do mention the teachers, assistants, etc by name! 1.Overall, how would you rate today's training event? - Value from 1 to and including 10 2.Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? - Open question 3.Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? - Open question 4.Training event organisation (e.g. announcement, registration, ...): \u2013 What did you like best? \u2013 Where should we improve? - Open question 5.Length of teaching today was - Adequate - Too short - Too long 6.Depth of content was - Adequate - Too superficial - Too profound 7.The pace of teaching was - Adequate - Too slow - Too fast 8.Teaching aids used (e.g. slides) were well prepared - Agree completely - Agree - No strong feelings - Disagree - Disagree completely 9.Hands-on exercises and demonstrations were - Adequate - Too few - Too many 10.Hands-on exercises and demonstrations were well prepared - Agree completely - Agree - No strong feelings - Disagree - Disagree completely 11.How would you rate the separate sessions? - Introduction - Load and run - Packages - Isolated environments - Python in batch - Interactive work - Jupyter - Parallel - Conda With answers - Poor - Fair - Good - Very good - Excellent - Did not attend Give you confidence levels of the following statements, using this scale: - 0: I don't know even what this is about ...? - 1: I have no confidence I can do this - 2: I have low confidence I can do this - 3: I have some confidence I can do this - 4: I have good confidence I can do this - 5: I absolutely can do this! Give you confidence levels of the following statements below: - I can use the module system to load a specific version of Python - I can run Python - I can use the Python interpreter - I can run IPython - I can use the IPython interpreter - I can run a Python script - I can determine the version of a Python package - I can determine that a Python package is not installed - I can load a Python package module - I can install a Python package using ``pip`` - I can work (create, activate, work, deactivate) with a ``venv`` virtual environment - I can write a bash script - I can submit a script to the job scheduler - I can write a bash script that uses GPUs - I can start an interactive session - I can check that I am in an interactive session - I can start an interactive session with multiple cores - I can check that I am in an interactive session with multiple cores - I can start Jupyter - I can start a script that uses parallel code - I can measure the effect of using more nodes for parallel code 13.Did today's course meet your expectation? - Yes - No - Not sure 14.Which future training topics would you like to be provided by the training host(s)? - Open question 15.Do you have any additional comments? - Open question Only question 12 (the confidences) and 15 (any other comments) are the ones that matter to me. Question 11 is useful to me, but needlessly personal, hurtful and I don\u2019t care about satisfaction. [ ] In meeting with teachers, discuss the evaluation form","title":"Reflection before teaching"},{"location":"reflections/20241022_richel/#reflection-after-teaching","text":"9:18: Around 12 learners have visited. After telling each of them that they can go if (1) they have logged in and (2) can use a text editor, around 3 hang around with their cameras off. I assume they are waiting for the lesson to start. [x] Make this more explicit in the Python and R schedule The course started at 10:01. Due to technical issues, I took over the first sessions without any preparations. I improvised in following the Mike Bell teaching model with Prior, Present, Challenge and Feedback and I think that went well enough. During this improvised session, around a third of the learners had their camera on, as per my request. I felt there was enough time to work on exercises without disturbance. When the technical issues were over, at 10:35, my colleague took over the teacher role again. In that improvised sessions, I did not talk much about IPython, as I feel it is useless and time should be spent differently, hence I predict low confidence for IPython. Additionally, a colleague helped me remind about a shared document where learners can ask questions. Also here, I spent less time on this shared document, as I think it is not worth my time to talk about a shared document I do not care about. I am happy how we, teachers, dealt with the technical problem. There was a learner that found a mistake in my teaching material, about a mismatch in versions of matplotlib. I already fixed it :-) My prepared part started at the 11:05 sharp (as per schedule). Here, a fourth of the learners had their camera on. I felt there was enough time to work on exercises without disturbance. The durations of the sessions was long enough. When polling if learners had completed the exercise by using Zoom Reactions, two thirds gave a \u2018yes\u2019 as a reaction, the rest (all without camera) having a \u2018no\u2019 as a reaction. One thing I did wrong, was a question from a LUNARC. She wanted to know if pip list --user required an additional argument, such as a username. Instead of asking for clarification first, I rushed to give an answer, that was different from her question. My bad! One possible cause for this is that I feel insecure about pip list --user : I\u2019ve never used it and I don\u2019t care, but it was in the course material. I should remove this. [x] Remove pip list --user from the course material After the break, there was a simultaneous session that I was allowed to teach, for UPPMAX users only. There 6 out of the 7 learners had their camera on. Here, all learners with a camera having achieved the teaching goal of starting Jupyter. 1 out of 7 got stuck and I lead her to also start Jupyter. I felt there was enough time to work on the exercise without disturbance. The durations of the session was long enough. I did not prepare for this session at all, as the material was good enough: I\u2019d feel at home with an UPPMAX-only group anyways and am confident I can improvise there. I did not use the course material at all, instead send the learners to the documentation page with the procedure. I think that that documentation page can be improved, by rewriting an existing page. [x] Improve https://docs.uppmax.uu.se/software/jupyter/ similar to https://docs.uppmax.uu.se/software/jupyter_on_bianca/ [x] Add video What I noticed in the simultaneous session, upon questioning the learners, that 6 out of 6 learners did not know what a login node is. From that I infer that this course is the first formal introduction to our clusters for most of our learners. For the summary and evaluation, I used question 12 of the evaluation form. I think that was good enough. I did not use a breakout room, as we did not practice that enough today. In more general terms: I am happy to have had questions asked by learners during sessions. It hints that some think I am approachable. I am happy that all my exercises have answers and a video: the Feedback is there. Also, Feedback consisted out of two demos by learners and one one-on-one demo by me. The answers I provide, however, can be improved, by adding more \u2018How does that look like?\u2019 dropdowns. I did not do it this time, as I struggled with the RST markup too much All my exercise sessions were 15 minutes. This was a good length. It was a lot of waiting and I was able to be silent too I am happy that my exercises use wget instead of a tarball. The word \u2018tarball\u2019 seemed already new for the learners, judging by their facial expression I am reasonably happy with the amount of interaction I got. I spoke with all learners that had their camera on. There were too many cameras off for my taste, maybe the recording causes this? Suggests for next course: The prerequirements should only be pre-requirements, not things that are \u2018nice to have\u2019. I suggest to remove the whole \u2018Get familiar with the Linux/Bash command line\u2019 session The login sessions should only be about login. Stating what the goal of this optional session is, would be enough. I suggest to remove anything but that. Remove IPython from session on running Python The evaluation form will be useless: in a course with 4 teachers, rating the course as a whole gives little information that is useful to a teacher. I don\u2019t care about the satisfaction of learners either: all I care about is if they have learned something, i.e. if they are confident in a topic I taught. I am interested in their free comments too. Besides that, the evaluation form feels like an administrative monster","title":"Reflection after teaching"},{"location":"reflections/20241022_richel/#summary-of-evaluation-results","text":"There are 5 evaluation results, out of the 9 learners that stayed in the end. 1. Overall, how would you rate today's training event? - 7.4. Average Number This question gives me no useful info. 2. Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? - \"the best parts were how to find correct modules, submit jobs, and the examples for parallel codes \" - \"materials\" This question gives me no useful info. 3. Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? - \"the lecturers shouldn't say the person's names and point to specific persons to reply questions\" - \"Going through the examples more throughly.\" I will definitely keep pointing to learners, as per the literature. When I told the class I would do so, I told them to turn off the cameras if they\u2019d be uncomfortable with me doing so. I don\u2019t know why this learner kept his/her camera on if he/she is uncomfortable with this. I did explain why I think this is important (to get to know what all learners think, not just the ones that are more vocal) and I stressed multiple times that \u2018I don\u2019t know\u2019 is a fine answer too. In other courses, where I take the time to show my teaching style, this feedback is not found in the evaluations. Next time, I will show the references to the literature again. [ ] Next iteration: take the time to discuss my teaching style 4. Training event organisation (e.g. announcement, registration, ...): \u2013 What did you like best? \u2013 Where should we improve? - \"material online is excellent\" - \"well organized\" No info. 5. Length of teaching today was Adequate 3 Too short 1 Too long 1 No info. 6. Depth of content was Adequate 5 Too superficial 0 Too profound 0 No info. 7. The pace of teaching was Adequate 3 Too slow 0 Too fast 2 No info: which session? 8. Teaching aids used (e.g. slides) were well prepared Agree completely 0 Agree 4 No strong feelings 0 Disagree 1 Disagree completely 0 No info: which session? 9. Hands-on exercises and demonstrations were Adequate 3 Too few 2 Too many 0 No info: which session? 10. Hands-on exercises and demonstrations were well prepared Agree completely 3 Agree 2 No strong feelings 0 Disagree 0 Disagree completely 0 No info: which session? 11. How would you rate the separate sessions? I am happy to see that the \u2018Excellents\u2019 are all in sections that I taught (\u2018Packages\u2019, \u2018Isolated environments\u2019) or that I may have taught (i.e. these we discussed in the simultaneous session) (\u2018Interactive work\u2019, \u2018Jupyter\u2019) or that I partially taught (\u2018Load and run\u2019). The lowest rating (\u2018poor\u2019) is in none of my sessions. I do see a low \u2018fair\u2019 rating in my sessions, except for Jupyter, where there are more teachers. My theory is that the person that did not like to be addressed by name followed Jupyter in the HPC2N or LUNARC session and gave a \u2018fair\u2019 to the other sessions. 12. Give your confidence levels of the following statements I am happy to see that the IPython question is not there. Seems like I am not the only one that thinks it can be removed from the course material? The confidences that were lowest of the part I am one of the teachers are \u2018I can start an interactive session with multiple cores\u2019 and \u2018I can check that I am in an interactive session with multiple cores\u2019. I for sure did not teach this, as I felt there are more important things to do. Also: I think other things are more important. The sessions that I do care about .. I can determine the version of a Python package I can determine that a Python package is/is not installed I can load a Python (machine learning) module I can install a Python package I can work (create, activate, work, deactivate) with a venv virtual environment \u2026 did reasonably well, with 1 learner stating \u2018I have some confidence in this\u2019. I wish I knew what was the problem with that one learner. A weird response is \u2018I can export and import a virtual environment\u2019, which had same confidences as the other session, although I did not teach it! I hoped that the learners would all state \u2018I have no idea what this is about\u2019. Maybe some carry-over effect? [x] Remove question \u2018I can export and import a virtual environment\u2019 from evaluation 13.Did today's course meet your expectation? Yes 3 No 0 Not sure 2 Useless info to me. 14. Which future training topics would you like to be provided by the training host(s)? \"- xarray and dask\" Useless info to me. 15. Do you have any additional comments? - A sheet summarizing the most important parts with key code snippets to do the items in 12)\" Fun! This learner suggests that I actually should have worked on a proper summary page, with -indeed- the key points. Will do so next time! Thanks! [ ] Base summary on evaluation form","title":"Summary of evaluation results"},{"location":"reflections/20241022_richel/#analyzing-the-amount-of-learners","text":"Analyzing the amount of learners present during the day (data: 20241022_counts.csv ) \u2026 I see that there have been 19 learners max (at around 10:35), where there were 10 at the start of lunch. This happened even though these sessions are evaluated positively. Maybe this hints that we cannot convince the learners why these things are important, but that is just a guess\u2026","title":"Analyzing the amount of learners"},{"location":"reflections/20241024_richel/","text":"Reflection \u00b6 Teaching day: 2024-10-24 Topic: R Written on 2024-10-24 Schedule \u00b6 This is the schedule of that day: Time Topic Teacher(s) 9:00 (optional) First login BB + PO + RB 9:45 Break . 10:00 Introduction RB 10:10 Syllabus RB 10:20 Load modules and run RB 10:45 Break . 11:00 Packages BB 11:30 Isolated environments BB 12:00 Lunch . 13:00 Batch BB 13:30 Parallel PO 14:15 Break . 14:30 Simultaneous session PO * RB * RP 15:15 Break . 15:30 Machine learning BB or PO 16:00 Summary and evaluation RB 16:15 Done . Login session \u00b6 4 learners showed up for this optional session: 3x could already login 1x a LUNARC user that needed help, I put him in a Zoom room with the LUNARC teacher Session: 10:00-10:45: intro and syllabus and load and run R \u00b6 There were 5 learners. I did the regular teaching cycles, with a Prior at the start. There were 3 that -per my request- turned on their camera, with 2 left after the announcement that the session will be recorded. At 10:21 I started the exercise and at 10:40 I discussed it. At 10:40, upon asking for a yes/no reaction, 5 out of 5 learners signalled they had been able to do the exercise. I rounded of the session by going over the key points again. As already was in the lesson plan, I don\u2019t like this session. I do think many people think this stock-standard way of teaching is fine, but I\u2019d enjoy better to have the learners learn from each center\u2019s documentation, as this will be more helpful. Instead, now, the session is more a copy-paste exercise, except for the Prior and Repeat. I can imagine learners like it (we know learners like passive teaching), but they could actually have learned more. [ ] In the next course iteration: use more active learning methods in this session Session 14:30-15:15: interactive and RStudio \u00b6 It was delayed 15 minutes, which was no problem. Prior took 15 minutes, in which I recapped earlier sessions too. I gave the learners 15 minutes for the exercise. All 4 learners were able to complete the procedure. During the 15 minutes, there was 1 learner that shared her screen with a question beyond the content of this session. 1 learner needed to install a local ThinLinc client and this did not work. That learner did see how it looked like and stated she is confident she could do it via the website. As the learning objectives were achieved, I ended the session earlier (there were no more questions). I do think the session\u2019s length is correct, in the case there would be some slower learners. I told the 4 learners when the next session would start, which was 15 minutes later on the schedule, as I expected other sessions to do take the scheduled amount of time. However, it turned out that the next session would like to start earlier. I took the blame and hoped for the best. What I did not know is that it would be only the UPPMAX learners left, so we could have easily started 15 minutes earlier. Session 16:00-16:15: summary and evaluation \u00b6 I\u2019ve not shown the summary, although it follows the evaluation questions, as I feel it increases the evaluation results, as the confidences will be higher, due to the refresher. The full procedure took 3 minutes. It took 5 minutes for the 5 learners to fill in the evaluation forms. All learners said bye, except for 1. That 1 learner had a question, which was answered. I discussed this story with a colleague afterwards. He will try out a combination of showing summary yes/no, answers in summary yes/no and report to me. Analysis \u00b6 20241024_counts.csv Centers of R learners Languages of R learners Number of learners per language Learners in time Evaluation results \u00b6 5 responses 1. Overall, how would you rate today's training event? - 7.4. Average Number Useless info to me. 2. Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? - \"I liked being able to run through the examples myself. \" - \"Pedro's sessions\" Useless to me, nice for Pedro! 3. Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? - \"I think sometimes there could have been slightly less kinds of examples, and one main example per module with extra focus and more time to complete. \" I think I focussed well on one example and did have enough time for exercises. 4. Training event organisation (e.g. announcement, registration, ...): \u2013 What did you like best? \u2013 Where should we improve? - \"I liked the general organization, felt like any questions one might have where answered in the introductory email.\" Nice for Birgitte! 5. Length of teaching today was Adequate 4 Too short 0 Too long 1 Useless to me: which sessions? 6. Depth of content was Adequate 4 Too superficial 1 Too profound 0 Useless to me: which sessions? 7. The pace of teaching was Adequate 3 Too slow 0 Too fast 2 Useless to me: which sessions? 8. Teaching aids used (e.g. slides) were well prepared Agree completely 2 Agree 3 No strong feelings 0 Disagree 0 Disagree completely 0 Useless to me: which sessions? 9. Hands-on exercises and demonstrations were Adequate 3 Too few 2 Too many 0 Useless to me: which sessions? 10. Hands-on exercises and demonstrations were well prepared Agree completely 2 Agree 3 No strong feelings 0 Disagree 0 Disagree completely 0 Useless to me: which sessions? 11. How would you rate the separate sessions? Only my sessions: Introduction: 2x very good, 2x excellent Load and run: 4x excellent Interactive work on the compute node: 1x very good, 4x excellent Using RStudio: 2x very good, 2x excellent So, although I dislike my \u2018Load and run\u2019 session, it has been appreciated by the learners. 12. Give your confidence levels of the following statements My learning objectives: I can find the module to be able to run R: 1x some, 1x good, 3x absolutely I can load the module to be able to run R: 1 some, 1 good, 3 absolutely I can run the R interpreter: 1 some, 1 good, 3 absolutely I can run the R command to get the list of installed R packages: 1 some, 1 good, 3 absolutely I can run an R script from the command-line: 1 some, 2 good, 2 absolutely I can find out if an R package is already installed: 1 some, 1 good, 3 absolutely I can start an interactive session: 1 low, 1 some, 1 good, 2 absolutely I can verify I am on the login node yes/no: 2 some, 2 good, 1 absolutely I can start an interactive session with multiple cores: 2 some, 1 good, 2 absolutely I can start RStudio: 1 some, 1 good, 3 absolutely It seems my simultaneous session was weakest: they did feel confident to start RStudio, but starting an interactive node and verify on which node(s) was indeed done a bit too informal in hindsight. It is weird that a learner has a low confidence in starting any interactive session, but some confidence when it is multiple cores\u2026? To do next time: [ ] In the simultaneous session, stay formal 13.Did today's course meet your expectation? Yes 3 No 0 Not sure 2 Useless info. 14. Which future training topics would you like to be provided by the training host(s)? - 2 responses No idea what those are 15. Do you have any additional comments? - 1 response No idea what those are","title":"Reflection"},{"location":"reflections/20241024_richel/#reflection","text":"Teaching day: 2024-10-24 Topic: R Written on 2024-10-24","title":"Reflection"},{"location":"reflections/20241024_richel/#schedule","text":"This is the schedule of that day: Time Topic Teacher(s) 9:00 (optional) First login BB + PO + RB 9:45 Break . 10:00 Introduction RB 10:10 Syllabus RB 10:20 Load modules and run RB 10:45 Break . 11:00 Packages BB 11:30 Isolated environments BB 12:00 Lunch . 13:00 Batch BB 13:30 Parallel PO 14:15 Break . 14:30 Simultaneous session PO * RB * RP 15:15 Break . 15:30 Machine learning BB or PO 16:00 Summary and evaluation RB 16:15 Done .","title":"Schedule"},{"location":"reflections/20241024_richel/#login-session","text":"4 learners showed up for this optional session: 3x could already login 1x a LUNARC user that needed help, I put him in a Zoom room with the LUNARC teacher","title":"Login session"},{"location":"reflections/20241024_richel/#session-1000-1045-intro-and-syllabus-and-load-and-run-r","text":"There were 5 learners. I did the regular teaching cycles, with a Prior at the start. There were 3 that -per my request- turned on their camera, with 2 left after the announcement that the session will be recorded. At 10:21 I started the exercise and at 10:40 I discussed it. At 10:40, upon asking for a yes/no reaction, 5 out of 5 learners signalled they had been able to do the exercise. I rounded of the session by going over the key points again. As already was in the lesson plan, I don\u2019t like this session. I do think many people think this stock-standard way of teaching is fine, but I\u2019d enjoy better to have the learners learn from each center\u2019s documentation, as this will be more helpful. Instead, now, the session is more a copy-paste exercise, except for the Prior and Repeat. I can imagine learners like it (we know learners like passive teaching), but they could actually have learned more. [ ] In the next course iteration: use more active learning methods in this session","title":"Session: 10:00-10:45: intro and syllabus and load and run R"},{"location":"reflections/20241024_richel/#session-1430-1515-interactive-and-rstudio","text":"It was delayed 15 minutes, which was no problem. Prior took 15 minutes, in which I recapped earlier sessions too. I gave the learners 15 minutes for the exercise. All 4 learners were able to complete the procedure. During the 15 minutes, there was 1 learner that shared her screen with a question beyond the content of this session. 1 learner needed to install a local ThinLinc client and this did not work. That learner did see how it looked like and stated she is confident she could do it via the website. As the learning objectives were achieved, I ended the session earlier (there were no more questions). I do think the session\u2019s length is correct, in the case there would be some slower learners. I told the 4 learners when the next session would start, which was 15 minutes later on the schedule, as I expected other sessions to do take the scheduled amount of time. However, it turned out that the next session would like to start earlier. I took the blame and hoped for the best. What I did not know is that it would be only the UPPMAX learners left, so we could have easily started 15 minutes earlier.","title":"Session 14:30-15:15: interactive and RStudio"},{"location":"reflections/20241024_richel/#session-1600-1615-summary-and-evaluation","text":"I\u2019ve not shown the summary, although it follows the evaluation questions, as I feel it increases the evaluation results, as the confidences will be higher, due to the refresher. The full procedure took 3 minutes. It took 5 minutes for the 5 learners to fill in the evaluation forms. All learners said bye, except for 1. That 1 learner had a question, which was answered. I discussed this story with a colleague afterwards. He will try out a combination of showing summary yes/no, answers in summary yes/no and report to me.","title":"Session 16:00-16:15: summary and evaluation"},{"location":"reflections/20241024_richel/#analysis","text":"20241024_counts.csv Centers of R learners Languages of R learners Number of learners per language Learners in time","title":"Analysis"},{"location":"reflections/20241024_richel/#evaluation-results","text":"5 responses 1. Overall, how would you rate today's training event? - 7.4. Average Number Useless info to me. 2. Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 What did you like best? - \"I liked being able to run through the examples myself. \" - \"Pedro's sessions\" Useless to me, nice for Pedro! 3. Today's content and feedback to the lecturers (e.g. materials, exercises, structure): \u2013 Where should we improve? - \"I think sometimes there could have been slightly less kinds of examples, and one main example per module with extra focus and more time to complete. \" I think I focussed well on one example and did have enough time for exercises. 4. Training event organisation (e.g. announcement, registration, ...): \u2013 What did you like best? \u2013 Where should we improve? - \"I liked the general organization, felt like any questions one might have where answered in the introductory email.\" Nice for Birgitte! 5. Length of teaching today was Adequate 4 Too short 0 Too long 1 Useless to me: which sessions? 6. Depth of content was Adequate 4 Too superficial 1 Too profound 0 Useless to me: which sessions? 7. The pace of teaching was Adequate 3 Too slow 0 Too fast 2 Useless to me: which sessions? 8. Teaching aids used (e.g. slides) were well prepared Agree completely 2 Agree 3 No strong feelings 0 Disagree 0 Disagree completely 0 Useless to me: which sessions? 9. Hands-on exercises and demonstrations were Adequate 3 Too few 2 Too many 0 Useless to me: which sessions? 10. Hands-on exercises and demonstrations were well prepared Agree completely 2 Agree 3 No strong feelings 0 Disagree 0 Disagree completely 0 Useless to me: which sessions? 11. How would you rate the separate sessions? Only my sessions: Introduction: 2x very good, 2x excellent Load and run: 4x excellent Interactive work on the compute node: 1x very good, 4x excellent Using RStudio: 2x very good, 2x excellent So, although I dislike my \u2018Load and run\u2019 session, it has been appreciated by the learners. 12. Give your confidence levels of the following statements My learning objectives: I can find the module to be able to run R: 1x some, 1x good, 3x absolutely I can load the module to be able to run R: 1 some, 1 good, 3 absolutely I can run the R interpreter: 1 some, 1 good, 3 absolutely I can run the R command to get the list of installed R packages: 1 some, 1 good, 3 absolutely I can run an R script from the command-line: 1 some, 2 good, 2 absolutely I can find out if an R package is already installed: 1 some, 1 good, 3 absolutely I can start an interactive session: 1 low, 1 some, 1 good, 2 absolutely I can verify I am on the login node yes/no: 2 some, 2 good, 1 absolutely I can start an interactive session with multiple cores: 2 some, 1 good, 2 absolutely I can start RStudio: 1 some, 1 good, 3 absolutely It seems my simultaneous session was weakest: they did feel confident to start RStudio, but starting an interactive node and verify on which node(s) was indeed done a bit too informal in hindsight. It is weird that a learner has a low confidence in starting any interactive session, but some confidence when it is multiple cores\u2026? To do next time: [ ] In the simultaneous session, stay formal 13.Did today's course meet your expectation? Yes 3 No 0 Not sure 2 Useless info. 14. Which future training topics would you like to be provided by the training host(s)? - 2 responses No idea what those are 15. Do you have any additional comments? - 1 response No idea what those are","title":"Evaluation results"},{"location":"reflections/20241025_richel/","text":"Reflection \u00b6 Teaching day: - Topic: entire course Written on 2024-10-25 Registrations over the centers Registrations per language Number of registrations per language Learners in time for the Python day Learners in time for the R day Email \u00b6 Below is the email sent to participants, with some info removed. I think it is too long TODO: [ ] Make shorter, suggest at meeting [ ] Decide upon first hour with the team, put the optional hour -if needed- in there. > Hello! > > This email contains information for the UPPMAX/HPC2N/LUNARC four days online course \u201cIntroduction to running R, Python, Julia, and Matlab in HPC\u201d, which will be given Tuesday-Friday, 22-25 October, 2024, 9:00-16:00 each day. > > NOTE that the course starts at 9:00 not 9:15! > > We will use Zoom for this online course. See below (#11) for links. > > NOTE please go through everything, as there is important information about pre-requirements, how to login, etc. Also be aware that you may need to set up MFA for UPPMAX, and Pocket Pass for LUNARC, which can take a day to get setup, so do this immediately. > > NOTE onboarding for those that need help with login, Monday 21 October 13-14. Zoom (only for onboarding): https://umu.zoom.us/j/62612416748?pwd=FX3dgIDLfyAkYa9drDQiLO6bTxNfAy.1 > > 1) Schedule: > > Python (Tuesday): https://uppmax.github.io/R-matlab-julia-HPC/python/intro.html#schedule > Julia (Wednesday): https://uppmax.github.io/R-matlab-julia-HPC/julia/introJulia.html#preliminary-schedule > R (Thursday): https://uppmax.github.io/R-matlab-julia-HPC/r/introR.html#schedule > Matlab (Friday): https://uppmax.github.io/R-matlab-julia-HPC/matlab/introMatlab.html#preliminary-schedule > > 2) Materials: https://github.com/UPPMAX/R-matlab-julia-HPC I don't think learners care about the raw course material. I suggest to remove point 2. > 3) Rendered presentations for the course: https://uppmax.github.io/R-matlab-julia-HPC/ > > 4) Important information about the course is gathered here: https://umeauniversity.sharepoint.com/:w:/s/HPC2N630/EQNh3Uht3ExCnyqVUm1t_bEB1e7FOp_pqxtFix3x2UPXtQ Suggest to use the course website instead and remove point 4. > 5) Questions and answers page for the course: https://umeauniversity.sharepoint.com/:w:/s/HPC2N630/Ebo1tbwHnz5GhBq7p04ex6sB9or9WlqWHHWIlcUTW230mA Suggest to use the course website instead and remove point 5. > 6) Please make sure you have an account at SUPR (https://supr.naiss.se/) and at UPPMAX (or HPC2N if you are affiliated with UMU, SLU, IRF, LTU, or MIUN, or LUNARC if you are affiliated with LU). The site account for UPPMAX/HPC2N/LUNARC can be applied for through SUPR after you have your SUPR account and have been added to the course project. The account at SUPR and the accounts at HPC2N/UPPMAX/LUNARC are all SEPARATE. You should have already been contacted about getting such accounts if you did not already have them. > > 7) Please make sure you have installed either ThinLinc or an SSH client. ThinLinc is strongly RECOMMENDED for this course and can be installed from https://www.cendio.com/thinlinc/download > > Login info > UPPMAX \u2013 Rackham (the course project is at UPPMAX) > SSH: rackham.uppmax.uu.se > ThinLinc: rackham-gui.uppmax.uu.se > From web browser: https://rackham-gui.uppmax.uu.se/ > HPC2N - Kebnekaise > SSH: kebnekaise.hpc2n.umu.se > ThinLinc: kebnekaise-tl.hpc2n.umu.se > From web browser: https://kebnekaise-tl.hpc2n.umu.se:300/ > LUNARC \u2013 Cosmos > SSH: cosmos.lunarc.lu.se > ThinLinc: cosmos-dt.lunarc.lu.se Suggest to use the course website instead and remove point 7. Only recommenend to be able to log in in at least 1 way. > Note that if you are using ThinLinc, you will connect to a graphical interface and do not need a separate X11 server to enable opening graphics and GUIs. Suggest to use the course website instead and remove this point. > Note that if you are using Windows and do not already have an SSH client you use, we strongly recommend using ThinLinc. Suggest to use the course website instead and remove this point. > 8) Pre-requirements: This page contains some info on getting ready to login (including the above info), but also a short summary/list of links for things like Linux, editors, and coding: https://uppmax.github.io/R-matlab-julia-HPC/prereqs.html > 9) Onboarding: If you need help with logging in, ThinLinc, etc. then we have an optional onboarding session Monday, 21 October 2024, 13:00-14:00. Zoom for onboarding: https://umu.zoom.us/j/62612416748?pwd=FX3dgIDLfyAkYa9drDQiLO6bTxNfAy.1 > 10) Course project and policy: As part of the hands-on, you will be given temporary access to a course project, which will be used for running the hands-on examples. There are some policies regarding this, that we ask that you follow: > > You may be given access to the project before the course; please do not use the allocation for running your own codes. Usage of the project before the course means the priority of jobs submitted to it goes down, diminishing the opportunity for you and your fellow participants to run the examples during the course. > The course project will be open 1-2 weeks after the course, giving the participants the opportunity to test run examples and shorter codes related to the course. During this time, we ask that you only use it for running course related jobs. Use your own discretion, but it could be: (modified) examples from the hands-on, short personal codes that have been modified to test things learned at the course, etc. > Anyone found to be misusing the course project, using up large amounts of the allocation for their own production runs, will be removed from the course project. > When the course is no longer active, all files in the attached storage directory will be deleted. Please copy out anything you want to save before that. > Project ID, UPPMAX: [project_id] > Project ID, HPC2N: [project_id] > Project ID, LUNARC: [project_id] > Directory name on rackham: /proj/r-py-jl-m-rackham > Please create a suitably named subdirectory below /proj/r-py-jl-m-rackham, for your own exercises. > Directory name on Kebnekaise: /proj/nobackup/r-py-jl-m > Please create a suitably named subdirectory below /proj/nobackup/[project_id], for your own exercises. > Cosmos: Since the Cosmos home directories are quite a lot larger than on Rackham or Kebnekaise, you will be using your home directory there for the exercises. Please create a suitably named subdirectory there. Suggest to use the course website instead. The course website should show how to make sure you have gotten SUPR access > 11) Zoom info for the course: > > a) There will be a zoom for the lectures. It is the same zoom link for each day. > > b) When you join the Zoom meeting, use your REAL NAME. > > c) Please MUTE your microphone when you are not speaking and use the \u201cRaise hand\u201d functionality under the \u201cParticipants\u201d window during the lecture. Please do not clutter the Zoom chat. Behave politely! Suggest to remove this point. Teachers can talk too. > > d) If you have questions during the lectures, you can write them on this page: https://umeauniversity.sharepoint.com/:w:/s/HPC2N630/Ebo1tbwHnz5GhBq7p04ex6sB9or9WlqWHHWIlcUTW230mA Suggest to remove this point. Teachers can state how they want to have questions asked. Some want to be interrupted. > e) There may be breakout rooms used in the Zoom for the hands-ons and for discussions. Most likely there will be a \u201csilent\u201d room for those who just wish to work on their own, a discussion room (or rooms) for cooperation, and the rest will be in the main room. If you are in the silent room and need help, go to the main room and contact a helper. There will be breakout rooms that can be used if you want individual help. Suggest to remove this point, as I see no need to write down our Zoom room policies here. > f) The lectures and demos will be recorded, but not the hands-ons sessions. If you ask questions during the lectures, you may thus be recorded. If you do not wish to be recorded, then please keep your microphone muted and your camera off and write your questions in the Q/A document. > ZOOM (same each day): > > > Zoom: [URL] > Meeting ID: [ID] > Passcode: [passcode] > >See you Tuesday!","title":"Reflection"},{"location":"reflections/20241025_richel/#reflection","text":"Teaching day: - Topic: entire course Written on 2024-10-25 Registrations over the centers Registrations per language Number of registrations per language Learners in time for the Python day Learners in time for the R day","title":"Reflection"},{"location":"reflections/20241025_richel/#email","text":"Below is the email sent to participants, with some info removed. I think it is too long TODO: [ ] Make shorter, suggest at meeting [ ] Decide upon first hour with the team, put the optional hour -if needed- in there. > Hello! > > This email contains information for the UPPMAX/HPC2N/LUNARC four days online course \u201cIntroduction to running R, Python, Julia, and Matlab in HPC\u201d, which will be given Tuesday-Friday, 22-25 October, 2024, 9:00-16:00 each day. > > NOTE that the course starts at 9:00 not 9:15! > > We will use Zoom for this online course. See below (#11) for links. > > NOTE please go through everything, as there is important information about pre-requirements, how to login, etc. Also be aware that you may need to set up MFA for UPPMAX, and Pocket Pass for LUNARC, which can take a day to get setup, so do this immediately. > > NOTE onboarding for those that need help with login, Monday 21 October 13-14. Zoom (only for onboarding): https://umu.zoom.us/j/62612416748?pwd=FX3dgIDLfyAkYa9drDQiLO6bTxNfAy.1 > > 1) Schedule: > > Python (Tuesday): https://uppmax.github.io/R-matlab-julia-HPC/python/intro.html#schedule > Julia (Wednesday): https://uppmax.github.io/R-matlab-julia-HPC/julia/introJulia.html#preliminary-schedule > R (Thursday): https://uppmax.github.io/R-matlab-julia-HPC/r/introR.html#schedule > Matlab (Friday): https://uppmax.github.io/R-matlab-julia-HPC/matlab/introMatlab.html#preliminary-schedule > > 2) Materials: https://github.com/UPPMAX/R-matlab-julia-HPC I don't think learners care about the raw course material. I suggest to remove point 2. > 3) Rendered presentations for the course: https://uppmax.github.io/R-matlab-julia-HPC/ > > 4) Important information about the course is gathered here: https://umeauniversity.sharepoint.com/:w:/s/HPC2N630/EQNh3Uht3ExCnyqVUm1t_bEB1e7FOp_pqxtFix3x2UPXtQ Suggest to use the course website instead and remove point 4. > 5) Questions and answers page for the course: https://umeauniversity.sharepoint.com/:w:/s/HPC2N630/Ebo1tbwHnz5GhBq7p04ex6sB9or9WlqWHHWIlcUTW230mA Suggest to use the course website instead and remove point 5. > 6) Please make sure you have an account at SUPR (https://supr.naiss.se/) and at UPPMAX (or HPC2N if you are affiliated with UMU, SLU, IRF, LTU, or MIUN, or LUNARC if you are affiliated with LU). The site account for UPPMAX/HPC2N/LUNARC can be applied for through SUPR after you have your SUPR account and have been added to the course project. The account at SUPR and the accounts at HPC2N/UPPMAX/LUNARC are all SEPARATE. You should have already been contacted about getting such accounts if you did not already have them. > > 7) Please make sure you have installed either ThinLinc or an SSH client. ThinLinc is strongly RECOMMENDED for this course and can be installed from https://www.cendio.com/thinlinc/download > > Login info > UPPMAX \u2013 Rackham (the course project is at UPPMAX) > SSH: rackham.uppmax.uu.se > ThinLinc: rackham-gui.uppmax.uu.se > From web browser: https://rackham-gui.uppmax.uu.se/ > HPC2N - Kebnekaise > SSH: kebnekaise.hpc2n.umu.se > ThinLinc: kebnekaise-tl.hpc2n.umu.se > From web browser: https://kebnekaise-tl.hpc2n.umu.se:300/ > LUNARC \u2013 Cosmos > SSH: cosmos.lunarc.lu.se > ThinLinc: cosmos-dt.lunarc.lu.se Suggest to use the course website instead and remove point 7. Only recommenend to be able to log in in at least 1 way. > Note that if you are using ThinLinc, you will connect to a graphical interface and do not need a separate X11 server to enable opening graphics and GUIs. Suggest to use the course website instead and remove this point. > Note that if you are using Windows and do not already have an SSH client you use, we strongly recommend using ThinLinc. Suggest to use the course website instead and remove this point. > 8) Pre-requirements: This page contains some info on getting ready to login (including the above info), but also a short summary/list of links for things like Linux, editors, and coding: https://uppmax.github.io/R-matlab-julia-HPC/prereqs.html > 9) Onboarding: If you need help with logging in, ThinLinc, etc. then we have an optional onboarding session Monday, 21 October 2024, 13:00-14:00. Zoom for onboarding: https://umu.zoom.us/j/62612416748?pwd=FX3dgIDLfyAkYa9drDQiLO6bTxNfAy.1 > 10) Course project and policy: As part of the hands-on, you will be given temporary access to a course project, which will be used for running the hands-on examples. There are some policies regarding this, that we ask that you follow: > > You may be given access to the project before the course; please do not use the allocation for running your own codes. Usage of the project before the course means the priority of jobs submitted to it goes down, diminishing the opportunity for you and your fellow participants to run the examples during the course. > The course project will be open 1-2 weeks after the course, giving the participants the opportunity to test run examples and shorter codes related to the course. During this time, we ask that you only use it for running course related jobs. Use your own discretion, but it could be: (modified) examples from the hands-on, short personal codes that have been modified to test things learned at the course, etc. > Anyone found to be misusing the course project, using up large amounts of the allocation for their own production runs, will be removed from the course project. > When the course is no longer active, all files in the attached storage directory will be deleted. Please copy out anything you want to save before that. > Project ID, UPPMAX: [project_id] > Project ID, HPC2N: [project_id] > Project ID, LUNARC: [project_id] > Directory name on rackham: /proj/r-py-jl-m-rackham > Please create a suitably named subdirectory below /proj/r-py-jl-m-rackham, for your own exercises. > Directory name on Kebnekaise: /proj/nobackup/r-py-jl-m > Please create a suitably named subdirectory below /proj/nobackup/[project_id], for your own exercises. > Cosmos: Since the Cosmos home directories are quite a lot larger than on Rackham or Kebnekaise, you will be using your home directory there for the exercises. Please create a suitably named subdirectory there. Suggest to use the course website instead. The course website should show how to make sure you have gotten SUPR access > 11) Zoom info for the course: > > a) There will be a zoom for the lectures. It is the same zoom link for each day. > > b) When you join the Zoom meeting, use your REAL NAME. > > c) Please MUTE your microphone when you are not speaking and use the \u201cRaise hand\u201d functionality under the \u201cParticipants\u201d window during the lecture. Please do not clutter the Zoom chat. Behave politely! Suggest to remove this point. Teachers can talk too. > > d) If you have questions during the lectures, you can write them on this page: https://umeauniversity.sharepoint.com/:w:/s/HPC2N630/Ebo1tbwHnz5GhBq7p04ex6sB9or9WlqWHHWIlcUTW230mA Suggest to remove this point. Teachers can state how they want to have questions asked. Some want to be interrupted. > e) There may be breakout rooms used in the Zoom for the hands-ons and for discussions. Most likely there will be a \u201csilent\u201d room for those who just wish to work on their own, a discussion room (or rooms) for cooperation, and the rest will be in the main room. If you are in the silent room and need help, go to the main room and contact a helper. There will be breakout rooms that can be used if you want individual help. Suggest to remove this point, as I see no need to write down our Zoom room policies here. > f) The lectures and demos will be recorded, but not the hands-ons sessions. If you ask questions during the lectures, you may thus be recorded. If you do not wish to be recorded, then please keep your microphone muted and your camera off and write your questions in the Q/A document. > ZOOM (same each day): > > > Zoom: [URL] > Meeting ID: [ID] > Passcode: [passcode] > >See you Tuesday!","title":"Email"},{"location":"reflections/20250324_richel/","text":"Reflection \u00b6 Author: Richel Date: 2025-03-24 Language: R Lesson plans Evaluation Registrations: 33 Participants: 12 (36% of registrations shows up) With the 1 hour optional login session, we give 1 hour of teaching to the unprepared, while sacrificing 1 hour of teaching for the prepared. I think we should reward the prepared, by not having this drop-in. However, at the drop-in, 9 showed up, of which 5 were put in a breakout room with me. There, I discovered how useful it is. There were 2 out of 5 that could not have logged in without this session. Also, it is a great first impression of our students. +-------+------------------------------+--------------+ | Time | Topic | Teacher ( s ) | + ======= + ============================== + ============== + | 9 :00 | ( optional ) First login | BB + PO + RB | +-------+------------------------------+--------------+ | 9 :45 | Break | . | +-------+------------------------------+--------------+ | 10 :00 | Introduction | RB | +-------+------------------------------+--------------+ | 10 :10 | Syllabus | RB | +-------+------------------------------+--------------+ | 10 :20 | Load modules and run | RB | +-------+------------------------------+--------------+ | 10 :45 | Break | . | +-------+------------------------------+--------------+ | 11 :00 | Packages | BB | +-------+------------------------------+--------------+ | 11 :30 | Isolated environments | BB | +-------+------------------------------+--------------+ | 12 :00 | Lunch | . | +-------+------------------------------+--------------+ | 13 :00 | Batch | BB | +-------+------------------------------+--------------+ | 13 :30 | Parallel | PO | +-------+------------------------------+--------------+ | 14 :15 | Break | . | +-------+------------------------------+--------------+ | 14 :30 | Simultaneous session | . | +-------+------------------------------+--------------+ | . | HPC2N: ThinLinc, RStudio | PO | +-------+------------------------------+--------------+ | . | LUNARC: On-Demand, RStudio | RP | +-------+------------------------------+--------------+ | . | UPPMAX: Interactive, RStudio | RB | +-------+------------------------------+--------------+ | 15 :15 | Break | . | +-------+------------------------------+--------------+ | 15 :30 | Machine learning | PO | +-------+------------------------------+--------------+ | 16 :00 | Summary and evaluation | RB | +-------+------------------------------+--------------+ | 16 :15 | Done | . | +-------+------------------------------+--------------+ I overestimated the number or learners leaving: from my data is estimated 25% of all learners to leave. Instead, before and after the break there were around 12 learners. Recording has a clear effect on the number of learners with camera on/off. Online instruction in higher education: Promising, research-based, and evidence-based practices From: Boettcher, Judith V., and Rita-Marie Conrad. The Online Teaching Survival Guide : Simple and Practical Pedagogical Tips, John Wiley & Sons, Table 3.1: Be present at the course site Create a supportive online course community Develop a set of explicit expectations for your learners and yourself as to how you will communicate and how much time students should be working on the course each week Use a variety of large group, small group, and individual work experiences Use synchronous and asynchronous activities Ask for informal feedback early in the term Prepare discussion posts that invite responses, questions, discussions, and re\ufb02ections Think digital and mobile for all course content Combine core concept learning with customized and personalized learning Plan a good closing and wrap activity for the course Assess as you go by gathering evidences of learning Rigorously connect content to core concepts and learning outcomes Develop and use a content frame for your course Design experiences to help learners make progress on their novice-to-expert journey. From: Nilson, Linda B., and Ludwika A. Goodson. Online teaching at its best: Merging instructional design with teaching and learning research. John Wiley & Sons, 2021. I quote: Students learn new material better and can remember it longer when they learn it by engaging in an activity than when they passively watch or listen to an instructor talk [8 references] [\u2026] long lectures and presentations will fail because students stop viewing and listening after about six minutes. This phenomenon parallels McKeachie\u2019s earlier classroom findings about inattention after five to ten minutes [reference]. In online classes, such student inattention becomes explicitly visible through electronic monitoring of activities and questions from students about what has already been covered in a long presentation. Reasons why learners do not turn on their camera, is Tobi, Bernadette, et al. \u201cA case study on students\u2019 reasons for not switching on their cameras during online class sessions.\u201d Learning 6.41 (2021): 216-224 with results (number of learners = 50): Reason Percentage Fear of insufficient internet data 54% Poor internet connection 50% Physical condition of students\u2019 background/location 46% Physical appearance on camera (not looking good) 46% Uncomfortable to be looked at all the time 42% Or, from Alim, Syahrul, Sirirat Petsangsri, and John Morris. \u201cDoes an activated video camera and class involvement affect academic achievement? An investigation of distance learning students.\u201d Education and Information Technologies 28.5 (2023): 5875-5892. the following reasons for not turning on the camera: Reason Percentage Unready to learn 44% Unstable internet connection and limited quota 37% psychological reasons 11% devices overheating 4% following others 4% \u2018Unready to learn\u2019 means, I quote: \u2018the environment did not support camera activation, they had not taken a bath, were sick, wanted to sleep, were still doing something else, were fatigued or needed to go to the toilet.\u2019. The course is an introduction course. I feel the start-up parts are given too little time, for the things that I do not consider being beginner topics. I feel beginner things need their time, over sacrificing it for more sexy topics. Remove, in order of my preference: Machine learning Parallel Isolated I will suggest this in a meeting: [x] Suggest to remove \u2018Machine learning\u2019 [x] Suggest to remove \u2018Parallel\u2019 [x] Suggest to remove \u2018Isolated environments\u2019 My favorite schedule +-------+------------------------------+--------------+ | Time | Topic | Teacher ( s ) | + ======= + ============================== + ============== + | 9 :00 | ( optional ) First login | BB + PO + RB | +-------+------------------------------+--------------+ | 9 :45 | Break | . | +-------+------------------------------+--------------+ | 10 :00 | Introduction | RB | +-------+------------------------------+--------------+ | 10 :10 | Syllabus | RB | +-------+------------------------------+--------------+ | 10 :20 | Load modules and run | RB | +-------+------------------------------+--------------+ | 11 :00 | Break | . | +-------+------------------------------+--------------+ | 11 :15 | Packages | BB | +-------+------------------------------+--------------+ | 12 :00 | Lunch | . | +-------+------------------------------+--------------+ | 13 :00 | Batch | BB | +-------+------------------------------+--------------+ | 14 :00 | Break | . | +-------+------------------------------+--------------+ | 14 :15 | Simultaneous session | . | +-------+------------------------------+--------------+ | . | HPC2N: ThinLinc, RStudio | PO | +-------+------------------------------+--------------+ | . | LUNARC: On-Demand, RStudio | RP | +-------+------------------------------+--------------+ | . | UPPMAX: Interactive, RStudio | RB | +-------+------------------------------+--------------+ | 15 :00 | Break | . | +-------+------------------------------+--------------+ | 15 :15 | To be decided by vote | ? | +-------+------------------------------+--------------+ | 15 :45 | Summary and evaluation | RB | +-------+------------------------------+--------------+ | 16 :00 | Done | . | +-------+------------------------------+--------------+ I felt rushed during my session, with only 45 minutes. The learners did reach the learning outcomes in time, but I had no time to discuss this with them. Sure, one could argue that the LOs have been achieved, yet, on the other hand, there was no proper Feedback phase. I was a helper during the other sessions. I was not a very good helper: I find it hard to stay focused during monologues. Evaluations \u00b6 Q1: Course satisfaction: 8.25 \u00b6 Useless information to me. Q2: Pace of teaching \u00b6 Useless information to me. Q3 \u00b6 My grades (average is 77): learning_outcome success_score Comment I can find the module to be able to run R 94 Mine, great I can load the module to be able to run R 94 Mine, great I can run the R interpreter 94 Mine, great I can run the R command to get the list of installed R packages 88 Mine, great I can run an R script from the command-line 88 Mine, great I can find out if an R package is already installed 81 Mine, was extra I can load the pre-installed R packages 88 Mine, great I can install an R package from CRAN 81 Not mine I can use renv to create, activate, use and deactivate a virtual environment 62 Not mine I can submit a job to the scheduler to run an R script with regular code 78 Not mine I can submit a job to the scheduler to run an R script that uses parallel code 69 Not mine I can submit a job to the scheduler to run an R script that uses a GPU 56 Not mine I can find and load the R machine learning modules 50 Not mine I can submit a job to the scheduler to run an R script that uses machine learning 50 Not mine I can start an interactive session 81 Simultaneous session I can verify I am on the login node yes/no 78 Mine, not taught explicitly I can start an interactive session with multiple cores 78 Mine, not taught explicitly I can start RStudio 84 Simultaneous session My worst sessions were those that were optional and/or in the simultaneous sessions. I find it hard to convince myself to take a second look at my material, as the learning outcomes are achieved too well. Q4: would recommend \u00b6 Useless to me. Q5: suggestions for future topics \u00b6 More on parallelizing in R Even more? It would be nice to perhaps have a little module on transferring files to and from the server: while I feel pretty confident about using R, I\u2019m not completelt sure how to get files to and from he server. Maybe link to the NAISS file transfer course [x] Link to the NAISS file transfer course in the course material And secondly, I\u2019m 99% sure it should be reasonably straightforward, but it might be nice for there to be a little extra note on best practices for installing and using STAN for Markov Chain Monte Carlo (MCMC). R, Julia and MATLAB can work with STAN, and because MCMC is such a slow process, I can imagine that this could be helpful. This feels quite niche to me. Q6: other feedback \u00b6 For some portions of this course it was a bit unclear when/what to do hands-on. Agree: it was unclear to me too sometimes. I am unsure if this applies to my session. The course documentation is very good and will help me the most in the future. Nice. I really liked the materials and the web page. It was extremely helpful, and I have bookmarked the course page because it is easier to use than all the official documentation. Nice. The exercises were well-prepared, Nice \u2026 although the parallel processing section was less good on both the web page, and the exercise code needed a bit of editing to get it to work. Not my session The organization was great Well done course coordinator! however the initial email maybe didn\u2019t make it quite clear enough that setting up a login account could take many days. I happened to already have one, but sitting the day before to try to set up, I could easily have missed that. Consider: [x] Suggest to make even clearer that getting an account takes days I really appreciate the amount of hands-on demonstrations: they were great. Unsure if this applies to me. Only a very, very small comment is that it was not always clear which .sh file was appropriate for the relevant exercises, for instance serial and parallel at the start. The exercises might be named with the same names, which would make it easier to find them. But this is such a small complaint. Does not apply to me. This seems like an easy fix. The whole material for the day was excellent, and I\u2019m feeling extremely confident about moving forward with getting started. Nice. I had some issues with my account not being set up correctly, which set me behind for the whole course and meant there were some things I couldn\u2019t test myself as they were being explained. This was not the fault of the course facilitators, but did make the course less useful for me. Agree. I liked the exercises best, makes it easy to understand how you can directly apply. I would have appreciated more time for them though. I agree. The same feedback as always :-) The length of the course is good, Nice. the machine learning part was difficult for me to follow. I agree. Not my session. To do \u00b6 [x] Suggest to remove \u2018Machine learning\u2019 [x] Suggest to remove \u2018Parallel\u2019 [x] Suggest to remove \u2018Isolated environments\u2019 [x] Suggest my favorite schedule +-------+------------------------------+--------------+ | Time | Topic | Teacher ( s ) | + ======= + ============================== + ============== + | 9 :00 | ( optional ) First login | BB + PO + RB | +-------+------------------------------+--------------+ | 9 :45 | Break | . | +-------+------------------------------+--------------+ | 10 :00 | Introduction | RB | +-------+------------------------------+--------------+ | 10 :10 | Syllabus | RB | +-------+------------------------------+--------------+ | 10 :20 | Load modules and run | RB | +-------+------------------------------+--------------+ | 11 :00 | Break | . | +-------+------------------------------+--------------+ | 11 :15 | Packages | BB | +-------+------------------------------+--------------+ | 12 :00 | Lunch | . | +-------+------------------------------+--------------+ | 13 :00 | Batch | BB | +-------+------------------------------+--------------+ | 14 :00 | Break | . | +-------+------------------------------+--------------+ | 14 :15 | Simultaneous session | . | +-------+------------------------------+--------------+ | . | HPC2N: ThinLinc, RStudio | PO | +-------+------------------------------+--------------+ | . | LUNARC: On-Demand, RStudio | RP | +-------+------------------------------+--------------+ | . | UPPMAX: Interactive, RStudio | RB | +-------+------------------------------+--------------+ | 15 :00 | Break | . | +-------+------------------------------+--------------+ | 15 :15 | To be decided by vote | ? | +-------+------------------------------+--------------+ | 15 :45 | Summary and evaluation | RB | +-------+------------------------------+--------------+ | 16 :00 | Done | . | +-------+------------------------------+--------------+ [x] Link to the NAISS file transfer course in the course material [x] Suggest to make even clearer that getting an account takes days","title":"Reflection"},{"location":"reflections/20250324_richel/#reflection","text":"Author: Richel Date: 2025-03-24 Language: R Lesson plans Evaluation Registrations: 33 Participants: 12 (36% of registrations shows up) With the 1 hour optional login session, we give 1 hour of teaching to the unprepared, while sacrificing 1 hour of teaching for the prepared. I think we should reward the prepared, by not having this drop-in. However, at the drop-in, 9 showed up, of which 5 were put in a breakout room with me. There, I discovered how useful it is. There were 2 out of 5 that could not have logged in without this session. Also, it is a great first impression of our students. +-------+------------------------------+--------------+ | Time | Topic | Teacher ( s ) | + ======= + ============================== + ============== + | 9 :00 | ( optional ) First login | BB + PO + RB | +-------+------------------------------+--------------+ | 9 :45 | Break | . | +-------+------------------------------+--------------+ | 10 :00 | Introduction | RB | +-------+------------------------------+--------------+ | 10 :10 | Syllabus | RB | +-------+------------------------------+--------------+ | 10 :20 | Load modules and run | RB | +-------+------------------------------+--------------+ | 10 :45 | Break | . | +-------+------------------------------+--------------+ | 11 :00 | Packages | BB | +-------+------------------------------+--------------+ | 11 :30 | Isolated environments | BB | +-------+------------------------------+--------------+ | 12 :00 | Lunch | . | +-------+------------------------------+--------------+ | 13 :00 | Batch | BB | +-------+------------------------------+--------------+ | 13 :30 | Parallel | PO | +-------+------------------------------+--------------+ | 14 :15 | Break | . | +-------+------------------------------+--------------+ | 14 :30 | Simultaneous session | . | +-------+------------------------------+--------------+ | . | HPC2N: ThinLinc, RStudio | PO | +-------+------------------------------+--------------+ | . | LUNARC: On-Demand, RStudio | RP | +-------+------------------------------+--------------+ | . | UPPMAX: Interactive, RStudio | RB | +-------+------------------------------+--------------+ | 15 :15 | Break | . | +-------+------------------------------+--------------+ | 15 :30 | Machine learning | PO | +-------+------------------------------+--------------+ | 16 :00 | Summary and evaluation | RB | +-------+------------------------------+--------------+ | 16 :15 | Done | . | +-------+------------------------------+--------------+ I overestimated the number or learners leaving: from my data is estimated 25% of all learners to leave. Instead, before and after the break there were around 12 learners. Recording has a clear effect on the number of learners with camera on/off. Online instruction in higher education: Promising, research-based, and evidence-based practices From: Boettcher, Judith V., and Rita-Marie Conrad. The Online Teaching Survival Guide : Simple and Practical Pedagogical Tips, John Wiley & Sons, Table 3.1: Be present at the course site Create a supportive online course community Develop a set of explicit expectations for your learners and yourself as to how you will communicate and how much time students should be working on the course each week Use a variety of large group, small group, and individual work experiences Use synchronous and asynchronous activities Ask for informal feedback early in the term Prepare discussion posts that invite responses, questions, discussions, and re\ufb02ections Think digital and mobile for all course content Combine core concept learning with customized and personalized learning Plan a good closing and wrap activity for the course Assess as you go by gathering evidences of learning Rigorously connect content to core concepts and learning outcomes Develop and use a content frame for your course Design experiences to help learners make progress on their novice-to-expert journey. From: Nilson, Linda B., and Ludwika A. Goodson. Online teaching at its best: Merging instructional design with teaching and learning research. John Wiley & Sons, 2021. I quote: Students learn new material better and can remember it longer when they learn it by engaging in an activity than when they passively watch or listen to an instructor talk [8 references] [\u2026] long lectures and presentations will fail because students stop viewing and listening after about six minutes. This phenomenon parallels McKeachie\u2019s earlier classroom findings about inattention after five to ten minutes [reference]. In online classes, such student inattention becomes explicitly visible through electronic monitoring of activities and questions from students about what has already been covered in a long presentation. Reasons why learners do not turn on their camera, is Tobi, Bernadette, et al. \u201cA case study on students\u2019 reasons for not switching on their cameras during online class sessions.\u201d Learning 6.41 (2021): 216-224 with results (number of learners = 50): Reason Percentage Fear of insufficient internet data 54% Poor internet connection 50% Physical condition of students\u2019 background/location 46% Physical appearance on camera (not looking good) 46% Uncomfortable to be looked at all the time 42% Or, from Alim, Syahrul, Sirirat Petsangsri, and John Morris. \u201cDoes an activated video camera and class involvement affect academic achievement? An investigation of distance learning students.\u201d Education and Information Technologies 28.5 (2023): 5875-5892. the following reasons for not turning on the camera: Reason Percentage Unready to learn 44% Unstable internet connection and limited quota 37% psychological reasons 11% devices overheating 4% following others 4% \u2018Unready to learn\u2019 means, I quote: \u2018the environment did not support camera activation, they had not taken a bath, were sick, wanted to sleep, were still doing something else, were fatigued or needed to go to the toilet.\u2019. The course is an introduction course. I feel the start-up parts are given too little time, for the things that I do not consider being beginner topics. I feel beginner things need their time, over sacrificing it for more sexy topics. Remove, in order of my preference: Machine learning Parallel Isolated I will suggest this in a meeting: [x] Suggest to remove \u2018Machine learning\u2019 [x] Suggest to remove \u2018Parallel\u2019 [x] Suggest to remove \u2018Isolated environments\u2019 My favorite schedule +-------+------------------------------+--------------+ | Time | Topic | Teacher ( s ) | + ======= + ============================== + ============== + | 9 :00 | ( optional ) First login | BB + PO + RB | +-------+------------------------------+--------------+ | 9 :45 | Break | . | +-------+------------------------------+--------------+ | 10 :00 | Introduction | RB | +-------+------------------------------+--------------+ | 10 :10 | Syllabus | RB | +-------+------------------------------+--------------+ | 10 :20 | Load modules and run | RB | +-------+------------------------------+--------------+ | 11 :00 | Break | . | +-------+------------------------------+--------------+ | 11 :15 | Packages | BB | +-------+------------------------------+--------------+ | 12 :00 | Lunch | . | +-------+------------------------------+--------------+ | 13 :00 | Batch | BB | +-------+------------------------------+--------------+ | 14 :00 | Break | . | +-------+------------------------------+--------------+ | 14 :15 | Simultaneous session | . | +-------+------------------------------+--------------+ | . | HPC2N: ThinLinc, RStudio | PO | +-------+------------------------------+--------------+ | . | LUNARC: On-Demand, RStudio | RP | +-------+------------------------------+--------------+ | . | UPPMAX: Interactive, RStudio | RB | +-------+------------------------------+--------------+ | 15 :00 | Break | . | +-------+------------------------------+--------------+ | 15 :15 | To be decided by vote | ? | +-------+------------------------------+--------------+ | 15 :45 | Summary and evaluation | RB | +-------+------------------------------+--------------+ | 16 :00 | Done | . | +-------+------------------------------+--------------+ I felt rushed during my session, with only 45 minutes. The learners did reach the learning outcomes in time, but I had no time to discuss this with them. Sure, one could argue that the LOs have been achieved, yet, on the other hand, there was no proper Feedback phase. I was a helper during the other sessions. I was not a very good helper: I find it hard to stay focused during monologues.","title":"Reflection"},{"location":"reflections/20250324_richel/#evaluations","text":"","title":"Evaluations"},{"location":"reflections/20250324_richel/#q1-course-satisfaction-825","text":"Useless information to me.","title":"Q1: Course satisfaction: 8.25"},{"location":"reflections/20250324_richel/#q2-pace-of-teaching","text":"Useless information to me.","title":"Q2: Pace of teaching"},{"location":"reflections/20250324_richel/#q3","text":"My grades (average is 77): learning_outcome success_score Comment I can find the module to be able to run R 94 Mine, great I can load the module to be able to run R 94 Mine, great I can run the R interpreter 94 Mine, great I can run the R command to get the list of installed R packages 88 Mine, great I can run an R script from the command-line 88 Mine, great I can find out if an R package is already installed 81 Mine, was extra I can load the pre-installed R packages 88 Mine, great I can install an R package from CRAN 81 Not mine I can use renv to create, activate, use and deactivate a virtual environment 62 Not mine I can submit a job to the scheduler to run an R script with regular code 78 Not mine I can submit a job to the scheduler to run an R script that uses parallel code 69 Not mine I can submit a job to the scheduler to run an R script that uses a GPU 56 Not mine I can find and load the R machine learning modules 50 Not mine I can submit a job to the scheduler to run an R script that uses machine learning 50 Not mine I can start an interactive session 81 Simultaneous session I can verify I am on the login node yes/no 78 Mine, not taught explicitly I can start an interactive session with multiple cores 78 Mine, not taught explicitly I can start RStudio 84 Simultaneous session My worst sessions were those that were optional and/or in the simultaneous sessions. I find it hard to convince myself to take a second look at my material, as the learning outcomes are achieved too well.","title":"Q3"},{"location":"reflections/20250324_richel/#q4-would-recommend","text":"Useless to me.","title":"Q4: would recommend"},{"location":"reflections/20250324_richel/#q5-suggestions-for-future-topics","text":"More on parallelizing in R Even more? It would be nice to perhaps have a little module on transferring files to and from the server: while I feel pretty confident about using R, I\u2019m not completelt sure how to get files to and from he server. Maybe link to the NAISS file transfer course [x] Link to the NAISS file transfer course in the course material And secondly, I\u2019m 99% sure it should be reasonably straightforward, but it might be nice for there to be a little extra note on best practices for installing and using STAN for Markov Chain Monte Carlo (MCMC). R, Julia and MATLAB can work with STAN, and because MCMC is such a slow process, I can imagine that this could be helpful. This feels quite niche to me.","title":"Q5: suggestions for future topics"},{"location":"reflections/20250324_richel/#q6-other-feedback","text":"For some portions of this course it was a bit unclear when/what to do hands-on. Agree: it was unclear to me too sometimes. I am unsure if this applies to my session. The course documentation is very good and will help me the most in the future. Nice. I really liked the materials and the web page. It was extremely helpful, and I have bookmarked the course page because it is easier to use than all the official documentation. Nice. The exercises were well-prepared, Nice \u2026 although the parallel processing section was less good on both the web page, and the exercise code needed a bit of editing to get it to work. Not my session The organization was great Well done course coordinator! however the initial email maybe didn\u2019t make it quite clear enough that setting up a login account could take many days. I happened to already have one, but sitting the day before to try to set up, I could easily have missed that. Consider: [x] Suggest to make even clearer that getting an account takes days I really appreciate the amount of hands-on demonstrations: they were great. Unsure if this applies to me. Only a very, very small comment is that it was not always clear which .sh file was appropriate for the relevant exercises, for instance serial and parallel at the start. The exercises might be named with the same names, which would make it easier to find them. But this is such a small complaint. Does not apply to me. This seems like an easy fix. The whole material for the day was excellent, and I\u2019m feeling extremely confident about moving forward with getting started. Nice. I had some issues with my account not being set up correctly, which set me behind for the whole course and meant there were some things I couldn\u2019t test myself as they were being explained. This was not the fault of the course facilitators, but did make the course less useful for me. Agree. I liked the exercises best, makes it easy to understand how you can directly apply. I would have appreciated more time for them though. I agree. The same feedback as always :-) The length of the course is good, Nice. the machine learning part was difficult for me to follow. I agree. Not my session.","title":"Q6: other feedback"},{"location":"reflections/20250324_richel/#to-do","text":"[x] Suggest to remove \u2018Machine learning\u2019 [x] Suggest to remove \u2018Parallel\u2019 [x] Suggest to remove \u2018Isolated environments\u2019 [x] Suggest my favorite schedule +-------+------------------------------+--------------+ | Time | Topic | Teacher ( s ) | + ======= + ============================== + ============== + | 9 :00 | ( optional ) First login | BB + PO + RB | +-------+------------------------------+--------------+ | 9 :45 | Break | . | +-------+------------------------------+--------------+ | 10 :00 | Introduction | RB | +-------+------------------------------+--------------+ | 10 :10 | Syllabus | RB | +-------+------------------------------+--------------+ | 10 :20 | Load modules and run | RB | +-------+------------------------------+--------------+ | 11 :00 | Break | . | +-------+------------------------------+--------------+ | 11 :15 | Packages | BB | +-------+------------------------------+--------------+ | 12 :00 | Lunch | . | +-------+------------------------------+--------------+ | 13 :00 | Batch | BB | +-------+------------------------------+--------------+ | 14 :00 | Break | . | +-------+------------------------------+--------------+ | 14 :15 | Simultaneous session | . | +-------+------------------------------+--------------+ | . | HPC2N: ThinLinc, RStudio | PO | +-------+------------------------------+--------------+ | . | LUNARC: On-Demand, RStudio | RP | +-------+------------------------------+--------------+ | . | UPPMAX: Interactive, RStudio | RB | +-------+------------------------------+--------------+ | 15 :00 | Break | . | +-------+------------------------------+--------------+ | 15 :15 | To be decided by vote | ? | +-------+------------------------------+--------------+ | 15 :45 | Summary and evaluation | RB | +-------+------------------------------+--------------+ | 16 :00 | Done | . | +-------+------------------------------+--------------+ [x] Link to the NAISS file transfer course in the course material [x] Suggest to make even clearer that getting an account takes days","title":"To do"},{"location":"reflections/20251006_richel/","text":"Reflection \u00b6 Author: Richel Date: 2025-10-06 Day: R I was ill in the morning. I was ill already for a week and hence was more absent from all team discussions. Teaching 9:00 First login \u00b6 There were 5 visitors. Me and the team were able to help all of them. I\u2019ve helped 1 learner and it was a good experience: she was a complete beginner and at the end of my, around 15 minutes, help, she was super ready. Teaching 10:00 Load modules and run \u00b6 There were 6 learners. Most time was spent with the 1 learner that did not attend the login session. When ill, my talking is slower. A good thing. However, due to the constant pain, I do not enjoy my teaching sessions as usual. I did meet all students properly. At 10:47 2 (out of 6) were done: I sent them on an earlier break. Teaching 14:00 RStudio \u00b6 Me and a colleague split up the learners: the colleague offered to help, as it was unsure how well I was (as I was less response in chat, due to -indeed- illness). I think it is nice to get unexpected help and I think it is a good idea that she had a 1-on-1 with \u2018her\u2019 COSMOS users. So, we split up, after announcing this in the main room. I was very happy to see my learners again. We were with 3, 1 that used Sigma, and 2 that used Dardel. I started with the Sigma user, as (1) she has no documentation, and (2) Dardel has Open OnDemand, which is easier. I did so after discussing this with the whole group. When the Sigma user was in the system (for the first time), I told her to try a bit for herself, so I could talk to the Dardel users. We agreed. I disturbed the Dardel users. They were already done. I told them that in 5 minutes I will give a bit more in-depth demo about RStudio, after helping the Sigma user (which I estimated to cost that long). The Dardel users agreed. The Sigma user was in quickly, so we started talking as a class again. I did my demo on RStudio, until it was time again to back to the main room. I remember from earlier reflections that I should not be too informal, and I think I succeeded reasonably (but not perfectly) well: it felt too nice to have them back :-) 15:00 Summary and Evaluation \u00b6 I went through the summary. What was missing was a list of other course. [x] Add link to SCoRe list of courses The evaluation was clean: I told them why I put them in breakout rooms. One out of six learners came back to ask his questions, which I discussed in a breakout room. Predictions \u00b6 My sessions went very well: In the \u2018Load modules and run\u2019 session, I have seen all run the R interpreter In the RStudio session, I have seen all run RStudio","title":"Reflection"},{"location":"reflections/20251006_richel/#reflection","text":"Author: Richel Date: 2025-10-06 Day: R I was ill in the morning. I was ill already for a week and hence was more absent from all team discussions.","title":"Reflection"},{"location":"reflections/20251006_richel/#teaching-900-first-login","text":"There were 5 visitors. Me and the team were able to help all of them. I\u2019ve helped 1 learner and it was a good experience: she was a complete beginner and at the end of my, around 15 minutes, help, she was super ready.","title":"Teaching 9:00 First login"},{"location":"reflections/20251006_richel/#teaching-1000-load-modules-and-run","text":"There were 6 learners. Most time was spent with the 1 learner that did not attend the login session. When ill, my talking is slower. A good thing. However, due to the constant pain, I do not enjoy my teaching sessions as usual. I did meet all students properly. At 10:47 2 (out of 6) were done: I sent them on an earlier break.","title":"Teaching 10:00 Load modules and run"},{"location":"reflections/20251006_richel/#teaching-1400-rstudio","text":"Me and a colleague split up the learners: the colleague offered to help, as it was unsure how well I was (as I was less response in chat, due to -indeed- illness). I think it is nice to get unexpected help and I think it is a good idea that she had a 1-on-1 with \u2018her\u2019 COSMOS users. So, we split up, after announcing this in the main room. I was very happy to see my learners again. We were with 3, 1 that used Sigma, and 2 that used Dardel. I started with the Sigma user, as (1) she has no documentation, and (2) Dardel has Open OnDemand, which is easier. I did so after discussing this with the whole group. When the Sigma user was in the system (for the first time), I told her to try a bit for herself, so I could talk to the Dardel users. We agreed. I disturbed the Dardel users. They were already done. I told them that in 5 minutes I will give a bit more in-depth demo about RStudio, after helping the Sigma user (which I estimated to cost that long). The Dardel users agreed. The Sigma user was in quickly, so we started talking as a class again. I did my demo on RStudio, until it was time again to back to the main room. I remember from earlier reflections that I should not be too informal, and I think I succeeded reasonably (but not perfectly) well: it felt too nice to have them back :-)","title":"Teaching 14:00 RStudio"},{"location":"reflections/20251006_richel/#1500-summary-and-evaluation","text":"I went through the summary. What was missing was a list of other course. [x] Add link to SCoRe list of courses The evaluation was clean: I told them why I put them in breakout rooms. One out of six learners came back to ask his questions, which I discussed in a breakout room.","title":"15:00 Summary and Evaluation"},{"location":"reflections/20251006_richel/#predictions","text":"My sessions went very well: In the \u2018Load modules and run\u2019 session, I have seen all run the R interpreter In the RStudio session, I have seen all run RStudio","title":"Predictions"},{"location":"reflections/20251010_richel/","text":"Reflection \u00b6 Author: Richel Date: 2025-10-10 Day: Advanced Preparation \u00b6 Time was tight, also due to illness, yet I felt fine about the final result, due to some ruthless prioritizations. Start \u00b6 At 9:00, there were zero learners. With the four teachers there, we decided what to do: start teaching at 10:00 again, as some assumed that our learners may think that the course starts at 10:00 (as all other days have a login session at 9:00). I left the Zoom room, to work on other things. At around 9:17 I saw a Matrix message about a learner having arrived. I came back and started teaching to 1 learner. It was very interactive, also thanks to the learner himself, that was not shy to ask questions: he really seemed to know how to get the most out of a course. I introduced the schedule for the day. We did all the exercises in my first session, including reading in silence. It was really silent then. As this session was interactive (when not reading), it was nice to see the misconceptions that popped up and could be fixed: the learner thought that the time units could not be split up. I\u2019ve updated the course material to reflect that. We did a subset of the exercises of \u2018Thread parallelism\u2019: Exercise Done 1 Yes 2 Yes 3 Yes 4 Yes 5 No: nobody to share with 6 No: no time 7 Yes X1 No: exercise X2 was more interesting X2 Yes The main reason for not doing everything was due to (1) having 20 minutes less, (2) me preferring an in-depth discussion over doing more. Would it start at the scheduled starting time, I think it would have worked. During my first two hours, there were some colleagues in the background. One of them used the chat to explain why the speed measurements on Kebnekaise are so noisy: Kebnekaise is very heterogeneous https://www.hpc2n.umu.se/resources/hardware/kebnekaise ! I\u2019ve already added this to the course material during reading. All in all, I\u2019ve had a good time with our learner and I think we spent it well.","title":"Reflection"},{"location":"reflections/20251010_richel/#reflection","text":"Author: Richel Date: 2025-10-10 Day: Advanced","title":"Reflection"},{"location":"reflections/20251010_richel/#preparation","text":"Time was tight, also due to illness, yet I felt fine about the final result, due to some ruthless prioritizations.","title":"Preparation"},{"location":"reflections/20251010_richel/#start","text":"At 9:00, there were zero learners. With the four teachers there, we decided what to do: start teaching at 10:00 again, as some assumed that our learners may think that the course starts at 10:00 (as all other days have a login session at 9:00). I left the Zoom room, to work on other things. At around 9:17 I saw a Matrix message about a learner having arrived. I came back and started teaching to 1 learner. It was very interactive, also thanks to the learner himself, that was not shy to ask questions: he really seemed to know how to get the most out of a course. I introduced the schedule for the day. We did all the exercises in my first session, including reading in silence. It was really silent then. As this session was interactive (when not reading), it was nice to see the misconceptions that popped up and could be fixed: the learner thought that the time units could not be split up. I\u2019ve updated the course material to reflect that. We did a subset of the exercises of \u2018Thread parallelism\u2019: Exercise Done 1 Yes 2 Yes 3 Yes 4 Yes 5 No: nobody to share with 6 No: no time 7 Yes X1 No: exercise X2 was more interesting X2 Yes The main reason for not doing everything was due to (1) having 20 minutes less, (2) me preferring an in-depth discussion over doing more. Would it start at the scheduled starting time, I think it would have worked. During my first two hours, there were some colleagues in the background. One of them used the chat to explain why the speed measurements on Kebnekaise are so noisy: Kebnekaise is very heterogeneous https://www.hpc2n.umu.se/resources/hardware/kebnekaise ! I\u2019ve already added this to the course material during reading. All in all, I\u2019ve had a good time with our learner and I think we spent it well.","title":"Start"}]}